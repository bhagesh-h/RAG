<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Large Language Models for Bioinformatics</title>
<!--Generated on Fri Jan 10 01:39:36 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on  %Co‐first␣authors .-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2501.06271v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S1" title="In Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S2" title="In Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background of Language Models and Foundation Models in Bioinformatics</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S2.SS1" title="In 2 Background of Language Models and Foundation Models in Bioinformatics ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Foundations of Language Models and Bioinformatics Overview</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S2.SS1.SSS1" title="In 2.1 Foundations of Language Models and Bioinformatics Overview ‣ 2 Background of Language Models and Foundation Models in Bioinformatics ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Basics of Large Language Models and Foundations Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S2.SS1.SSS2" title="In 2.1 Foundations of Language Models and Bioinformatics Overview ‣ 2 Background of Language Models and Foundation Models in Bioinformatics ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Bioinformatics Applications and Challenges</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S2.SS2" title="In 2 Background of Language Models and Foundation Models in Bioinformatics ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Training Methods and Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S2.SS3" title="In 2 Background of Language Models and Foundation Models in Bioinformatics ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Bioinformatics-Specific Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S2.SS4" title="In 2 Background of Language Models and Foundation Models in Bioinformatics ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Model Evolution and Key Milestones</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S3" title="In Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Applications in Bioinformatics Problems</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S3.SS1" title="In 3 Applications in Bioinformatics Problems ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Genome Level</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S3.SS1.SSS1" title="In 3.1 Genome Level ‣ 3 Applications in Bioinformatics Problems ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>LLM for DNA Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S3.SS1.SSS2" title="In 3.1 Genome Level ‣ 3 Applications in Bioinformatics Problems ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>LLM for RNA Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S3.SS2" title="In 3 Applications in Bioinformatics Problems ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Gene Products Level</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S3.SS3" title="In 3 Applications in Bioinformatics Problems ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Epigenomics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S3.SS4" title="In 3 Applications in Bioinformatics Problems ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Protein Level</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S3.SS4.SSS1" title="In 3.4 Protein Level ‣ 3 Applications in Bioinformatics Problems ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>Models for Protein LLM</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S3.SS4.SSS2" title="In 3.4 Protein Level ‣ 3 Applications in Bioinformatics Problems ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Downstream Tasks for Protein LLM</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S3.SS5" title="In 3 Applications in Bioinformatics Problems ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Metabolomics</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S3.SS5.SSS1" title="In 3.5 Metabolomics ‣ 3 Applications in Bioinformatics Problems ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.1 </span>Data Integration and Interpretation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S3.SS5.SSS2" title="In 3.5 Metabolomics ‣ 3 Applications in Bioinformatics Problems ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.2 </span>Biomarker Discovery and Validation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S3.SS5.SSS3" title="In 3.5 Metabolomics ‣ 3 Applications in Bioinformatics Problems ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.3 </span>Metabolic Pathway Analysis and Drug Discovery</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S3.SS5.SSS4" title="In 3.5 Metabolomics ‣ 3 Applications in Bioinformatics Problems ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.4 </span>Personalized Medicine</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S3.SS5.SSS5" title="In 3.5 Metabolomics ‣ 3 Applications in Bioinformatics Problems ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.5 </span>Literature Mining and Knowledge Discovery</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S3.SS5.SSS6" title="In 3.5 Metabolomics ‣ 3 Applications in Bioinformatics Problems ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.6 </span>Quality Control and Data Standardization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S3.SS5.SSS7" title="In 3.5 Metabolomics ‣ 3 Applications in Bioinformatics Problems ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.7 </span>Predictive Modeling and Simulation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S4" title="In Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span> Disease-Specific Bio-medical Applications</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S4.SS1" title="In 4 Disease-Specific Bio-medical Applications ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Brain Aging and Brain Disease</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S4.SS1.SSS1" title="In 4.1 Brain Aging and Brain Disease ‣ 4 Disease-Specific Bio-medical Applications ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Clinical Diagnosis Support</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S4.SS1.SSS2" title="In 4.1 Brain Aging and Brain Disease ‣ 4 Disease-Specific Bio-medical Applications ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Therapeutic Assistance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S4.SS1.SSS3" title="In 4.1 Brain Aging and Brain Disease ‣ 4 Disease-Specific Bio-medical Applications ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Information Driven
Decision-making</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S4.SS2" title="In 4 Disease-Specific Bio-medical Applications ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Cancer Treated by Radiation Therapy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S4.SS3" title="In 4 Disease-Specific Bio-medical Applications ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Infectious Diseases</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S4.SS3.SSS1" title="In 4.3 Infectious Diseases ‣ 4 Disease-Specific Bio-medical Applications ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Disease Prediction and Vaccine Efficacy Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S4.SS3.SSS2" title="In 4.3 Infectious Diseases ‣ 4 Disease-Specific Bio-medical Applications ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Vaccine Adherence and Risk Prediction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S4.SS3.SSS3" title="In 4.3 Infectious Diseases ‣ 4 Disease-Specific Bio-medical Applications ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.3 </span>Biomarker Analysis and Antigen Prediction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S4.SS3.SSS4" title="In 4.3 Infectious Diseases ‣ 4 Disease-Specific Bio-medical Applications ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.4 </span>Vaccine Recommendation and Immune Response</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S4.SS3.SSS5" title="In 4.3 Infectious Diseases ‣ 4 Disease-Specific Bio-medical Applications ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.5 </span>Sentiment Analysis and Public Attitude Research on Social Media</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S4.SS3.SSS6" title="In 4.3 Infectious Diseases ‣ 4 Disease-Specific Bio-medical Applications ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.6 </span>Epidemiology and Public Health Data Analysis</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S5" title="In Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Drug Discovery and Development</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S5.SS1" title="In 5 Drug Discovery and Development ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Drug Target Identification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S5.SS2" title="In 5 Drug Discovery and Development ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Molecular Docking and Drug Design</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S6" title="In Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Immunology and Vaccine Development</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S6.SS1" title="In 6 Immunology and Vaccine Development ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Immune Response Analysis and Biomarker Research</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S6.SS2" title="In 6 Immunology and Vaccine Development ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Vaccine Development and Recommendation Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S6.SS3" title="In 6 Immunology and Vaccine Development ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Vaccine Efficacy Prediction and Immunogenicity Studies</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S6.SS4" title="In 6 Immunology and Vaccine Development ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Vaccine Hesitancy and Public Attitude Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S6.SS5" title="In 6 Immunology and Vaccine Development ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Vaccine Safety and Adverse Event Detection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S6.SS6" title="In 6 Immunology and Vaccine Development ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.6 </span>Vaccine-Related Social and Health Data Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S7" title="In Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Discussion and Future Directions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S8" title="In Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Large Language Models for Bioinformatics</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wei Ruan
</span><span class="ltx_author_notes">Co-first authors.
<span class="ltx_contact ltx_role_affiliation">School of Computing, University of Georgia, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yanjun Lyu<span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotemark: </span></span></span></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Computer Science and Engineering, University of Texas at Arlington, TX,
USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jing Zhang<span class="ltx_note ltx_role_footnotemark" id="footnotex2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotemark: </span></span></span></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Computer Science and Engineering, University of Texas at Arlington, TX,
USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiazhang Cai
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
Department of Statistics, University of Georgia, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Peng Shu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">School of Computing, University of Georgia, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yang Ge
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Epidemiology and Biostatistics, University of Georgia, Athens, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yao Lu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Epidemiology and Biostatistics, University of Georgia, Athens, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shang Gao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Biostatistics, UNC Chapel Hill, NC, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yue Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">School of Computing, University of Georgia, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Peilong Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Radiation Oncology, Mayo Clinic, Phoenix, AZ, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lin Zhao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">School of Computing, University of Georgia, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tao Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
Department of Statistics, University of Georgia, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yufang Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
Department of Statistics, University of Georgia, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Luyang Fang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
Department of Statistics, University of Georgia, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ziyu Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
Department of Statistics, University of Georgia, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhengliang Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">School of Computing, University of Georgia, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yiwei Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">School of Computing, University of Georgia, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zihao Wu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">School of Computing, University of Georgia, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Junhao Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">School of Computing, University of Georgia, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hanqi Jiang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">School of Computing, University of Georgia, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yi Pan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">School of Computing, University of Georgia, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhenyuan Yang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">School of Computing, University of Georgia, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jingyuan Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Radiation Oncology, Mayo Clinic, Phoenix, AZ, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shizhe Liang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Institute of Plant Breeding, Genetics &amp; Genomics, University of Georgia, Athens, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wei Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">School of Computer and Cyber Sciences, Augusta University, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Terry Ma
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuan Dou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">College of Engineering, University of Georgia, Athens, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jianli Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">College of Engineering, University of Georgia, Athens, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xinyu Gong
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">College of Engineering, University of Georgia, Athens, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qi Gan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">College of Engineering, University of Georgia, Athens, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yusong Zou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">College of Engineering, University of Georgia, Athens, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zebang Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">College of Engineering, University of Georgia, Athens, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuanxin Qian
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">College of Engineering, University of Georgia, Athens, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shuo Yu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">College of Engineering, University of Georgia, Athens, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jin Lu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">School of Computing, University of Georgia, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kenan Song
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">College of Engineering, University of Georgia, Athens, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xianqiao Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">College of Engineering, University of Georgia, Athens, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andrea Sikora
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Biomedical Informatics, University of Colorado, CO, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gang Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Radiology, University of North Carolina at Chapel Hill, NC, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiang Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Radiology, Massachusetts General Hospital and Harvard Medical School, MA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Quanzheng Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Radiology, Massachusetts General Hospital and Harvard Medical School, MA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yingfeng Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Computer Science and Engineering, University of Tennessee at Chattanooga, TN, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lu Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Computer Science, Indiana University Indianapolis, IN, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yohannes Abate
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Physics and Astronomy, University of Georgia, Athens, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lifang He
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Computer Science and Engineering, Lehigh University, PA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wenxuan Zhong
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
Department of Statistics, University of Georgia, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rongjie Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
Department of Statistics, University of Georgia, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chao Huang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Epidemiology and Biostatistics, University of Georgia, Athens, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wei Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Radiation Oncology, Mayo Clinic, Phoenix, AZ, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ye Shen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Epidemiology and Biostatistics, University of Georgia, Athens, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ping Ma
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
Department of Statistics, University of Georgia, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hongtu Zhu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Biostatistics, UNC Chapel Hill, NC, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yajun Yan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">College of Engineering, University of Georgia, Athens, GA, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dajiang Zhu
</span><span class="ltx_author_notes">Corresponding authors.
<span class="ltx_contact ltx_role_affiliation">Department of Computer Science and Engineering, University of Texas at Arlington, TX,
USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tianming Liu<span class="ltx_note ltx_role_footnotemark" id="footnotex3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotemark: </span></span></span></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">School of Computing, University of Georgia, GA, USA
</span></span></span>
</div>
<div class="ltx_dates">(January 2025)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1"><span class="ltx_text" id="id1.id1.1" lang="en">With the rapid advancements in large language model (LLM) technology and the emergence of bioinformatics-specific language models (BioLMs), there is a growing need for a comprehensive analysis of the current landscape, computational characteristics, and diverse applications. This survey aims to address this need by providing a thorough review of BioLMs, focusing on their evolution, classification, and distinguishing features, alongside a detailed examination of training methodologies, datasets, and evaluation frameworks. We explore the wide-ranging applications of BioLMs in critical areas such as disease diagnosis, drug discovery, and vaccine development, highlighting their impact and transformative potential in bioinformatics. We identify key challenges and limitations inherent in BioLMs, including data privacy and security concerns, interpretability issues, biases in training data and model outputs, and domain adaptation complexities. Finally, we highlight emerging trends and future directions, offering valuable insights to guide researchers and clinicians toward advancing BioLMs for increasingly sophisticated biological and clinical applications.</span></p>
</div>
<section class="ltx_section" id="S1" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The rapid development of large language models (LLMs) such as BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib1" title="">1</a>]</cite>, GPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib2" title="">2</a>]</cite>, and their specialized counterparts has revolutionized the field of natural language processing (NLP). Their ability to model context, interpret complex data patterns, and generate human-like responses has naturally extended their applicability to bioinformatics, where biological sequences often mirror the structure and complexity of human languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib3" title="">3</a>]</cite>. LLMs have been successfully applied across various bioinformatics domains, including genomics, proteomics, and drug discovery, offering insights that were previously unattainable through traditional computational methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib4" title="">4</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Despite significant advancements, challenges remain in the systematic categorization and comprehensive evaluation of applications of these models on bioinformatic problems. Considering the variety of bioinformatics data and the complexity of life activities, navigating the field can often be challenging, as existing studies tend to focus on a limited scope of applications. This leaves gaps in understanding the broader utility of LLMs in various bioinformatics subfields <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">This survey aims to address these challenges by providing a comprehensive overview of LLM applications in bioinformatics.
By focusing on different levels of life activities, this article collected and exhibited related works from two major views: life science and biomedical applications.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">We have collaborated with domain experts to compile a thorough analysis spanning key areas in these views, such as nucleoid analysis, protein structure and function prediction, genomics, drug discovery, and disease modeling, including applications in brain diseases and cancers, as well as vaccine development.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In addition, we propose the new term ‘Life Active Factors’ (LAFs) to describe the molecular and cellular components that serve as candidates for life science research targets, which widely includes not only concrete entities (DNA, RNA, protein, genes, drugs) but also abstract components (bio-pathways, regulators, gene-networks, protein interactions) and biological measurements (phenotypes, disease biomarkers). LAFs is a comprehensive term that is capable of reconciling the conceptual divergence arising from research across various bioinformatics subfields, benefiting the understanding of multi-modality data for LAFs and their interplays in complex bio-systems. The introduction of LAFs aligns well with the spirit of foundational models and emphasizes the unification across sequence, structure, and function of the LAFs while respecting the interrelationships of each LAF as a node within the biological network.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">By bridging existing knowledge gaps, this work seeks to equip bioinformaticians, biologists, clinicians, and computational researchers with an understanding of how LLMs can be effectively leveraged to tackle pressing problems in bioinformatics. Our survey not only highlights recent advances but also identifies open challenges and opportunities, laying the foundation for future interdisciplinary collaboration and innovation (Figure  <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="403" id="S1.F1.g1" src="extracted/6122597/figures/P1.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">The applications of the methodology of LLMs in bioinformatics tasks.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S2" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background of Language Models and Foundation Models in Bioinformatics</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Bioinformatics has become a fundamental and transformative field in life sciences, bridging computational techniques and biological research. It emphasizes the development and application of computational tools and methodologies to manage and interpret vast amounts of biomedical data, transforming them into actionable insights and driving advancements across diverse downstream applications. Modern computational tools, particularly those rooted in deep learning technology, have significantly accelerated the evolution of biological research.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The rapid advancements in LLMs technologies have inspired new approaches to bioinformatics computing. Considering the complexity of biological systems and highly structured nature of bioinformatics data, LLM-based computing methods have proven effective in addressing challenges across fields such as genomics, proteomics, and molecular biology. Inspired by LLM architectures like transformers, foundation models in bioinformatics excel at capturing complex patterns and relationships in biological data. They have evolved from single-modality tools to sophisticated multimodal systems, integrating diverse datasets such as genomic sequences and protein structures.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Central to their success is the availability of large-scale, high-quality training data and the adoption of self-supervised pretraining and fine-tuning techniques. These methods allow models to extract meaningful features from unlabeled data and adapt to specific bioinformatics tasks. Together with advances in architecture design, these innovations have broadened the capabilities and impact of foundation models, unlocking new insights into biological systems and accelerating progress in the life sciences. The following sections discuss these advanced computing methods along with the intrinsic properties of biological systems and structured bioinformatics data.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Foundations of Language Models and Bioinformatics Overview</h3>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Basics of Large Language Models and Foundations Models</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">Traditional language models are engineered to process and generate text in a human-like manner, leveraging the extensive datasets used during their training. These models excel at interpreting context, producing coherent and contextually appropriate responses, performing translations, summarizing text, and answering questions. LLMs are a type of foundation model trained on vast datasets to provide flexible and powerful capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib8" title="">8</a>]</cite> that address a broad spectrum of use cases and applications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib66" title="">66</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib67" title="">67</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib68" title="">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib69" title="">69</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib71" title="">71</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib72" title="">72</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib73" title="">73</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib74" title="">74</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib75" title="">75</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib76" title="">76</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib77" title="">77</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib78" title="">78</a>]</cite>. By efficiently handling diverse tasks, LLMs eliminate the need for building and training separate domain-specific models for each use case—a process that is often limited by cost and resource constraints. This unified approach not only fosters synergies across tasks but also frequently results in superior performance, making LLMs a more scalable and efficient solution. There are several key elements that make the language model successful in adaptation to bioinformatics tasks (Figure  <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_tag">1</span></a>(a)).</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p2">
<p class="ltx_p" id="S2.SS1.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p2.1.1">Representation learning and tokenization</span>
Tokenization in LLMs is influenced by the design of their tokenization algorithms, which primarily use subword-level vocabularies to represent text sequence data effectively. Popular tokenization algorithms, such as Byte-Pair Encoding (BPE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib79" title="">79</a>]</cite>, WordPiece <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib80" title="">80</a>]</cite>, and Unigram <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib81" title="">81</a>]</cite>, are widely used. Although their vocabularies cannot perfectly capture every possible variation of input expressions, these tokenization methods effectively encode the features of words and their contextual relationships.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p3">
<p class="ltx_p" id="S2.SS1.SSS1.p3.1">In the view of representation learning, the tokenization and token embedding algorithms of the language model generally succeeded in representing the hidden factors of variation behind the data. This representation is based on the unsupervised learning scheme of the language models. The sub-word context features learned in the encoder modules or embedding layers follow the probabilistic modeling and continuously update the representations on large corpus datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib82" title="">82</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p4">
<p class="ltx_p" id="S2.SS1.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p4.1.1">Attention mechanism</span>
LLMs widely use the transformer model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib83" title="">83</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib84" title="">84</a>]</cite> as their foundational architecture. A core innovation of the transformer model is the multi-head self-attention mechanism, which establishes relationships among all relevant tokens, enabling more effective encoding of each word in the input sequence. The self-attention layer processes a sequence of tokens (analogous to words in a language) and learns context information across the entire sequence. The "multi-head" aspect refers to multiple attention heads operating simultaneously to capture diverse contextual features. Inside a single attention head, a token output embedding in a sequence is computed and fused with other tokens in the context with a proper causal mask. Such global level attention mechanic enables efficient information fusion along available context windows.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p5">
<p class="ltx_p" id="S2.SS1.SSS1.p5.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p5.1.1">Self-supervised training methods</span>
Language models are trained using self-supervised learning methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib85" title="">85</a>]</cite>. Unlike supervised learning, which typically requires human annotations, language models can leverage vast amounts of unannotated text data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib86" title="">86</a>]</cite>. The objective of unsupervised learning is to analyze unlabeled data by identifying and capturing its meaningful properties. Neural networks can extend some of these approaches. For example, autoencoders compress data into a low-dimensional representation through a hidden layer known as the bottleneck layer and then reconstruct the original input data from this representation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib87" title="">87</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib88" title="">88</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib89" title="">89</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib90" title="">90</a>]</cite>.
Language models leverage either the next word in a sentence as a natural label for the context or artificially mask a known word and predict it. This method, where unstructured data generates its own labels (e.g., predicting the next word or a masked word) and language models are trained to predict them, is known as self-supervised learning. Transformer-based models, with their parallel processing capabilities and ability to capture correlations across entire sequences, have achieved state-of-the-art (SOTA) performance  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib91" title="">91</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib92" title="">92</a>]</cite>. A more advanced training diagram is the text-to-text framework. This kind of training diagram unified multiple kinds of tasks, including translation, question answering, classification, formulated and feeding to model as input and training it as a generative model to predict target text. This framework, which is named ‘T5’ benefits using the same model, loss function, hyperparameters, etc. across a diverse set of tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib93" title="">93</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p6">
<p class="ltx_p" id="S2.SS1.SSS1.p6.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p6.1.1">Pre-training</span>
In many supervised learning problems, input data is represented by multiple features, comprising numerical or categorical information that can aid in making predictions. Scratch-trained models, which initialize and train all parameters from the ground up using task-specific datasets, typically require numerous iterations to converge fully on a single task. In general, transformer-based language models fall into two categories: scratch-trained models and pre-trained models. LLMs apply transformer-based pre-trained models that are trained from large amounts of unlabeled data and then fine-tuned for specific tasks. Pre-training learns general information from unlabeled data which can improve the convergence rate of the target tasks and often has better generalization than training parameters from scratch <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib94" title="">94</a>]</cite>. The use of context information in a large corpus to pre-train the whole model (or encoder modules) has achieved SOTA results in various downstream tasks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Bioinformatics Applications and Challenges</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">Using deep learning methods like language models to tackle bioinformatics problems is challenging. While deep learning models have shown superior accuracy in specific bioinformatics applications (e.g., genomics applications) compared to SOTA approaches and are adept at handling multimodal and highly heterogeneous data, significant challenges remain. Further work is required to integrate and analyze diverse datasets required for deep learning for genomic prediction and prognostic tasks. This is especially important for the development of explainable language models that can identify novel biomarkers and elucidate regulatory interactions across various biology levels: pathological conditions, including different tissues and disease states. These advancements require a deep understanding of complex bioinformatics data, the related tasks, and their mutual relationships <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib95" title="">95</a>]</cite>.
In this review, we discuss such issues through two lens: the various biology levels and the inherent regulations of life activities.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p2">
<p class="ltx_p" id="S2.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS2.p2.1.1">Various biology levels</span>
Although no gold standard division was available, the levels of life-science factors in bioinformatics can be divided into five levels, from micro to macro. Here, we take the mammal model organisms as a template, the levels can be divided into: the molecular level, the genome-scale level, the cellular level, the tissue/organ system level, and the population/community/metagenomics level (Figure  <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_tag">1</span></a>(b)). Bioinformatics often focuses on the first three levels (i.e., the molecular level, the genomic-scale level, and the cellular level). The molecular level analysis targets involved biologically active molecules, which include nucleic acids, amino acids, and other small bioactive molecules, and the relative experiments aimed at interpreting the life activities at this scale. The genomic-scale level models the life activities from DNA, RNA, and proteins to metabolomics. The most famous regulation at the genomic scale level is The Central Dogma, which reveals the intrinsic relations of main life-activity factors on a sub-cellular scale. The whole sub-cellular system is modeled hierarchically, beginning with DNA, mRNA, and proteins, extending to metabolomics, and ultimately inferring the phenotype <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib96" title="">96</a>]</cite>.
At the cellular level, understanding cellular mechanisms is a fundamental challenge in biology and holds significant importance in biomedical fields, particularly concerning disease phenotypes and precision medicine. Using Genes (The specific sequences of nucleotides within DNA that control downstream life activities) as a unit, the functions of genes and the gene products are essential research targets at this level. A comprehensive, structured, computation-accessible representation of gene function and variations is crucial for bioinformatic understanding of the cellular organism or virus. At the same time, the gene networks and mutual influences of gene products pose a challenge for such areas.
Single-cell sequencing technologies allow us to obtain gene expression data at the mRNA level, providing a foundation for analyzing entire cellular systems. This data is now extensively used to identify cell states during development, characterize specific tissues or organs, and evaluate patient-specific drug responses.
In this review, the molecular components at the genomics level and cellular levels and their respective sets are collectively referred to as Life Active Factors (LAFs). It is important to note that the sequence representation format is the most commonly observed for each LAF. However, multi-modality data for LAF is also significant for representing the property of LAF, i.e., the highly structured data format to record the function descriptions, abundancy, variations, and expressions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib97" title="">97</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib98" title="">98</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p3">
<p class="ltx_p" id="S2.SS1.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS2.p3.1.1">Inherent regulations of life activities</span> Since most LAFs at each biological level are represented in a sequence format, transformer-based pre-trained language models are particularly well-suited for analyzing these sequences. An emerging consensus suggests that these sequences embody an underlying language that can be deciphered using language models. However, in order to play the roles in life activities, an essential logic of a single LAF is ‘sequences-structures-functions’. Take proteomics analysis as an example, protein sequences can be viewed as a concatenation of letters from the amino acids, analogously to human languages. The latest protein language models utilize these formatted letter representations of secondary structural elements, which combine to form domains responsible for specific functions. The protein language models also direct inference of full atomic-level protein structure from primary sequence and produce functional proteins that evolution would require hundreds of millions of years to uncover <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib99" title="">99</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib100" title="">100</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib101" title="">101</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p4">
<p class="ltx_p" id="S2.SS1.SSS2.p4.1">In life activities, there are important regulation relationships among the LAFs across different levels as well as intra-level relationships. Considering the genomics level, genes control hereditary traits primarily by regulating the production of RNA and protein products. According to the central dogma of molecular biology, genes within DNA are transcribed into messenger RNA (mRNA), which is then translated into gene products, such as proteins. For any given gene product, whether RNA or protein, its origin can be traced back to the gene that directed its synthesis. This traceability highlights that fully understanding a gene’s functionality requires considering not only the gene itself but also the roles and functions of all its associated products.
Genes regulate each other and create feedback loops to form cyclic chains of dependencies in gene regulatory networks, graph neural network-styled operations are suitable to model the “steady state” of genes. It is the same for proteins in protein-protein
interactions (PPI). In the layer of pathways, it is a hypergraph where each hyperedge is a pathway including multiple proteins.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p5">
<p class="ltx_p" id="S2.SS1.SSS2.p5.1">Within the cellular level, pathways integrate individual genes or protein products to perform certain cell functions under mutual intra-level regulations. Proteins interact with one another in various ways, such as inhibiting, activating, or combining with others, thereby influencing expression levels or protein abundances within cells. These interactions are collectively referred to as PPI. Some databases systematically organize results by annotating functionalities using Gene Ontology (GO), utilizing the unique gene identifiers assigned to each gene within the genomic system <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib102" title="">102</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib103" title="">103</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Training Methods and Models</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p1.1.1">Pre-training</span> is a critical phase in the development of LLMs, where a model learns foundational linguistic representations by training on extensive and diverse datasets. This process typically employs self-supervised learning techniques such as masked language modeling (e.g., BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib1" title="">1</a>]</cite>) or causal language modeling (e.g., GPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib2" title="">2</a>]</cite>), enabling the model to predict masked tokens or the next word in a sequence. Unlike traditional deep neural networks (DNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib104" title="">104</a>]</cite>, which are often pre-trained on domain-specific datasets such as ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib105" title="">105</a>]</cite>, pre-training of LLMs is conducted on significantly larger datasets comprising diverse domains, including books, encyclopedias, and web content. Moreover, pre-training LLMs involves models with billions or even trillions of parameters, making it computationally and resource-intensive compared to conventional DNNs.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">The primary advantage of pre-training lies in the model’s ability to generalize across diverse language tasks, often achieving zero-shot <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib106" title="">106</a>]</cite> or few-shot <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib107" title="">107</a>]</cite> performance without additional task-specific training. This broad generalization enables LLMs to excel in tasks spanning natural language understanding, generation, and reasoning. However, the disadvantages of pre-training include high computational and energy costs, often requiring distributed systems with high-performance hardware. Additionally, pre-trained models can inherit biases and errors present in the training corpus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib108" title="">108</a>]</cite>, potentially leading to biased or undesirable outputs.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p3.1.1">Fine-tuning</span> is the subsequent stage that builds upon the pre-trained model by adapting it to specific tasks or domains through additional supervised or semi-supervised training. This process utilizes smaller, targeted datasets and optimizes the model for a specific use case. Fine-tuning can be categorized into task-specific fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib109" title="">109</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib110" title="">110</a>]</cite>, where models are specialized for particular tasks such as sentiment analysis or machine translation; domain-specific fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib38" title="">38</a>]</cite>, which refines the model for specialized fields such as medicine or law; and instruction fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib111" title="">111</a>]</cite>, where the model is trained to respond to natural language prompts in an aligned manner. Recent advancements in parameter-efficient fine-tuning methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib112" title="">112</a>]</cite>, such as LoRA (Low-Rank Adaptation) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib113" title="">113</a>]</cite> and adapters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib114" title="">114</a>]</cite>, have further improved the efficiency of this process by updating only a subset of the model’s parameters while maintaining the computational benefits of the pre-trained foundation.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">Fine-tuning enhances the model’s performance on specific tasks by leveraging domain- or task-specific data, achieving state-of-the-art results in various applications. However, it introduces challenges such as the risk of overfitting to the fine-tuning dataset, potentially diminishing the model’s generalization capabilities. Furthermore, fine-tuning requires high-quality labeled data to ensure reliability and accuracy in specialized applications.</p>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p5.1.1">Reinforcement Learning with Human Feedback (RLHF)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib115" title="">115</a>]</cite> represents a crucial additional stage in the training pipeline of large language models, designed to align model outputs with human preferences and expectations. While pre-training and fine-tuning equip the model with general linguistic understanding and task-specific expertise, RLHF optimizes the model’s behavior to produce responses that are more aligned with human values, instructions, or conversational styles, which is particularly critical for applications such as conversational agents, where user interaction quality is paramount.</p>
</div>
<div class="ltx_para" id="S2.SS2.p6">
<p class="ltx_p" id="S2.SS2.p6.1">RLHF involves three primary components: a reward model trained on human-labeled preferences, a reinforcement learning (RL) algorithm to optimize the model’s behavior based on the reward model, and iterative human feedback to refine the reward system. The reward model is typically developed by collecting a dataset of model outputs ranked by human evaluators. This ranking serves as the ground truth to train the reward model, which predicts the desirability of a given output. Subsequently, reinforcement learning algorithms, such as Proximal Policy Optimization (PPO) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib116" title="">116</a>]</cite>, adjust the model parameters to maximize the reward score predicted by the reward model. Besides, Direct Preference Optimization (DPO) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib117" title="">117</a>]</cite> algorithms operates on a dataset of ranked preferences, directly optimizing the model to prefer the more highly ranked output in each pair.</p>
</div>
<div class="ltx_para" id="S2.SS2.p7">
<p class="ltx_p" id="S2.SS2.p7.1">The primary advantage of RLHF is its ability to align the outputs of a pre-trained and fine-tuned model with human expectations, improving qualities such as coherence, relevance, and ethical compliance. This approach is particularly effective in mitigating undesirable behaviors, such as generating toxic, biased, or irrelevant content. Furthermore, RLHF enables the incorporation of domain-specific human expertise, allowing models to better serve niche applications.
However, RLHF introduces several challenges. First, the quality of human feedback is critical, poorly designed feedback mechanisms or misaligned human preferences can lead to suboptimal or even harmful model behavior. Second, RLHF requires significant resources for human annotation and computationally expensive RL training. Furthermore, over-optimization for the reward model can lead to undesirable artifacts, such as the model exploiting weaknesses in the reward system rather than genuinely improving its outputs—a phenomenon known as "reward hacking" <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib118" title="">118</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib119" title="">119</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p8">
<p class="ltx_p" id="S2.SS2.p8.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p8.1.1">Knowledge Distillation.</span> Knowledge Distillation (KD) has emerged as a key approach for efficient training and deploying LLMs by transferring the knowledge embedded in high-capacity teacher models to smaller, more efficient student models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib120" title="">120</a>]</cite>. In essence, the student model learns to mimic both the predictive outcomes and the internal representation patterns of the teacher, thereby significantly reducing computational costs and memory demands during the pre-training phase <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib121" title="">121</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib122" title="">122</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib123" title="">123</a>]</cite>. This methodology promotes the development of leaner LLMs without sacrificing their ability to perform complex language tasks.</p>
</div>
<div class="ltx_para" id="S2.SS2.p9">
<p class="ltx_p" id="S2.SS2.p9.1">Recent advancements in KD extend beyond final-output matching. Modern methods utilize established LLMs to generate not only predictions but also detailed reasoning steps, which are often referred to as chain-of-thought sequences or intermediate logic traces <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib124" title="">124</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib125" title="">125</a>]</cite>. These rich annotations can then be incorporated into the fine-tuning process, enabling the target LLM to acquire deeper problem-solving skills and enhance interpretability without extensive manual labeling. By integrating these reasoning pathways, KD no longer serves solely as a compression mechanism but also imparts advanced critical thinking and inference capabilities to newly trained models.
Moreover, recent work explores expanding KD to support specialized or domain-specific tasks where the established teacher models can guide the target LLM toward focusing on task-relevant knowledge, filtering out less pertinent information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib126" title="">126</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib127" title="">127</a>]</cite>. This approach helps produce models that are better aligned with their intended applications.
Additionally, a Bayesian perspective on KD has been introduced, offering a transparent interpretation of its statistical foundations and equipping the target model with robust uncertainty quantification capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib128" title="">128</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib129" title="">129</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p10">
<p class="ltx_p" id="S2.SS2.p10.1">The integration of pre-training, fine-tuning, KD, and RLHF represents a comprehensive training paradigm for LLMs. Pre-training serves as the foundation, equipping the model with general knowledge and linguistic capabilities through large-scale unsupervised learning. Fine-tuning adapts the model to specific tasks or domains, enhancing its performance in targeted applications. KD supports efficiency by enabling the transfer of knowledge from established teacher models to target models, while RLHF refines the model’s behavior to align with human preferences, ensuring outputs are both functionally accurate and socially acceptable. These stages are complementary and iterative. Insights gained during RLHF can inform improvements in fine-tuning datasets or methodologies, while advancements in fine-tuning and KD can enhance the quality of RLHF outcomes. Together, this pipeline not only ensures that LLMs are powerful and versatile but also makes them more usable and aligned with human-centered goals. This multi-stage training paradigm has been instrumental in the development of state-of-the-art models like OpenAI’s ChatGPT and Anthropic’s Claude, setting a benchmark for future advancements in the field.
These advancements include the release of both full-scale and lightweight versions, with KD often playing a role in optimizing the latter <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib130" title="">130</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Bioinformatics-Specific Datasets</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The rapid advancements in large language models have significantly propelled the development of bioinformatics by enabling more efficient data interpretation and knowledge extraction. LLMs excel in understanding, processing, and generating complex textual and numerical data, making them powerful tools for tasks such as sequence analysis, annotation, and predictive modeling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib131" title="">131</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib132" title="">132</a>]</cite>. Leveraging bioinformatics-specific datasets, LLMs can further refine their understanding to address domain-specific challenges, transforming raw data into tangible, interpretable forms that accelerate research and innovation.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p2.1.1">Question Answering (QA)</span> systems play a vital role in biomedicine, assisting with clinical decision support and powering medical chatbots. The development of robust QA systems relies heavily on diverse and well-curated datasets. Over the past decade, several biomedical QA datasets have been introduced, each targeting specific challenges and domains. For instance,, MedMCQA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib133" title="">133</a>]</cite> and MedQA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib134" title="">134</a>]</cite> focus on general medical knowledge, providing open-domain questions and multiple-choice answers derived from medical licensing and entrance exams. GeneTuring targets genomics-specific tasks, such as gene name conversion and nucleotide sequence alignment. Meanwhile, BioASQ<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib135" title="">135</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib136" title="">136</a>]</cite> and PubMedQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib137" title="">137</a>]</cite> incorporate supporting materials, such as PubMed articles, to answer domain-specific questions with formats ranging from yes/no to multi-class classifications. These datasets are crucial for benchmarking QA systems, as they provide domain-specific contexts and evaluation metrics that drive the development of more accurate and reliable models tailored to biomedical needs.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p3.1.1">Text Summarization (TS)</span> in the biomedical and healthcare is a critical application of natural language processing, enabling the condensation of complex medical texts into concise, informative summaries without compromising essential details. This task is particularly valuable in areas such as the summarization of the literature, the summarization of radiology reports, and the summarization of clinical notes. Among these, the summarization of radiology reports plays an essential role in transforming detailed imaging reports - including X-rays, CT scans, MRI scans, and ultrasounds - into easily understandable summaries. Datasets like MIMIC-CXR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib138" title="">138</a>]</cite> are instrumental in advancing this field, providing a large-scale resource with 473,057 chest X-ray images and 206,563 corresponding reports. Such data sets are essential for training and evaluating summarization models, offering domain-specific content and structured formats that drive improvements in both accuracy and reliability, ultimately enhancing clinical workflows and decision making.</p>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p4.1.1">Information Extraction (IE)</span> in biomedicine involves organizing unstructured text into structured formats through tasks like named entity recognition (NER) and relation extraction (RE). Robust IE systems rely on high-quality datasets for training and evaluation. For instance: datasets such as BC5CDR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib139" title="">139</a>]</cite>, NCBI-disease <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib140" title="">140</a>]</cite>, ChemProt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib141" title="">141</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib142" title="">142</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib143" title="">143</a>]</cite>, DDI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib144" title="">144</a>]</cite>, GAD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib145" title="">145</a>]</cite>, BC2GM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib146" title="">146</a>]</cite>, and JNLPBA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib147" title="">147</a>]</cite> have become benchmarks for NER and RE tasks, addressing challenges involving diseases, chemicals, genes, and other biomedical entities. These datasets are essential benchmarks for tackling real-world biomedical challenges, enabling the development of more accurate and generalizable models.</p>
</div>
<div class="ltx_para" id="S2.SS3.p5">
<p class="ltx_p" id="S2.SS3.p5.1">LLMs have also shown potential in various biomedical tasks like coreference resolution and text classification. The effectiveness of these applications often depends on the availability of high-quality datasets. For coreference resolution, datasets such as MEDSTRACT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib148" title="">148</a>]</cite>, FlySlip <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib149" title="">149</a>]</cite>, GENIA-MedCo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib150" title="">150</a>]</cite>, DrugNerAR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib151" title="">151</a>]</cite>, BioNLP-ST’11 COREF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib152" title="">152</a>]</cite>, HANAPIN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib153" title="">153</a>]</cite> and CRAFT-CR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib154" title="">154</a>]</cite> provide essential benchmarks for identifying links between mentions of the same entity in biomedical texts. Pre-trained models such as BioBERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib155" title="">155</a>]</cite> and SpanBERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib156" title="">156</a>]</cite> have achieved notable success in this domain. In text classification, datasets like HoC (comprising 1,580 manually annotated PubMed abstracts for multi-label classification of cancer hallmarks) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib157" title="">157</a>]</cite> have been pivotal.</p>
</div>
<div class="ltx_para" id="S2.SS3.p6">
<p class="ltx_p" id="S2.SS3.p6.1">In summary, the rapid progress in LLMs have transformed biomedical applications by improving data interpretation, knowledge extraction, and task automation. From question answering and text summarization to information extraction, LLMs have demonstrated their potential across a wide range of Bioinformatics-Specific tasks. Central to their success is the availability of high-quality, domain-specific datasets, which are indispensable for training, benchmarking, and refining these models to address real-world challenges. These datasets not only enhance the effectiveness of LLMs but also act as a driving force in advancing the field of bioinformatics and biomedicine. As the availability of diverse and richly annotated datasets continues to expand, they will fuel the integration of LLMs into increasingly complex and specialized applications. Looking to the future, combining Bioinformatics-Specific datasets with cutting-edge techniques promises to unlock groundbreaking solutions, enabling more precise, efficient, and scalable innovations that will shape the next generation of biomedical research and healthcare.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Model Evolution and Key Milestones</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">The evolution of LLMs in bioinformatics has marked a transformative journey. Initially developed for natural language processing tasks, these models, such as BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib1" title="">1</a>]</cite> and GPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib158" title="">158</a>]</cite>, have demonstrated remarkable potential in addressing challenges specific to the bioinformatics domain. Leveraging their ability to process and generate sequences, LLMs have been adapted for various biological data types, including DNA, RNA, proteins, and drug molecules <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib3" title="">3</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">In genomics, models like DNABERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib159" title="">159</a>]</cite> and GROVER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib160" title="">160</a>]</cite> are trained on DNA sequences to predict functional regions, such as promoters and enhancers, and analyze mutations. Similarly, transcriptomics benefits from models like SpliceBERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib161" title="">161</a>]</cite> and RNA-FM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib162" title="">162</a>]</cite>, which assist in understanding RNA splicing and secondary structure prediction. For proteomics, PPLMs like ProtTrans<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib163" title="">163</a>]</cite> and ProtGPT2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib164" title="">164</a>]</cite> enhance predictions related to protein structure, function, and interactions. These advances are made possible by the foundational transformer architecture, which excels at processing sequential data. Fine-tuning these pre-trained models for domain-specific tasks extends their utility to applications drug discovery, where SMILES representations of molecules and protein sequences are integrated to predict interactions and properties.</p>
</div>
<div class="ltx_para" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1">A notable breakthrough in bioinformatics has been the AlphaFold series, which has applied cutting-edge machine learning to solve protein structure prediction challenges. AlphaFold2 (AF2) revolutionized structural biology with its unprecedented accuracy in predicting protein structures based solely on amino acid sequences. Its attention-based deep learning architecture captured intricate protein folding patterns, surpassing traditional physics-based and homology-modeling methods. By leveraging evolutionary information through multiple sequence alignments (MSAs), AF2 provided reliable predictions even in the absence of experimental data, significantly reducing the time and costs associated with obtaining protein structural information, accelerating advancements in drug discovery and functional genomics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib165" title="">165</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS4.p4">
<p class="ltx_p" id="S2.SS4.p4.1">Building on AF2’s success, AlphaFold3 (AF3) introduced groundbreaking capabilities, particularly in modeling protein complexes, including protein-peptide interactions. Transitioning from individual protein structure predictions to multi-component biological assemblies, AF3 addressed challenges protein-protein docking and protein-peptide interaction modeling. Through its template-based (TB) and template-free (TF) approaches, further extended the versatility and impact of the AlphaFold series <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib166" title="">166</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS4.p5">
<p class="ltx_p" id="S2.SS4.p5.1">Key Features of AlphaFold3
Enhanced Accuracy in Complex Structures: AF3 excels in predicting protein-peptide complex structures, achieving a high percentage of accurate models in challenging scenarios; Innovative Template-Free Modeling: While maintaining strengths in template-based predictions, AF3 introduces powerful template-free algorithms that allow for diverse model generation with reliable accuracy, even in the absence of homologous structural data; Sophisticated Scoring and Ranking: AF3 integrates advanced scoring metrics such as DockQ and MolProbity, ensuring accurate evaluation of predicted structures. Its models show fewer issues like twisted peptides or cis non-proline residues, reflecting improved protein-like properties and geometric quality.</p>
</div>
<div class="ltx_para" id="S2.SS4.p6">
<p class="ltx_p" id="S2.SS4.p6.1">The progression from AF2 to AF3 reflects the iterative refinement of computational methods to address increasingly complex biological problems. While AF2 focused on individual protein structures, AF3 emphasizes dynamic interactions within biological systems, signaling a shift toward a more holistic understanding of molecular biology. These innovations underscore how machine learning continues to redefine bioinformatics, enabling accurate and efficient modeling of protein structures and interactions. The AlphaFold series exemplifies the potential for transformative breakthroughs in biology and medicine, paving the way for future applications in understanding complex biological systems.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Applications in Bioinformatics Problems</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">At the heart of LLMs lies the transformer architecture, which leverages an attention mechanism to manage word importance in context without the traditional constraints of recurrent (RNN) or convolutional (CNN) neural networks. The self-attention mechanism of transformers not only allows for robust parallelization and scalability but also excels at capturing long-range dependencies in text. In bioinformatics, the growing availability of extensive datasets across diverse tissues, species, and modalities presents both an opportunity and a challenge. Bioinformatics analysis typically seeks to uncover hidden relationships within vast amounts of data, which can be broadly categorized into two formats: molecular and cellular. Molecular data often consist of sequences—strings of four bases for DNA and RNA, and strings of twenty different amino acids for proteins. Cellular data, such as that from single-cell RNA-seq, single-cell ATAC-seq, or single-cell CITE-seq, typically takes the form of a count matrix with cells as rows and modalities as columns. While there are parallels between these data types and the structured data used in NLP, significant differences pose unique challenges for applying LLMs directly.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">A comprehensive LLM framework for bioinformatics involves three critical stages: data tokenization, model pre-training, and subsequent analyses. Due to the inherent differences between bioinformatics and conventional NLP data, researchers have been pioneering adaptations to the LLM architecture to better suit bioinformatics applications. The following section will provide a detailed overview of notable contributions in this evolving field.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Genome Level</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Genome data primarily provide molecular-level insights, focusing on the sequences of DNA and RNA. This format bears a strong resemblance to natural language, as it is structured as ordered sequences of strings. In this analogy, each nucleotide in a sequence read is akin to a character, each read is akin to a sentence, and the entire genome is comparable to the full article. To bridge the genome sequence and natural language, multiple studies try several ways to tokenize the genome sequence to make it similar to the concept of “word” in the natural language. To gain deeper insights into the functionalities of various genome segments, most studies apply the BERT (Bidirectional Encoder Representations from Transformers) as the core model, which excels in understanding the functions of a genome segment in relation to its surrounding genome region and is easily extended to different specific tasks by fine-tuning the model with specific dataset.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>LLM for DNA Analysis</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">In DNA analyses, biological sequences are encoded into structured tokens to facilitate effective model processing. A commonly adopted method involves tokenizing sequences into k-mers, typically ranging from 3 to 6 bases in length. This approach creates a vocabulary of k-mer permutations analogous to words in natural language, allowing the pre-trained model to decipher patterns within these k-mers. The choice of k directly affects the complexity and size of the resulting library, presenting a trade-off between modeling efficiency and accuracy.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.1">One of the pioneering methods, DNABERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib159" title="">159</a>]</cite>, tokenizes DNA sequence data using overlapping fixed-length k-mers, as well as the recently developed Nucleotide Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib167" title="">167</a>]</cite>. To enhance model efficiency, subsequent versions like DNABERT-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib168" title="">168</a>]</cite> and GROVER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib160" title="">160</a>]</cite> have employed Byte Pair Encoding (BPE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib79" title="">79</a>]</cite>, a statistical compression technique that iteratively merges the most frequently co-occurring genome segments. This method extends beyond fixed k-mer lengths, significantly improving the efficiency and generalizability of the models. HyenaDNA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib169" title="">169</a>]</cite> uses one-mer to tokenize the DNA sequence since it uses Hyena <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib170" title="">170</a>]</cite> as the core model, which allows much longer input than BERT. Additionally, some models integrate supplementary data into their tokenization process; for instance, DNAGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib171" title="">171</a>]</cite> incorporates species information, and MuLan-Methyl <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib172" title="">172</a>]</cite> combines sequence and taxonomy data into a natural language-like sentence to fully leverage existing LLM capabilities.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.p3.1">In terms of pre-training approaches, many models utilize the BERT architecture with a masked learning method (MLM) for self-supervised training. To boost training efficiency, DNABERT incorporates the AdamW optimizer with fixed weight decay and applies dropout to the output layer. DNABERT-2 introduces enhancements such as Attention with Linear Biases (ALiBi) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib173" title="">173</a>]</cite> and Flash Attention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib174" title="">174</a>]</cite>. In contrast, the MuLan-Methyl framework integrates five fine-tuned language models (BERT and four variants) for the joint identification of DNA methylation sites, maintaining consistency with their original pre-training setups. DNABERT-S <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib175" title="">175</a>]</cite> develops a contrastive learning-based method to help effectively cluster and separate different species. Some methods adopt other LLM models. For example, DNAGPT uses a GPT-based model and the next-token prediction for its pre-training, enabling it to forecast subsequent tokens based on previous ones. HyenaDNA uses Hyena, a new LLM model that allows a longer context input, to study long-range genomic sequence properties.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p4">
<p class="ltx_p" id="S3.SS1.SSS1.p4.1">When applying these models to specific bioinformatics tasks, most integrate additional task-relevant data for fine-tuning. For instance, DNABERT and its derivatives utilize the Eukaryotic Promoter Database (EPDnew) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib176" title="">176</a>]</cite> to predict gene promoters, the ENCODE database <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib177" title="">177</a>]</cite> for transcription factor binding site identification, and dbSNP for functional variant detection. MuLan-Methyl uses data from three main types of DNA methylation across multiple genomes for accurate predictions. Nucleotide Transformer includes multiple downstream tasks by fine-tuning the model with different datasets, like using histone ChIP-seq data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib177" title="">177</a>]</cite> for epigenetic marks prediction, using human enhancer elements data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib178" title="">178</a>]</cite> for enhancer sequence prediction, and using human annotated splice sites data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib179" title="">179</a>]</cite> for splice site prediction. DNAGPT leverages data on polyadenylation signals and translation initiation sites for genomic signal and region recognition. Moreover, due to the generative nature of GPT, DNAGPT can also generate artificial human genomes without additional fine-tuning data. Without further fine-tuning, some methods use the embedding from the model directly. DNABERT-S can be used for species clustering and classification.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>LLM for RNA Analysis</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">Unlike DNA, RNA analysis encompasses more complex and varied tasks, requiring tailored preprocessing strategies. RNABERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib180" title="">180</a>]</cite>, mirroring the structure of DNABERT, employs the k-mer method for tokenizing RNA sequences. Given the typically shorter sequences of RNA compared to DNA, other models like SpliceBERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib181" title="">181</a>]</cite>, RNA-MSM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib182" title="">182</a>]</cite>, and RNA-FM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib162" title="">162</a>]</cite> utilize single nucleotides (one-mers) for tokenization. In addition to sequence tokenization, these models often incorporate metadata during preprocessing. For instance, RNA-RBP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib183" title="">183</a>]</cite> labels each sequence as positive or negative based on the presence of an RNA-binding protein (RBP) region, while SpliceBERT similarly labels sequences for RNA-splicing sites. RNA-MSM enhances its input by including multiple sequence alignments (MSA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib184" title="">184</a>]</cite> to preserve the evolutionary history of sequences.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1">The pre-training approach for RNA largely follows that of DNA, utilizing BERT’s architecture and masked language modeling (MLM) for training. Specifically, RNA-MSM adopts a structure akin to AlphaFold2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib185" title="">185</a>]</cite>, leveraging an MSA-transformer architecture. Depending on the target application, models are pre-trained with different datasets: RNABERT and RNA-MSM use sequences from the Rfam database, RNA-FM utilizes non-coding RNA sequences from RNAcentral <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib186" title="">186</a>]</cite>, and SpliceBERT is pre-trained with RNA sequences from 72 vertebrates available on the UCSC Genome Browser <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib187" title="">187</a>]</cite>. BERT-RBP is trained using the eCLIP-seq dataset, which includes RBP information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib188" title="">188</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p3">
<p class="ltx_p" id="S3.SS1.SSS2.p3.1">Once trained, the BERT-based models process tokenized sequences to produce embeddings for each token. These embeddings are directly utilized in several applications; RNABERT employs them to classify RNAs from different families, while BERT-RBP uses them to predict RBP-binding sites. Furthermore, the attention maps generated as part of the model output play a critical role: SpliceBERT uses these maps to assess the impact of genetic variants on RNA splicing, BERT-RBP to analyze transcript region types and predict secondary structures, and RNA-MSM for secondary structure and solvent accessibility predictions.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p4">
<p class="ltx_p" id="S3.SS1.SSS2.p4.1">For task-specific enhancements, some models undergo fine-tuning with additional datasets. SpliceBERT, for example, is fine-tuned using a human Branchpoints dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib189" title="">189</a>]</cite> to predict BP sites and the Spliceator dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib190" title="">190</a>]</cite> to assess splice sites across species. RNA-FM is fine-tuned with the PDB dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib191" title="">191</a>]</cite> to facilitate RNA 3D structure reconstruction.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Gene Products Level</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">With advances in single-cell technologies, researchers have gained enhanced insights into the functional roles and regulatory mechanisms of gene products within individual cells <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib192" title="">192</a>]</cite>. Single-cell RNA sequencing (scRNA-seq) data, which records the expression levels of various genes across individual cells, is particularly instrumental. Typically presented in a count matrix format, scRNA-seq data contrasts with sequence data; it lacks a natural order and contains numerical values rather than sequences of strings. Researchers have explored various methods to adapt this data for compatibility with LLMs, adjusting the representation of scRNA-seq data to harness the power of LLM methodologies.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">To adapt scRNA-seq data for LLM compatibility, researchers have devised various strategies. Models like Cell2Sentence <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib193" title="">193</a>]</cite>, tGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib194" title="">194</a>]</cite>, and Geneformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib195" title="">195</a>]</cite> employ a ranked sequence of gene symbols by expression level as inputs. ScGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib196" title="">196</a>]</cite> and scBERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib197" title="">197</a>]</cite> discretize gene expressions and treat them as tokens. Additionally, scGPT incorporates metadata for position embedding, while scBERT leverages gene2vec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib198" title="">198</a>]</cite> to capture semantic similarities based on general co-expression.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Some methods utilize a transformer-based architecture, which accommodates non-discrete inputs more flexibly. CIForm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib199" title="">199</a>]</cite> segments the gene expression vector of each cell into equal-length sub-vectors or patches. TOCICA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib200" title="">200</a>]</cite> groups gene expression into patches representing specific pathways, and ScTransSort <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib201" title="">201</a>]</cite> employs CNNs to generate gene-embedding patches, transforming the expression matrix into multiple 2D square patches. TransCluster <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib202" title="">202</a>]</cite> uses linear discriminant analysis (LDA) to convert gene expression counts into embedding vectors.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">Unlike genome analyses, single-cell analyses adopt diverse model architectures for pre-training. For instance, Cell2Sentence, tGPT, and scGPT utilize GPT, whereas scBERT and Geneformer are based on BERT architecture. Transformer-based methods often integrate a linear classifier post-transformer and train a supervised model using cell types, as seen in CIForm, TOCICA, scTransSort, and TransCluster.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">The primary aim of scRNA LLM methodologies is to achieve accurate and generalized cell type annotations across various tissues and species. Supervised transformer-based methods use the pre-trained model directly for cell-type annotation. For instance, tGPT supports developmental lineage inference, and TOCICA enables interpretable dynamic trajectory analysis. LLM-based methods, post-pre-training, can be fine-tuned for specialized tasks or data-scarce scenarios. ScGPT is adaptable for tasks such as cell annotation, perturbation response prediction, batch effect correction, and gene regulatory network inference. Similarly, Geneformer can be fine-tuned to predict gene dosage sensitivity, chromatin dynamics, and gene network dynamics.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Epigenomics</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Decoding the information residing in the non-coding portion of the genome is one of the fundamental challenges in genomics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib203" title="">203</a>]</cite>. While substantial progress has been made in understanding the coding regions of the genome, non-coding regions remain poorly understood, particularly their roles in disrupting the regulatory syntax of DNA and their contributions to gene regulation. Existing LLMs, for example, Enformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib204" title="">204</a>]</cite>, which take DNA sequences as input and perform downstream tasks, face two critical limitations: they cannot predict the functions of sequences in different cellular contexts, and they fail to incorporate 3D chromatin interaction data.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">EpiGePT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib205" title="">205</a>]</cite> is a new LLM designed to overcome these challenges. It enables researchers to predict functionality in diverse cellular contexts and integrate 3D chromatin interaction data into genomic modeling. EpiGePT’s architecture consists of four key components: a sequence module that analyzes DNA sequences, a transcription factor (TF) module that encodes cellular contexts, a transformer module that examines long-range interactions between DNA regions, and a prediction module that outputs context-specific gene regulation insights. To predict function in novel cellular contexts, EpiGePT employs its TF module, which represents the expression and binding activities of hundreds of transcription factors as a context-specific vector. This vector is then combined with DNA sequence features, which are tokenized into genomic bins—each representing a segment of the DNA sequence. These tokens, enriched with both sequence and context-specific TF features, form the input to the model, ensuring it captures both the local sequence information and the cellular context. This approach allows the model to treat each genomic bin as a token with embedded positional and biological context, leveraging the self-attention mechanism in the transformer module to learn long-range interactions and context-specific functionality. EpiGePT also addresses the challenge of incorporating 3D chromatin interaction data, which is critical for understanding long-range gene regulation. It guides the self-attention mechanism of its transformer module using ground truth 3D interaction data, such as HiChIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib206" title="">206</a>]</cite> or Hi-C <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib207" title="">207</a>]</cite> loops. This alignment is achieved through a cosine similarity loss that adjusts the attention weights to reflect known 3D genomic interactions. By doing so, EpiGePT can model regulatory mechanisms, such as enhancer-promoter interactions, with higher fidelity than existing models.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Protein Level</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Mass spectrometry (MS)-based proteomics focuses on characterizing proteins within complex biological samples <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib208" title="">208</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib209" title="">209</a>]</cite>. Recent advancements in MS technology have enabled researchers to generate vast amounts of proteomics data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib210" title="">210</a>]</cite>. However, the rapid growth in data volume presents significant analytical challenges. To tackle these issues, Ding et al. introduced PROTEUS, an LLM-based tool designed for automating proteomics data analysis and hypothesis generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib211" title="">211</a>]</cite>. PROTEUS leverages a foundational LLM to integrate and coordinate existing bioinformatics tools, facilitating scientific discovery from raw proteomics data. Protein sequences share many similarities with natural language, and since breakthroughs have been achieved in applying NLP methods to protein sequence research, a variety of protein language models have emerged, differing in architecture, training strategies, and application scope <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib212" title="">212</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib213" title="">213</a>]</cite>. Here, we outline the main types of protein language models and downstream tasks, each tailored to address distinct bioinformatics challenges in protein modeling, structure prediction, and functional annotation.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Models for Protein LLM</h4>
<div class="ltx_para" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p1.1.1">Encoder-only models</span>, such as BERT-based models primarily designed for understanding protein sequences. These models excel in tasks that involve recognizing patterns within the sequences, making them suitable for protein classification, mutation effect prediction, and secondary structure analysis. Examples include ESM 1b <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib214" title="">214</a>]</cite>, ESM-1v <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib215" title="">215</a>]</cite>,
ProteinBert <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib216" title="">216</a>]</cite>, ProtTrans <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib217" title="">217</a>]</cite>, which leverage the bidirectional attention mechanisms of BERT to capture contextual relationships within amino acid sequences.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p2">
<p class="ltx_p" id="S3.SS4.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p2.1.1">Decoder-only models</span>, similar to the GPT family in NLP, focus on generating new sequences based on learned distributions. In protein research, these models can be applied to generate synthetic protein sequences with desired properties or to design novel proteins. Models like
ProGen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib218" title="">218</a>]</cite>,ProtGPT2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib219" title="">219</a>]</cite>,
ZymCTRL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib220" title="">220</a>]</cite>,
RITA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib221" title="">221</a>]</cite>,
IgLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib222" title="">222</a>]</cite>,
ProGen2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib223" title="">223</a>]</cite>,
and
PoET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib224" title="">224</a>]</cite> are notable for their ability to produce diverse protein sequences that exhibit specific biochemical functions. This category is instrumental in protein engineering and synthetic biology, where the generation of novel, functional proteins is crucial <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib212" title="">212</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p3">
<p class="ltx_p" id="S3.SS4.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p3.1.1">Encoder-decoder models</span> combine the strengths of both encoder-only and decoder-only architectures, making them highly adaptable to a range of protein-related tasks. They are particularly effective for sequence-to-sequence tasks, such as protein sequence alignment, where aligning amino acid sequences accurately is essential for understanding evolutionary relationships. These models can be fine-tuned for protein structure prediction or protein-protein interaction mapping, contributing to advancements in fields like drug discovery and disease diagnosis. The models include Fold2Seq <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib225" title="">225</a>]</cite>,
MSA2Prot <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib226" title="">226</a>]</cite>,
Sgarbossaetal <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib227" title="">227</a>]</cite>,
Leeetal <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib228" title="">228</a>]</cite>,
LM-Design <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib229" title="">229</a>]</cite>,
MSA-Augmenter <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib230" title="">230</a>]</cite>,
ProstT5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib231" title="">231</a>]</cite>,
xTrimoPGLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib232" title="">232</a>]</cite>,
SS-pLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib233" title="">233</a>]</cite>,
pAbT5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib234" title="">234</a>]</cite>,
ESM-GearNet-INR-MC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib235" title="">235</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p4">
<p class="ltx_p" id="S3.SS4.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p4.1.1">Multi-Modal Protein Models</span> integrate traditional protein language models with additional data types, such as structural and interaction information, to create powerful frameworks capable of analyzing both sequence and structural features simultaneously. By integrating textual protein sequences with structural annotations, these models enhance predictive capabilities for tasks such as 3D protein structure prediction, binding interaction analysis, and functional site identification. Frameworks like Multimodal Protein Representation Learning (MPRL) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib236" title="">236</a>]</cite> exemplify this approach by combining sequence information, 3D structural data, and functional annotations to capture the complex characteristics of proteins. For example, MPRL employs Evolutionary Scale Modeling (ESM-2) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib237" title="">237</a>]</cite> for sequence analysis, Variational Graph Autoencoders (VGAE) for residue-level graphs, and PointNet Autoencoders (PAE) for 3D point cloud representations. This comprehensive data integration preserves both spatial and evolutionary aspects of proteins, allowing the model to generalize effectively across tasks like protein–ligand binding affinity prediction and protein fold classification. Similarly, Models like ProtTrans <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib217" title="">217</a>]</cite> and ESM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib237" title="">237</a>]</cite> treat protein sequences as textual data, to learn rich embeddings that, when combined with 3D structural data, improve predictions of structure-function relationships. This multimodal synergy is essential for advancing protein engineering and drug discovery, mapping complex biological functions onto computational representations of proteins.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Downstream Tasks for Protein LLM</h4>
<div class="ltx_para" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1">Protein modeling, especially through deep learning approaches, addresses a variety of critical tasks in biological research and medicine. For instance, deep learning methods are extensively applied in predicting protein-protein interactions (PPIs), which are fundamental for cellular functions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib238" title="">238</a>]</cite>. This prediction aids in understanding disease mechanisms, drug-target interactions, and the structural features of proteins that contribute to complex molecular pathways. The prediction of PPIs also enables the identification of novel therapeutic targets, providing significant insights for drug discovery and design. The typical models include AlphaFold <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib239" title="">239</a>]</cite>, AlphaFold 2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib240" title="">240</a>]</cite>, AlphaFold 3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib166" title="">166</a>]</cite>, Graph-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib241" title="">241</a>]</cite>, MARPPI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib242" title="">242</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p2">
<p class="ltx_p" id="S3.SS4.SSS2.p2.1">Large-scale models also excel in predicting protein post-translational modifications (PTMs), which play essential roles in regulating protein function, stability, and cellular signaling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib243" title="">243</a>]</cite>. Various machine learning models, including those based on transformers and neural networks, have been adapted to predict PTM sites with improved accuracy. For instance, the PTMGPT2 model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib244" title="">244</a>]</cite>, developed by fine-tuning a GPT-2 architecture, leverages prompt-based approaches to identify subtle sequence motifs that correspond to PTM sites across diverse types <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib245" title="">245</a>]</cite>. By using custom tokens in its prompt, PTMGPT2 effectively captures sequence context and improves prediction accuracy, making it useful for identifying disease-associated mutations and potential drug targets.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p3">
<p class="ltx_p" id="S3.SS4.SSS2.p3.1">Additionally, protein structure prediction remains a pivotal task in computational biology. It involves understanding how proteins fold and how their structures determine functions. Advanced models, like those using transformer architectures, facilitate the accurate prediction of protein structures, providing crucial information for synthetic biology, enzyme design, and therapeutic protein engineering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib246" title="">246</a>]</cite>. These methods enable scientists to predict protein folding patterns and design novel proteins with specific functions, potentially revolutionizing fields like drug discovery and synthetic biology.The typical models include AlphaFold <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib239" title="">239</a>]</cite>, AlphaFold 2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib240" title="">240</a>]</cite>, AlphaFold 3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib166" title="">166</a>]</cite>, ColabFold <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib247" title="">247</a>]</cite>, Eigenfold <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib248" title="">248</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p4">
<p class="ltx_p" id="S3.SS4.SSS2.p4.1">The development of protein large language models (Prot-LLMs) relies on diverse datasets that capture the complexity of protein sequences and functions. These datasets typically include unlabeled data for unsupervised pre-training, such as protein sequences from repositories like UniProt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib249" title="">249</a>]</cite>, AlphaFoldDB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib250" title="">250</a>]</cite> which houses millions of protein sequences across species. For fine-tuning and evaluation, labeled datasets focus on specific protein characteristics, such as structure, function, and interactions. Examples include datasets for secondary structure prediction, protein-protein interaction networks, and specific post-translational modification (PTM) sites <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib251" title="">251</a>]</cite>. These labeled datasets enable Prot-LLMs to perform tasks like function annotation, PTM prediction, and protein structure modeling.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Metabolomics</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">Metabolomics represents the comprehensive analysis of the complete set of small-molecule metabolites within a biological system, providing a snapshot of the cellular biochemical status at a given time. This omics discipline is pivotal in elucidating the dynamic interactions between genotype and phenotype, as metabolites are the end-products of cellular processes and are directly involved in the regulation of biological functions. Metabolomics has emerged as a powerful tool in various areas of biological and medical research, including the identification of biomarkers for disease diagnosis, prognosis, and therapeutic monitoring <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib3" title="">3</a>]</cite>, as well as the elucidation of molecular mechanisms underlying disease pathogenesis. The integration of LLMs into metabolomics offers transformative potential for analyzing and interpreting metabolomic data. With their capacity to process vast amounts of textual and numerical information, LLMs, particularly transformer-based models adapted for biological data, have shown promise in metabolite identification and pathway analysis.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.1 </span>Data Integration and Interpretation</h4>
<div class="ltx_para" id="S3.SS5.SSS1.p1">
<p class="ltx_p" id="S3.SS5.SSS1.p1.1">One of the most significant challenges in metabolomics is the integration and interpretation of large, complex datasets. LLMs can facilitate the integration of metabolomic data with other omics data (e.g., genomics, transcriptomics, proteomics) and clinical data, a challenge increasingly addressed by dynamic modeling approaches to enhance our understanding of metabolic phenotypes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib252" title="">252</a>]</cite>. By processing and analyzing these multi-omics datasets, LLMs can identify patterns and correlations that may not be apparent through traditional statistical methods. For instance, LLMs can be trained to predict the biological pathways and processes associated with specific metabolite profiles, thereby providing insights into the molecular mechanisms of disease.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS1.p2">
<p class="ltx_p" id="S3.SS5.SSS1.p2.1">Recent advances in multi-modal LLM architectures have addressed key challenges in data integration. The development of cross-attention mechanisms specifically designed for metabolomic data has improved the ability to handle heterogeneous data types. These mechanisms allow for simultaneous processing of spectral data, chemical structures, and biological annotations. However, significant challenges remain in handling the high dimensionality and sparsity of metabolomic data. Novel approaches incorporating dimensionality reduction techniques and attention-based feature selection have shown promise in managing these challenges while maintaining biological relevance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.2 </span>Biomarker Discovery and Validation</h4>
<div class="ltx_para" id="S3.SS5.SSS2.p1">
<p class="ltx_p" id="S3.SS5.SSS2.p1.1">The identification of robust biomarkers is a critical aspect of metabolomics, with applications in disease diagnosis, prognosis, and therapeutic monitoring. LLMs can be employed to analyze large datasets from clinical trials and cohort studies to identify potential biomarkers associated with specific disease states. Integrated deep learning frameworks have addressed challenges such as matching uncertainty and metabolite identification, enabling more reliable biomarker discovery and validation through the integration of diverse data sources <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib253" title="">253</a>]</cite>. This can lead to the development of more accurate and reliable biomarker panels for clinical use.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS2.p2">
<p class="ltx_p" id="S3.SS5.SSS2.p2.1">The validation of metabolomic biomarkers presents unique challenges that LLMs are increasingly equipped to address. Recent developments in uncertainty quantification for LLMs have improved the reliability of biomarker predictions. Statistical frameworks incorporating false discovery rate control and multiple hypothesis testing have been integrated into LLM-based biomarker discovery pipelines. Furthermore, the development of interpretable deep learning architectures has enhanced our ability to understand the biological mechanisms underlying identified biomarkers, leading to more robust validation processes.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS5.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.3 </span>Metabolic Pathway Analysis and Drug Discovery</h4>
<div class="ltx_para" id="S3.SS5.SSS3.p1">
<p class="ltx_p" id="S3.SS5.SSS3.p1.1">Metabolomics data can provide valuable insights into the perturbations of metabolic pathways in disease states. LLMs exhibit remarkable capabilities in analyzing biological data, such as genomic sequences and protein structures, making them instrumental in identifying druggable targets and novel therapeutic compounds <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib5" title="">5</a>]</cite>. For example, LLMs can be trained to predict the effects of gene variants on enzyme activity and metabolic fluxes, thereby aiding in the identification of druggable targets. Additionally, LLMs can be used in the discovery of novel therapeutic compounds by predicting the binding affinity of small molecules to metabolic enzymes and pathways.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS3.p2">
<p class="ltx_p" id="S3.SS5.SSS3.p2.1">Advanced graph neural network architectures have emerged as powerful tools for metabolic pathway analysis when integrated with LLMs. These hybrid approaches can capture both the topological structure of metabolic networks and the chemical properties of individual metabolites. Recent developments in attention-based graph neural networks have improved our ability to predict metabolic flux distributions and identify regulatory bottlenecks. The integration of molecular docking simulations with LLM-based predictions has enhanced the accuracy of drug-target interaction predictions in metabolic pathways.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS5.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.4 </span>Personalized Medicine</h4>
<div class="ltx_para" id="S3.SS5.SSS4.p1">
<p class="ltx_p" id="S3.SS5.SSS4.p1.1">The application of metabolomics in personalized medicine is rapidly gaining momentum, with the potential to tailor treatments to individual patients based on their metabolic profiles. LLMs can play a crucial role in this context by analyzing patient-specific metabolomic data in conjunction with genomic, proteomic, and clinical data to develop personalized treatment plans. For instance, LLMs can be used to predict the response of individual patients to specific therapies based on their metabolic profiles, thereby enabling the selection of the most effective treatment options.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS5.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.5 </span>Literature Mining and Knowledge Discovery</h4>
<div class="ltx_para" id="S3.SS5.SSS5.p1">
<p class="ltx_p" id="S3.SS5.SSS5.p1.1">The vast amount of published literature in the field of metabolomics presents both an opportunity and a challenge for researchers. LLMs can be employed to mine this literature for relevant information, such as the identification of novel metabolites, the characterization of metabolic pathways, and the discovery of new biomarkers, addressing the challenge of synthesizing metabolomics research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib254" title="">254</a>]</cite>. By processing and analyzing textual data from scientific articles, LLMs can generate hypotheses and identify trends that may guide future research directions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS5.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.6 </span>Quality Control and Data Standardization</h4>
<div class="ltx_para" id="S3.SS5.SSS6.p1">
<p class="ltx_p" id="S3.SS5.SSS6.p1.1">The reproducibility and comparability of metabolomics data are critical for the advancement of the field. Tools like the LargeMetabo package facilitate the reproducibility and standardization of large-scale metabolomics datasets, ensuring consistency across studies <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">bbac455</span>]</cite>. LLMs can be used to standardize metabolomics data by identifying and correcting inconsistencies in data annotation, nomenclature, and reporting. Additionally, LLMs can assist in the development of quality control metrics and standards for metabolomics experiments, thereby improving the reliability and comparability of metabolomics data across different studies and platforms.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS5.SSS7">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.7 </span>Predictive Modeling and Simulation</h4>
<div class="ltx_para" id="S3.SS5.SSS7.p1">
<p class="ltx_p" id="S3.SS5.SSS7.p1.1">LLMs can be integrated with machine learning models to develop predictive models of metabolic pathways and networks. Advanced multivariate models, including machine learning techniques, have shown efficacy in analyzing metabolomics data to uncover predictive patterns of metabolic pathways <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib255" title="">255</a>]</cite>. These models can be used to simulate the effects of genetic, environmental, and pharmacological perturbations on metabolic processes, thereby providing insights into the molecular mechanisms of disease and the potential outcomes of therapeutic interventions. Furthermore, LLMs can be used to predict the outcomes of metabolic engineering strategies in synthetic biology applications, such as the optimization of metabolic pathways for the production of biofuels, pharmaceuticals, and other valuable chemicals.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS7.p2">
<p class="ltx_p" id="S3.SS5.SSS7.p2.1">The integration of LLMs into metabolomics represents a significant advancement in the field, with the potential to enhance data analysis, interpretation, and knowledge discovery. By leveraging the power of LLMs, researchers can unlock the full potential of metabolomics data, leading to new insights into disease mechanisms, the development of novel therapeutic strategies, and the advancement of personalized medicine. As LLMs continue to evolve, their applications in metabolomics are expected to expand, further accelerating the pace of discovery and innovation in this exciting field.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span> Disease-Specific Bio-medical Applications</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The application of LLM technology to medical-related bioinformatics data offers significant potential to enhance various downstream biomedical tasks (Figure  <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Large Language Models for Bioinformatics"><span class="ltx_text ltx_ref_tag">1</span></a>(c)).</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Brain Aging and Brain Disease</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Large Language Models are transforming the study and management of brain diseases by enabling innovative approaches to diagnosis, treatment, and knowledge discovery. These models excel in processing diverse data types including clinical notes, imaging studies, biological sequences, and brain signals, unlocking new possibilities for identifying disease patterns, predicting progression, and personalizing care. This section highlights the diverse applications of LLMs in brain diseases, focusing on three critical areas: clinical diagnostic support, therapeutic assistance, and information driven decision-making. Through these contributions, LLMs address longstanding challenges in managing complex neurological conditions, offering scalable and non-invasive solutions that enhance both research and clinical practice.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Clinical Diagnosis Support</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">Accurate and timely diagnosis is the foundation of effective medical care, particularly in complex and progressive conditions such as neurodegenerative diseases. The emergence of LLMs in healthcare offers transformative potential in clinical diagnostics by leveraging their advanced capabilities in processing diverse forms of unstructured data. From textual data to biological sequences and brain signals, LLMs excel at identifying patterns, extracting clinically relevant information, and supporting decision-making. Additionally, their ability to integrate multimodal data
has shown promise in improving diagnostic accuracy. This section explores how LLMs are applied to various data types crossing different brain diseases, highlighting their unique advantages and current challenges in clinical diagnosis.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p2.1.1">Textual data—Biomedical text</span> LLMs are increasingly applied to the analysis of biomedical textual data, including literature and electronic health records (EHRs). This form of biomedical textual data closely mirrors the fundamental structure of large language models. LLMs can identify significant insights within medical reports, enhancing diagnostic accuracy. In brain disease research, LLMs have been leveraged to diagnose conditions like seizures, Alzheimer’s disease (AD), headaches, strokes, Parkinson’s disease, and other neurodegenerative disorders using textual data from clinical notes, MRI reports, and neuropathological records.
For AD, LLMs provide a non-invasive, cost-effective, and scalable solution by analyzing unstructured data within EHRs. For example, Mao et al. demonstrated that the LLM can accurately predict MCI to AD progression using clinical notes as the early detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib256" title="">256</a>]</cite>. Feng et al. utilized LLMs to embed textual data in alignment with imaging data, significantly enhancing AD diagnosis through a multimodal approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib257" title="">257</a>]</cite>.
Beyond AD, LLMs have also shown promise in managing epilepsy, with studies successfully classifying seizure-free patients and extracting seizure frequency and other critical information from clinical notes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib258" title="">258</a>]</cite>. Additionally, in a study analyzing neurodegenerative disorders at the Mayo Clinic, diagnostic accuracies of 76%, 84%, and 76% were achieved using ChatGPT-3.5, ChatGPT-4, and Google Bard, respectively, underscoring the potential of LLMs in generating differential diagnoses for complex neuropathological cases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib259" title="">259</a>]</cite>.
EHRs also include detailed MRI reports, which are critical in neurological diagnoses. Bastien Le Guellec et al. evaluated the performance of LLMs in extracting information from real-world emergency MRI reports, demonstrating high accuracy without requiring additional training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib260" title="">260</a>]</cite>. Similarly, Kanazawa et al. showed that a fine-tuned LLM could classify MRI reports such as no brain tumor, post-treatment brain tumor, and pre-treatment brain tumor with accuracy comparable to human readers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib261" title="">261</a>]</cite>. These results highlight the growing importance of LLMs in processing MRI reports, which are essential components of EHRs, further enhancing their utility in brain disease diagnosis and management.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p3.1.1">Textual data—Transcription text</span> In addition to text-based data, transcriptions from speech data are increasingly valuable for diagnosing brain diseases that impair linguistic abilities. Patients with AD, for example, often exhibit distinct speech patterns when describing images, including word-finding difficulties, grammatical errors, repetitive language, and incoherent narratives. The ADReSS Challenge dataset inspired the research community to develop automated methods to analyze speech, acoustic, and linguistic patterns in individuals to detect cognitive changes, frequently used in such studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib262" title="">262</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib263" title="">263</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib264" title="">264</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib265" title="">265</a>]</cite>. LLMs outperform traditional methods like SVM, and Random Forest in this context. The existing work also shows that the combination of acoustic features with linguistic features for a multi-model can improve the performance, The maximum accuracy obtained by the acoustic feature is 64.5%, and the BERT Model provides a classification accuracy of 79.1% over the test dataset, the fusion of the acoustic model with the BERT Model shows an improvement of 6.1% classification accuracy over the BERT Model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib263" title="">263</a>]</cite>.
Linguistic analysis is also pivotal in diagnosing aphasia, a disorder commonly caused by left-hemisphere strokes. Chong et al. evaluated the clinical efficacy of LLM surprisals in a study where post-stroke aphasia patients narrated the story of Cinderella after reviewing a wordless picture book. The approach revealed significant potential for quantifying deficits and improving aphasia discourse assessment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib266" title="">266</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p4">
<p class="ltx_p" id="S4.SS1.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p4.1.1">Textual data—Text generation</span> In addition to biomedical text and speech data, recent advancements in text generation have further showcased the potential of large language models in clinical applications. Studies indicate that LLM-generated summaries are often preferred over those produced by human experts across various domains, including radiology reports, patient inquiries, progress notes, and doctor-patient dialogues <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib267" title="">267</a>]</cite>. This demonstrates the capacity of LLMs to synthesize complex clinical information effectively. Techniques such as Chain-of-Thought (CoT) prompting and text classification have been introduced to improve the confidence and precision of LLM outputs.
For example, when applied to neurologic cases, GPT-4 has shown promising results. By analyzing history and neurologic physical examination (H&amp;P) data from acute stroke cases, GPT-4 accurately localized lesions to specific brain regions and identified their size and number. This was achieved through Zero-Shot Chain-of-Thought and Text Classification prompting, highlighting the model’s potential for advanced neuroanatomical reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib268" title="">268</a>]</cite>. Similarly, in AD diagnostics, prompting LLMs with Clinical Chain-of-Thought frameworks has enabled them to generate detailed diagnostic rationales, demonstrating their ability to support reasoning-aware diagnostic frameworks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib269" title="">269</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p5">
<p class="ltx_p" id="S4.SS1.SSS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p5.1.1">Biological sequences</span> The process of DNA transcription to RNA, followed by translation into proteins, is fundamental to life and is often referred to as the Central Dogma of molecular biology. Many brain diseases, including AD, Parkinson’s disease (PD), autism spectrum disorder (ASD), and frontotemporal dementia (FTD), are closely associated with abnormalities in DNA, RNA, or protein sequences. To investigate the genetic and molecular mechanisms underlying these diseases, approaches such as Genome-Wide Association Studies (GWAS), transcriptome analysis, and proteomic profiling have been widely utilized. However, traditional methods often struggle to interpret the complex patterns present in these large-scale datasets. LLMs, with their advanced capabilities in processing sequential data, offer a transformative approach for analyzing biological sequences, enabling deeper insights into disease mechanisms and potential therapeutic targets. Several innovative LLMs have been developed for biological sequences. For DNA, models like Enformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib204" title="">204</a>]</cite>, Nucleotide Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib167" title="">167</a>]</cite>, and DNABERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib159" title="">159</a>]</cite> have shown significant promise. For RNA, RNABERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib270" title="">270</a>]</cite>, RNAFM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib162" title="">162</a>]</cite>, and RNA-MSM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib162" title="">162</a>]</cite> focus on structural inference and functional predictions. For proteins, models like ProteinBERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib216" title="">216</a>]</cite>, ESM-1b <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib214" title="">214</a>]</cite>, and ProtST <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib271" title="">271</a>]</cite> have demonstrated capabilities in understanding sequence-function relationships. Despite these advances, the application of LLMs to reveal relationships between abnormalities in biological sequences and specific brain diseases remains limited. Notable exceptions include epiBrainLLM, proposed by Liu et al., which extracts genomic features from personal DNA sequences using a retained LLM framework and combines these features to enhance diagnosis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib272" title="">272</a>]</cite>. This approach provides valuable insights into the causal pathways linking genotypes to brain measures and AD-related phenotypes. Another study utilized LLMs to predict protein phase transitions (PPTs) such as amyloid aggregation, a key pathological feature of age-related diseases like AD, demonstrating the potential of LLMs in advancing molecular-level understanding of neurodegenerative disorders <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib273" title="">273</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p6">
<p class="ltx_p" id="S4.SS1.SSS1.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p6.1.1">Brain signal</span> Brain signal data, including sMRI, fMRI, and EEG, is critical for diagnosing and understanding various brain diseases. Abnormalities in these signals are key diagnostic indicators for conditions such as epilepsy, ADHD, and mental health disorders.
For epilepsy, EEG abnormalities such as seizures, spikes, and slowing patterns are widely used for diagnosis. A fine-tuned LLM, named EEG-GPT, was developed for classifying EEG signals as normal or abnormal, showing strong performance in identifying these patterns <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib274" title="">274</a>]</cite>. Similarly, Liu et al. leveraged LLMs to guide affinity learning for rs-fMRI, enabling comprehensive brain function representation and improved diagnostic accuracy for brain diseases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib275" title="">275</a>]</cite>.
All of the LLM models above are based on the transformer architecture. Due to the long-range dependencies and temporal resolution in brain signals, Mamba-based LLM also show its potential in this field. Behrouz and Hashemi proposed BrainMamba, an efficient encoder for modeling spatio-temporal dependencies in multivariate brain signals. It combines a time-series encoder for brain signals and a graph encoder for spatial relationships, making it versatile for neuroimaging data. With a selective state space model design, BrainMamba achieves linear time complexity, enabling training on large-scale datasets. Evaluations on seven real datasets across three modalities (fMRI, MEG, EEG) and tasks such as seizure, ADHD, and mental state detection show that BrainMamba outperforms baselines with lower time and memory requirements <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib276" title="">276</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Therapeutic Assistance</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">LLMs have demonstrated a strong capability to engage in conversations on daily life topics, personal matters, and specific concerns. When fine-tuned to provide empathetic and understanding responses, they hold significant potential as tools for companionship and emotional support. This capability is particularly valuable for individuals with dementia (PwD), who often experience social isolation. Research indicates that social isolation is strongly linked to an increased risk of developing dementia later in life <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib277" title="">277</a>]</cite>. Addressing social isolation plays a vital role in mitigating cognitive decline among the elderly. Recent studies have explored the potential of LLMs to alleviate social isolation and provide therapeutic support. For example, Qi demonstrated that ChatGPT effectively reduces feelings of loneliness among older adults with mild cognitive impairment (MCI) by offering conversational engagement and cognitive stimulation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib278" title="">278</a>]</cite>. Similarly, Raile highlighted the dual role of ChatGPT as a complement to psychotherapy and an accessible entry point for individuals with mental health concerns who have yet to seek professional help <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib279" title="">279</a>]</cite>. These findings suggest that LLMs can serve as valuable tools to support mental health and cognitive functioning in vulnerable populations.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1">In the context of neurodegenerative diseases, wearable devices integrated with AI technologies offer promising avenues for continuous monitoring and personalized care. Mohammed and Venkataraman introduced an AI-powered wearable device that leveraging LLMs to monitor the daily activities of patients with PD by analyzing multimodal data such as tremors, movements, and posture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib280" title="">280</a>]</cite>. This approach enables real-time and personalized assessments of disease progression, potentially enhancing patient care and quality of life.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p3">
<p class="ltx_p" id="S4.SS1.SSS2.p3.1">Language impairments, such as aphasia, present significant challenges in communication. Manir et al. utilized BERT models to predict and complete sentences for individuals with aphasia, thereby improving the accuracy of speech prediction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib281" title="">281</a>]</cite>. This approach benefits caregivers and speech therapists by facilitating more effective communication strategies and supporting rehabilitation efforts.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p4">
<p class="ltx_p" id="S4.SS1.SSS2.p4.1">Brain-computer interfaces (BCIs) further exemplify the integration of advanced AI techniques into healthcare. Over recent decades, BCIs have provided novel solutions for various neurodegenerative disorders, including AD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib282" title="">282</a>]</cite> and PD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib283" title="">283</a>]</cite>. The incorporation of advanced AI algorithms, such as machine learning and deep learning, has significantly enhanced BCI performance, improving neuroergonomic systems, human-robot interactions, and robotic-assisted surgeries <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib284" title="">284</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib285" title="">285</a>]</cite>. Notably, integrating LLMs with BCIs introduces unique opportunities, such as reliably comprehending users’ emotional states to create emotionally aware conversational agents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib286" title="">286</a>]</cite> and decoding attempted speech from the brain activity of paralyzed patients <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib287" title="">287</a>]</cite>. These advancements highlight the transformative potential of LLMs in facilitating communication and enhancing the quality of life for individuals with severe disabilities.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p5">
<p class="ltx_p" id="S4.SS1.SSS2.p5.1">Collectively, these studies underscore the versatile applications of LLMs as therapeutic assistance in brain diseases. By enhancing social interaction, providing cognitive support, enabling continuous monitoring, and assisting in communication, LLMs represent a promising avenue for improving patient outcomes and overall quality of life.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Information Driven
Decision-making</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">LLMs have proven to be valuable tools for information retrieval, serving as vast repositories of knowledge. Saeidnia et al. reported that dementia caregivers expressed positive feedback on ChatGPT’s responses to non-clinical questions related to the daily lives of individuals with dementia <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib288" title="">288</a>]</cite>. This suggests that LLMs can support caregivers by providing accessible and practical information to manage everyday challenges. However, concerns remain about the depth and accuracy of medical information provided by LLMs. Studies comparing ChatGPT with traditional search engines have found limitations in the quality of responses, describing them as accurate but lacking in comprehensiveness <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib289" title="">289</a>]</cite>. These findings suggest that while LLMs can address basic queries, their applicability in complex medical contexts requires further refinement.
One solution to these limitations is fine-tuning LLMs using domain-specific data. For example, models trained on medical journals and textbooks have demonstrated improved performance in handling specialized medical queries <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib290" title="">290</a>]</cite>. In Alzheimer’s research, GPT-4-based tools have been developed to autonomously collect, process, and analyze health information, illustrating how customization can enhance the relevance and precision of information retrieval in specific medical domains <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib15" title="">15</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Cancer Treated by Radiation Therapy</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">LLMs have emerged as powerful tools in cancer research, offering innovative solutions for diagnosis, treatment planning, and biological insights. By processing vast datasets of scientific literature, clinical trial results, and genomic information, LLMs can facilitate the identification of novel biomarkers and treatment strategies. LLM-driven multimodal approaches have also enhanced target volume contouring in radiation oncology, integrating imaging data with clinical notes for improved precision<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib291" title="">291</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib292" title="">292</a>]</cite>. In radiobiology, these models contribute to understanding the complex interplay between radiation and cellular processes, informing the development of personalized treatment regimens<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib293" title="">293</a>]</cite>. Recent studies also explore the application of LLMs across chemotherapy, surgery, radiotherapy, and immunotherapy, demonstrating their versatility and potential in advancing oncology research.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Multimodal large language models (MLLMs) that integrate imaging analysis with natural language processing have shown promising results in automated organ-at-risk (OAR) and target volume delineation, achieving expert-level performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib294" title="">294</a>]</cite>. These models can process multiple imaging modalities—CT, MRI, and PET—simultaneously while incorporating clinical notes and radiology reports to improve contour accuracy. Additionally, LLMs are being utilized for dose prediction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib295" title="">295</a>]</cite>, where they have the potential to suggest optimal dose distributions for patients. Recent studies have explored their application in adaptive radiotherapy, where LLMs show potential in processing daily imaging data to recommend plan adaptations based on anatomical changes. Integrating LLMs with knowledge-based planning systems has also enhanced the quality of treatment plans by leveraging insights from large databases of previously treated cases. Furthermore, LLMs demonstrate potential in predicting treatment outcomes and toxicity risks by analyzing patient-specific factors, enabling more personalized treatment approaches.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">In clinical practice, LLMs are proving useful in automating routine tasks and supporting complex decision-making<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib296" title="">296</a>]</cite>. Tools like ChatGPT have been piloted for generating comprehensive patient case reports, improving the efficiency of clinical documentation. Furthermore, LLMs have shown promise in extracting discrete data elements from clinical notes, aiding in the creation of robust cancer databases. They were evaluated for supporting personalized oncology by recommending clinical trials for head and neck cancer and offering decision support for treatment planning. However, these applications require rigorous validation to ensure the accuracy and reliability of their outputs.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">In education, LLMs are transforming how knowledge is disseminated and acquired in oncology. Educational chatbots tailored to radiation oncology can simulate patient interactions, helping trainees refine their communication skills<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib297" title="">297</a>]</cite>. Additionally, LLMs assist in evaluating radiotherapy plans and providing structured feedback, as demonstrated by recent studies. These models foster a more interactive and adaptive learning environment, enabling personalized educational experiences for medical physicists, oncologists, and other healthcare professionals<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib298" title="">298</a>]</cite>. Despite challenges such as ensuring content accuracy and avoiding the propagation of biases, the integration of LLMs into educational frameworks holds the potential to enhance competency and foster innovation in cancer care.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Infectious Diseases</h3>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Disease Prediction and Vaccine Efficacy Analysis</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">LLMs such as GPT-3 and GPT-4, have emerged as powerful tools in disease prediction and vaccine efficacy analysis. By processing vast datasets, including biomedical records and epidemiological trends, LLMs can model the spread of infectious diseases, predict vaccination outcomes, and assist in assessing vaccine effectiveness. For example, neural networks combined with logistic regression have been applied to predict influenza vaccination outcomes, achieving significant accuracy based on demographic and clinical data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib299" title="">299</a>]</cite>. In the context of pediatric respiratory diseases, ChatGPT has been used to generate insights and recommendations for reducing severe cases post-COVID-19, highlighting the adaptability of LLMs in addressing real-world healthcare issues <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib300" title="">300</a>]</cite>. Additionally, machine learning algorithms based on clinical features have been validated for predicting influenza infection in patients with influenza-like illness (ILI), illustrating the role of LLMs in early diagnosis and targeted intervention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib301" title="">301</a>]</cite>. LLMs are also instrumental in identifying immune biomarkers that predict vaccine responsiveness, as seen in studies exploring apoptosis and other immune markers to assess influenza vaccine efficacy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib302" title="">302</a>]</cite>. Furthermore, LLMs have been applied to the extraction and analysis of post-marketing adverse events from the Vaccine Adverse Event Reporting System (VAERS), providing valuable insights into vaccine safety and public health implications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib303" title="">303</a>]</cite>. The use of machine learning for seasonal antigenic prediction, particularly for influenza A H3N2, demonstrates LLMs’ potential in tracking viral evolution and optimizing vaccine design to address emerging strains <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib304" title="">304</a>]</cite>. As LLM technology continues to advance, its application in disease prediction and vaccine efficacy is expected to become increasingly essential in public health management and disease prevention strategies.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Vaccine Adherence and Risk Prediction</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">Machine learning and feature selection techniques, facilitated by LLMs, are essential in analyzing vaccine adherence patterns and identifying factors influencing vaccination rates. These methods allow researchers to process large, complex datasets, uncovering demographic and health-related variables that impact vaccine adherence and risk prediction. For example, machine learning models have been applied to assess low adherence to influenza vaccination among adults with cardiovascular disease, offering insights into the unique barriers to vaccination faced by high-risk groups <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib305" title="">305</a>]</cite>. Real-time data from online self-reports, such as social media posts, have also been used to track influenza vaccine uptake, providing valuable insights into public sentiment and adherence trends <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib306" title="">306</a>]</cite>. Furthermore, sociodemographic predictors of vaccine acceptance, especially during the COVID-19 pandemic, have been studied extensively. For instance, machine learning has been used to explore the influence of variables like education level, income, and geographic location on vaccine hesitancy across various populations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib307" title="">307</a>]</cite>. In addition, validated scales such as the Parental Attitude about Childhood Vaccination Scale have been enhanced with feature selection techniques, refining our understanding of factors associated with vaccine acceptance and hesitancy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib308" title="">308</a>]</cite>. Other studies emphasize the broader implications of vaccine hesitancy by analyzing attitudes toward COVID-19 vaccinations across continents, highlighting the variability in hesitancy due to cultural and regional factors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib309" title="">309</a>]</cite>. Lastly, comparative studies on flu vaccine uptake pre- and post-COVID-19 leverage machine learning to identify shifts in adherence patterns and factors that predict vaccination behavior over time <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib310" title="">310</a>]</cite>. Together, these advancements in machine learning and feature selection provide a comprehensive understanding of vaccine adherence, informing targeted public health strategies to improve vaccination rates.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Biomarker Analysis and Antigen Prediction</h4>
<div class="ltx_para" id="S4.SS3.SSS3.p1">
<p class="ltx_p" id="S4.SS3.SSS3.p1.1">LLMs and machine learning approaches are increasingly being applied to analyze biomarkers and predict antigenic variations, which are essential for understanding immune responses and optimizing vaccine design. In biomarker analysis, studies have leveraged LLMs to investigate genetic relationships and autoimmune markers, helping to elucidate the factors that influence vaccination outcomes and susceptibility to infectious diseases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib311" title="">311</a>]</cite>. For example, differential network centrality analysis and feature selection techniques have been employed to identify key susceptibility hubs within biological networks, offering insights into factors that contribute to immune response variability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib312" title="">312</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS3.p2">
<p class="ltx_p" id="S4.SS3.SSS3.p2.1">Additionally, antigenic prediction plays a crucial role in designing effective influenza vaccines, especially for rapidly evolving strains. Statistical analyses of antigenic similarity, such as those conducted for influenza A (H3N2), highlight the potential of machine learning models in mapping antigenic drift and optimizing strain selection for seasonal vaccines <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib313" title="">313</a>]</cite>. Moreover, cellular correlates of protection identified through human influenza virus challenges have advanced our understanding of immune responses to oral vaccines, demonstrating the applicability of machine learning models in immune signature identification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib314" title="">314</a>]</cite>. Blood inflammatory biomarkers have also been analyzed to differentiate COVID-19 from influenza cases, showcasing the predictive power of LLMs in clinical biomarker differentiation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib315" title="">315</a>]</cite>. Seasonal antigenic prediction, particularly for influenza A H3N2, has benefited from machine learning approaches that help forecast viral evolution, supporting timely vaccine updates <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib304" title="">304</a>]</cite>. Finally, phylogenetic analyses have identified optimal influenza virus candidates for seasonal vaccines, underscoring the significance of LLMs in guiding vaccine development against anticipated strains <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib316" title="">316</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.4 </span>Vaccine Recommendation and Immune Response</h4>
<div class="ltx_para" id="S4.SS3.SSS4.p1">
<p class="ltx_p" id="S4.SS3.SSS4.p1.1">LLMs are increasingly leveraged in vaccine recommendation and immune response studies, especially in analyzing antigenicity and optimizing vaccine strain selection. For instance, the MAIVeSS platform utilizes LLMs to streamline the selection of high-yield, antigenically matched viruses for seasonal influenza vaccines, a critical step in addressing annual viral mutations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib317" title="">317</a>]</cite>. In populations with specific health conditions, such as HIV, LLMs have been applied to predict the immunogenicity of trivalent inactivated influenza vaccines, revealing key biomarkers and immune signatures that inform personalized vaccination strategies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib318" title="">318</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS4.p2">
<p class="ltx_p" id="S4.SS3.SSS4.p2.1">Antigenicity prediction models have also employed convolutional neural networks to optimize vaccine recommendations for influenza virus A (H3N2), facilitating the identification of effective vaccine strains through detailed computational modeling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib319" title="">319</a>]</cite>. Furthermore, temporal topic models generated from clinical text data allow for a more nuanced understanding of immune responses over time, especially in relation to patient health history and demographic factors, enhancing the precision of vaccine recommendations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib320" title="">320</a>]</cite>. Finally, studies on COVID-19 vaccine hesitancy among populations already immunized for influenza underscore the relevance of LLMs in analyzing and addressing hesitancy factors, which is vital for improving adherence to vaccination programs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib321" title="">321</a>]</cite>. Together, these applications illustrate the potential of LLM-based approaches in advancing vaccine recommendation processes and tailoring immune response strategies.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.5 </span>Sentiment Analysis and Public Attitude Research on Social Media</h4>
<div class="ltx_para" id="S4.SS3.SSS5.p1">
<p class="ltx_p" id="S4.SS3.SSS5.p1.1">LLM techniques are widely used in sentiment analysis to assess public attitudes towards vaccines, particularly through social media data. This approach provides insights into public sentiment trends and identifies factors contributing to vaccine hesitancy or acceptance. For instance, social media analysis of public messaging around influenza vaccination from 2017 to 2023 has shown how sentiment fluctuates in response to vaccine news, policy changes, and health crises, offering a longitudinal view of public perception <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib322" title="">322</a>]</cite>. Similarly, negative sentiments related to influenza vaccines, analyzed from over 260,000 Twitter posts, highlight recurring concerns and misconceptions that can be addressed through targeted public health messaging <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib323" title="">323</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS5.p2">
<p class="ltx_p" id="S4.SS3.SSS5.p2.1">Beyond social media, predictive models using smartwatch and smartphone data can monitor side effects and public reactions post-vaccination, enhancing our understanding of vaccine safety perceptions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib324" title="">324</a>]</cite>. The FDA’s Biologics Effectiveness and Safety Initiative also uses NLP to process unstructured data, identifying adverse events associated with vaccines and contributing to more accurate public health responses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib325" title="">325</a>]</cite>. Additionally, integrating immune cell population data and gene expression with CpG methylation patterns offers insights into immune responses that can correlate with public attitudes, informing data-driven interventions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib326" title="">326</a>]</cite>. These findings underscore the utility of LLMs in sentiment analysis, enabling public health authorities to monitor and respond to vaccine-related concerns effectively.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.6 </span>Epidemiology and Public Health Data Analysis</h4>
<div class="ltx_para" id="S4.SS3.SSS6.p1">
<p class="ltx_p" id="S4.SS3.SSS6.p1.1">Machine learning and large datasets have profoundly impacted epidemiology and public health, enabling the analysis of disease patterns, risk factors, and vaccination responses. Studies integrating socioeconomic, health, and safety data have examined how these factors affect COVID-19 spread, offering insights into the influence of demographics like income and healthcare access on infection rates <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib327" title="">327</a>]</cite>. Projects like the Human Vaccines Project also leverage large datasets to map immune responses across populations, enhancing our understanding of vaccine design and immunology <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib328" title="">328</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS6.p2">
<p class="ltx_p" id="S4.SS3.SSS6.p2.1">The use of wearable sensors in epidemiological studies, as demonstrated in the WE SENSE protocol, facilitates early detection of viral infections by analyzing real-time health metrics, thus supporting timely public health interventions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib329" title="">329</a>]</cite>. Pneumonia research, such as the work by the CAPNETZ study group, highlights unmet needs in understanding disease mechanisms, emphasizing the need for targeted data collection and analysis in developing effective treatment and intervention strategies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib330" title="">330</a>]</cite>. Additionally, sociodemographic studies on COVID-19 vaccine acceptance reveal how age, gender, and education level impact vaccine uptake, providing crucial insights for public health policy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib307" title="">307</a>]</cite>. These applications underscore the essential role of data-driven approaches in epidemiology and public health to improve disease prevention and health policy.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Drug Discovery and Development</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Drug Target Identification</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Drug discovery is a resource-intensive and time-consuming process, often spanning 7 to 20 years from initial development to market approval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib331" title="">331</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib332" title="">332</a>]</cite>. Central to this process is drug-target interaction (DTI) identification, which involves pinpointing molecules implicated in disease mechanisms. Traditional methods, including genomics, proteomics, RNAi, and molecular docking, have been instrumental but face limitations in cost, scalability, and adaptability to complex biological systems.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">Recent advancements in computational techniques, such as machine learning<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib333" title="">333</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib334" title="">334</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib335" title="">335</a>]</cite>, knowledge graph-based methods<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib336" title="">336</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib337" title="">337</a>]</cite>, and molecular docking simulations, driven by the rapid growth of large-scale biomedical datasets<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib338" title="">338</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib339" title="">339</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib340" title="">340</a>]</cite>, have significantly advanced DTI prediction. Beyond these methods, recent breakthroughs in LLMs and BioLMs represent a paradigm shift, enabling the integration and analysis of vast, heterogeneous datasets—including molecular data, biological networks, and scientific literature—while storing drug-related background knowledge through extensive pretraining <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib341" title="">341</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib342" title="">342</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib343" title="">343</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib344" title="">344</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib345" title="">345</a>]</cite>. This section provides an overview of LLM-based approaches for DTI prediction, categorized based on the type of data they utilize: sequence data, structural data, and relationship data, with the latter primarily derived from knowledge graphs.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">Sequence data, including amino acid sequences for proteins and Simplified Molecular Input Line Entry System (SMILES) representations for drugs, plays a central role in single-modal methods for DTI prediction. Pretrained language models (PLMs), such as PharmBERT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib346" title="">346</a>]</cite>, BioBERT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib347" title="">347</a>]</cite>, and ProteinBERT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib348" title="">348</a>]</cite>, have been widely utilized to extract meaningful representations from such data, enabling efficient and accurate predictions. For instance, DTI-LM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib349" title="">349</a>]</cite> addresses the cold-start problem by utilizing PLMs to predict DTIs based solely on molecular and protein sequences, enabling accurate predictions for novel drugs and uncharacterized targets. Similarly, ConPLex <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib350" title="">350</a>]</cite> generates co-embeddings of drugs and target proteins, achieving broad generalization to unseen proteins and over 10× faster inference compared to traditional sequence-based methods, making it ideal for tasks like drug repurposing and high-throughput screening. Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib351" title="">351</a>]</cite> further enhance DTI prediction by introducing high-frequency amino acid subsequence embedding and transfer learning, capturing functional interaction units and shared features across large datasets. Additionally, TransDTI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib352" title="">352</a>]</cite> employs transformer-based language models to classify drug-target interactions into active, inactive, or intermediate categories, offering competitive performance. Despite their advantages, single-modal methods are limited by their reliance on sequence data alone, making it challenging to capture interactions involving spatial, structural, or contextual dependencies.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">To address the limitations of single-modal approaches, multimodal frameworks integrate diverse data types—such as molecular graphs, protein sequences, and structural data—offering a more comprehensive understanding of DTIs. DrugLAMP<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib353" title="">353</a>]</cite> exemplifies this integration, utilizing Pocket-Guided Co-Attention (PGCA) and Paired Multi-Modal Attention (PMMA) to fuse molecular graphs with sequence data, achieving nuanced molecular interaction predictions. PGraphDTA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib354" title="">354</a>]</cite> incorporates 3D contact maps alongside protein sequences, outperforming sequence-only methods when structural data is available. Beyond predictive accuracy, multimodal frameworks like CGPDTA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib355" title="">355</a>]</cite> enhance interpretability by integrating interaction networks, providing insights into biological mechanisms. DrugChat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib356" title="">356</a>]</cite> combines prompt-based learning with sequence data and textual inputs. Pretrained on three datasets, it predicts indications, mechanisms of action, and pharmacodynamics while dynamically generating textual outputs in response to user prompts. This eliminates the need for retraining and enables flexible, interactive exploration of drug mechanisms. Similarly, DrugReAlign<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib357" title="">357</a>]</cite> employs a multi-source prompting approach that integrates diverse and reliable data inputs to integrate textual and structural data, enhancing drug repurposing efforts.</p>
</div>
<div class="ltx_para" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1">Beyond structural data, KG-based models leverage semantic relationships, such as shared pathways, biological processes, and functional annotations, along with diverse data sources to achieve competitive performance in DTI predictions. Y-Mol <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib357" title="">357</a>]</cite> enhances biomedical reasoning by integrating multiscale biomedical knowledge and using LLaMA2 as its base LLM. It learns from publications, knowledge graphs, and synthetic data, enriched by three types of drug-oriented prompts: description-based, semantic-based, and template-based, enabling robust drug interaction analysis.
Similarly, the multi-agent framework DrugAgent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib358" title="">358</a>]</cite> advances drug repurposing by combining AI-driven DTI models, knowledge graph extraction from databases (e.g., DrugBank, CTD<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib359" title="">359</a>]</cite>), and literature-based validation. This framework integrates diverse data sources to streamline repurposing candidate identification, enhancing efficiency, interpretability, and cost-effectiveness. Together, these models boost predictive power while fostering collaboration and refinement.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Molecular Docking and Drug Design</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">The advanced reasoning capabilities of large language models have enabled their application in biological and medical fields, demonstrating significant potential to accelerate drug discovery and screening processes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib73" title="">73</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib17" title="">17</a>]</cite>. Built upon the transformer architecture from Natural Language Processing (NLP), biology-focused language models have emerged as powerful tools to support both sequence-based and structure-based drug design <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib360" title="">360</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib361" title="">361</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib362" title="">362</a>]</cite>. By utilizing their strengths in text summarization and contextual understanding, these models can integrate information from diverse sources, such as scientific literatures, patent databases, and specialized datasets, to provide comprehensive analyses and insights into protein sequences, structures, binding pockets, and interaction sites <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib363" title="">363</a>]</cite>. Moreover, protein language models and other transformer-based models are being applied to exploit unknown structural information in structure-based drug design (SBDD) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib361" title="">361</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib362" title="">362</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Molecular docking, a pivotal component of Structure-Based Drug Design (SBDD), necessitates three-dimensional protein structures and precise binding site information to calculate binding affinities during in silico virtual screening <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib364" title="">364</a>]</cite>. LLMs have shown potential to enhance various aspects of molecular docking, including docking input file generation, binding site prediction, and protein structure prediction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib361" title="">361</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib362" title="">362</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib365" title="">365</a>]</cite>. AutoDock is a widely adopted software for molecular docking <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib366" title="">366</a>]</cite>. For high-throughput drug screening, it is necessary to generate docking commands in text file format and execute them in the terminal. Sharma et al. demonstrated the capability of ChatGPT to generate AutoDock input files and basic molecular docking scripts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib365" title="">365</a>]</cite>. Another notable example is DrugChat, a ChatGPT-like LLM for drug molecule graphs developed by Liang et al. With the input of compound molecule graphs and appropriate prompts, DrugChat is able to generate insightful responses<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib367" title="">367</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">Ligand binding site identification and prediction are essential for drug design. Due to the limited availability of experimentally determined protein crystal structures and incomplete protein structural knowledge, ligand binding site identification can be tough. Zhang et al. addressed this limitation through LaMPSite, an algorithm powered by EMS-2 protein language model, which only requires protein sequences and ligand molecular graphs as inputs without any protein structural information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib361" title="">361</a>]</cite>. This approach achieved comparable performance to those methods requiring 3D protein structures in benchmark evaluations. Regarding of deficiency of reliable protein structure, protein language models have been applied for protein structure prediction as well. For example, Fang et al. introduced HelixFold-Single, a multiple-sequence-alignment-free protein structure predictor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib362" title="">362</a>]</cite>. Unlike AlphaFold2, which enhances prediction accuracy by relying on multiple sequence alignments of homologous proteins, HelixFold-Single adopts a more efficient approach. It leverages large-scale protein language model training on the primary structures of proteins while integrating key components from AlphaFold2 for protein geometry.</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1">Recent advancements in protein-ligand binding prediction methods have further enhanced screening efficiency and accuracy. Shen et al. developed RTMScore, which integrated Graph Transformer to extract structural features of protein and molecule, using 3D residue graphs of protein and 2D molecular graphs as inputs for protein-ligand binding pose prediction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib368" title="">368</a>]</cite>. RTMScore outperformed many state-of-the-art docking software including Autodock Vina <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib369" title="">369</a>]</cite> , DeepBSP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib370" title="">370</a>]</cite>, and DeepDock <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib371" title="">371</a>]</cite> in performing virtual screening tasks. Another notable development is ConPlex, a sequence-based drug-target interaction (DTI) prediction method introduced by Singh et al <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib372" title="">372</a>]</cite>. By employing representations generated from pre-trained protein language models (PLMs) as the inputs, ConPlex benefits from a larger corpus of single protein sequences and alleviates the problem of limited DTI training data. Additionally, contrastive learning was adopted to address the fine-grained issues by employing contrastive coembedding, which is able to co-locate the proteins and the targets in a shared latent space. Thus, a high specificity can be achieved by separating the true interacting patterns and decoys. According to contrastive training results, the effective size between true and decoy scores was largely increased.</p>
</div>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1">Through automated data extraction and normalization, LLMs can greatly improve the efficiency and accuracy of drug property predictions. With ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity) analysis, LLMs can also help distinguish the compounds possessing favorable profiles from those showing adverse characters and allow developing the most promising drug candidates during the pipeline process. For instance, PharmaBench achieves this through its multi-intelligence system, whose core function is to extract ADMET-related data from multiple public databases using LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib373" title="">373</a>]</cite>. Beyond ADMET analysis, LLMs like ChatGPT have expanded their capabilities to predict and analyze other features of drugs, including pharmacodynamics and pharmacokinetics, thus providing a comprehensive evaluation of potential drug candidates <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib363" title="">363</a>]</cite>. LLMs powerfully accelerate the drug development pipeline by fastening data analysis, enhancing prediction accuracy, and offering all-rounded drug property evaluation, which in turn reduces both the time and resources needed for drug discovery and improves the chances of coming up with a successful drug candidate.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Immunology and Vaccine Development</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Large Language Models , including GPT-based architectures, have transformed the field of immunology and vaccine development by enabling advanced analyses of large, complex datasets. These models, combined with machine learning, NLP, and feature selection techniques, facilitate the identification of immune biomarkers, prediction of vaccine efficacy, understanding of vaccine hesitancy, and real-time monitoring of adverse events. This review synthesizes recent research highlighting the critical role of LLMs in advancing vaccine science, with a focus on immune response analysis, vaccine development, efficacy prediction, safety, and public attitudes.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Immune Response Analysis and Biomarker Research</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">Analyzing immune responses and identifying biomarkers are critical for understanding the efficacy and mechanisms of vaccines. Large Language Models , integrated with advanced computational techniques, play a key role in processing and interpreting complex datasets to uncover immune signatures and their correlation with vaccination outcomes. For example, LLMs can efficiently analyze high-dimensional datasets, such as the FluPRINT dataset, which provides a multidimensional analysis of the immune system’s imprint following influenza vaccination, revealing variability in immune responses across individuals <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib374" title="">374</a>]</cite>. By leveraging LLMs, researchers can extract patterns and relationships from immune cell populations, mRNA sequencing, and CpG methylation data, leading to more accurate predictions of humoral immunity and highlighting the impact of gene expression and epigenetic modifications on vaccine-induced immunity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib326" title="">326</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">Automated systems like SIMON utilize machine learning, augmented by LLMs for text-based data extraction and interpretation, to reveal immune signatures that predict vaccine responsiveness, providing deeper insights into immune mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib375" title="">375</a>]</cite>. Furthermore, LLMs facilitate the integration of multi-level models that incorporate gene expression interaction networks to predict antibody responses to vaccines, enabling the precise identification of immune predictors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib376" title="">376</a>]</cite>. For biomarker analysis, LLMs contribute to identifying apoptosis and inflammatory responses through their ability to process vast quantities of biological literature and experimental data, as seen in studies linking immune biomarkers with influenza vaccine responsiveness <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib302" title="">302</a>]</cite>. They also assist in differentiating immune responses to COVID-19 and influenza infections by analyzing blood inflammatory biomarkers and clinical data at scale <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib315" title="">315</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1">Additionally, human influenza virus challenge models, supported by LLM-driven analysis of experimental outcomes, have identified cellular correlates of protection, advancing our understanding of immune responses to oral vaccines <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib314" title="">314</a>]</cite>. LLMs streamline the analysis of complex immune response datasets, ensuring faster identification of key findings and improving collaboration across interdisciplinary research teams.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Vaccine Development and Recommendation Models</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">The development and optimization of vaccines rely on computational models to predict vaccine efficacy, identify suitable strains, and recommend antigenically matched candidates. Large Language Models have become invaluable tools in this domain by enhancing the ability to process and analyze vast datasets, extract patterns from biomedical literature, and improve antigenic prediction models. Neural networks and logistic regression have traditionally been applied to predict influenza vaccination outcomes, providing robust frameworks for assessing vaccine effectiveness based on demographic and clinical data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib299" title="">299</a>]</cite>. With the integration of LLMs, these predictive models can be further refined by incorporating insights derived from textual datasets, such as clinical notes, trial reports, and patient feedback.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">In silico approaches, combined with LLM-based text mining, enable the analysis of autoimmune diseases and their genetic relationships to vaccination. LLMs can extract relevant patterns across large corpora of genomic and immunological studies, offering deeper insights into immune response mechanisms and potential cross-reactivity among populations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib311" title="">311</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1">Platforms like MAIVeSS streamline the selection of antigenically matched, high-yield viruses for seasonal influenza vaccines by leveraging LLMs to analyze historical viral sequences, antigenic relationships, and experimental outcomes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib317" title="">317</a>]</cite>. Convolutional neural networks (CNNs), enhanced with LLM-derived insights, have been employed to predict antigenicity and recommend influenza virus vaccine strains by synthesizing complex relationships among viral genetic sequences and epidemiological data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib319" title="">319</a>]</cite>. Additionally, seasonal antigenic prediction models utilize machine learning algorithms integrated with LLMs to analyze influenza A (H3N2) evolution and forecast emerging strains, improving the accuracy and efficiency of vaccine formulation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib304" title="">304</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS2.p4">
<p class="ltx_p" id="S6.SS2.p4.1">Phylogenetic analyses are also augmented through LLM capabilities, which automate literature reviews and contextualize genetic relationships to identify influenza virus candidates for seasonal vaccines. This ensures antigenic compatibility, reduces manual analysis time, and maximizes immunogenic coverage <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib316" title="">316</a>]</cite>. By incorporating LLMs, researchers can process and synthesize global influenza surveillance data, generating actionable insights to address the challenge of rapidly evolving pathogens.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Vaccine Efficacy Prediction and Immunogenicity Studies</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">Accurately predicting vaccine efficacy and assessing immunogenicity are critical for improving vaccination strategies and understanding immune responses. LLMs play a pivotal role in processing vast datasets to extract critical insights, identify risk factors, and predict vaccine efficacy. LLMs are increasingly used to synthesize clinical, epidemiological, and behavioral data, which are key to identifying populations with low adherence to vaccination programs. For example, models analyzing high-risk groups, such as individuals with cardiovascular disease, have integrated LLM-driven data extraction from clinical records to uncover demographic and behavioral predictors of vaccine uptake <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib305" title="">305</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1">In real-time monitoring, LLMs enhance the analysis of self-reported data to estimate vaccine coverage and adherence. By processing text-based survey responses and digital health data, LLMs enable precise insights into population-wide vaccine uptake and the factors influencing these trends <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib306" title="">306</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.1">For immunogenicity studies, LLMs are employed to mine complex biological and clinical datasets, improving predictions of vaccine immune responses in targeted populations. For example, LLM-augmented artificial intelligence models have been used to predict immunogenicity in pediatric studies, such as for trivalent inactivated influenza vaccines in HIV-infected children, facilitating personalized vaccination strategies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib318" title="">318</a>]</cite>. Clinical feature-based models further benefit from LLMs’ ability to extract structured and unstructured data from clinical notes, improving predictions of infection risks in individuals post-vaccination <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib301" title="">301</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS3.p4">
<p class="ltx_p" id="S6.SS3.p4.1">In biomarker-based analyses, LLMs assist in synthesizing large-scale experimental and clinical literature to identify apoptosis markers and inflammatory biomarkers associated with vaccine responsiveness. This enables a better understanding of immune responses and facilitates personalized immunization approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib302" title="">302</a>]</cite>. Post-marketing vaccine safety surveillance systems, such as the VAERS dataset, have leveraged LLMs to extract, classify, and analyze adverse event reports. By automating the processing of unstructured clinical narratives, LLMs enhance the detection of adverse events and improve vaccine safety assessments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib303" title="">303</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS3.p5">
<p class="ltx_p" id="S6.SS3.p5.1">Comparative studies examining influenza vaccine uptake pre- and post-COVID-19 also benefit from LLMs’ ability to analyze large textual datasets, such as survey responses and social media discussions. These models provide actionable insights into behavioral shifts and critical predictors of vaccine adherence, contributing to data-driven vaccination strategies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib310" title="">310</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Vaccine Hesitancy and Public Attitude Analysis</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">Vaccine hesitancy remains a significant challenge to achieving widespread immunization, and LLMs have proven instrumental in uncovering the underlying causes, trends, and predictors of public attitudes toward vaccination. LLMs, combined with machine learning and NLP, enable the analysis of large-scale textual data, including social media, survey responses, and clinical reports, providing insights into public perceptions and vaccine acceptance patterns.</p>
</div>
<div class="ltx_para" id="S6.SS4.p2">
<p class="ltx_p" id="S6.SS4.p2.1">LLMs have been employed to process unstructured data for real-time monitoring of vaccine-related discussions, identifying concerns around side effects and safety perceptions. For example, predictive models using smartwatch and smartphone data, enhanced by LLM-driven text analysis, have been used to detect and predict the severity of side effects following vaccination, improving the understanding of public concerns regarding vaccine safety <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib324" title="">324</a>]</cite>. LLMs have further facilitated automated detection of vaccine-related messaging and adverse event reporting, as demonstrated by initiatives like the FDA Biologics Effectiveness and Safety Initiative <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib325" title="">325</a>]</cite>. These models analyze clinical notes and text-based reports at scale, streamlining post-vaccination safety monitoring.</p>
</div>
<div class="ltx_para" id="S6.SS4.p3">
<p class="ltx_p" id="S6.SS4.p3.1">Parental attitudes toward childhood vaccination have been analyzed using validated scales, with LLMs efficiently extracting themes and patterns from caregiver responses. These analyses highlight key concerns, such as vaccine safety and efficacy, and inform targeted education strategies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib308" title="">308</a>]</cite>. LLMs are also applied to sociodemographic studies, enabling the identification of key predictors of vaccine acceptance, including education level, income, and geographic location. By synthesizing national-scale survey data, LLMs provide a foundation for interventions aimed at addressing vaccine hesitancy in specific demographic groups <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib307" title="">307</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS4.p4">
<p class="ltx_p" id="S6.SS4.p4.1">Sentiment analysis of social media platforms, such as Twitter, has been revolutionized by LLMs like GPT-based architectures. These models analyze vaccine-related discourse, identifying trends in vaccine hesitancy and negative attitudes toward vaccination programs. For example, LLMs have revealed hesitancy trends related to influenza vaccination and highlighted shifts in public sentiment in response to public health campaigns and policy changes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib323" title="">323</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib322" title="">322</a>]</cite>. Longitudinal studies powered by LLMs demonstrate how public messaging evolves over multiple years, providing actionable insights for optimizing communication strategies and combating misinformation. Additionally, comparative studies across continents emphasize cultural and regional variations in vaccine attitudes, which LLMs can analyze to tailor communication strategies to local contexts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib309" title="">309</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS4.p5">
<p class="ltx_p" id="S6.SS4.p5.1">Vaccine hesitancy studies among specific groups, such as Canadians immunized for influenza, benefit from LLMs’ ability to process large-scale survey responses and extract nuanced concerns <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib321" title="">321</a>]</cite>. These insights underscore the complexity of public attitudes and the importance of sustained public education.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Vaccine Safety and Adverse Event Detection</h3>
<div class="ltx_para" id="S6.SS5.p1">
<p class="ltx_p" id="S6.SS5.p1.1">Ensuring vaccine safety and monitoring adverse events following immunization are critical components of immunization programs. LLMs play an increasingly vital role in enhancing vaccine safety surveillance by automating the detection, classification, and analysis of adverse events at scale.</p>
</div>
<div class="ltx_para" id="S6.SS5.p2">
<p class="ltx_p" id="S6.SS5.p2.1">Predictive models leveraging smartwatch and smartphone data, combined with LLM-powered text analysis, enable real-time monitoring of vaccine-related side effects. By processing unstructured patient-reported outcomes and wearable device data, LLMs help identify patterns in the severity of side effects following COVID-19 and influenza vaccinations, facilitating timely interventions and improving patient outcomes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib324" title="">324</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS5.p3">
<p class="ltx_p" id="S6.SS5.p3.1">The FDA’s Biologics Effectiveness and Safety Initiative has utilized NLP techniques powered by LLMs to analyze unstructured clinical data, such as physician notes and medical records, for detecting vaccine-related adverse events. LLMs significantly enhance the ability to process and interpret large-scale textual datasets, streamlining the identification of safety signals and improving the efficiency of post-marketing surveillance systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib325" title="">325</a>]</cite>. These automated systems reduce manual effort, accelerate safety signal detection, and enable regulators to respond quickly to emerging concerns.</p>
</div>
<div class="ltx_para" id="S6.SS5.p4">
<p class="ltx_p" id="S6.SS5.p4.1">Deep learning approaches applied to the Vaccine Adverse Event Reporting System (VAERS) have also been strengthened by the integration of LLMs. By extracting and categorizing adverse event reports from free-text submissions, LLMs improve the accuracy and granularity of vaccine safety assessments. For instance, LLMs can identify subtle patterns and correlations within adverse event reports, allowing researchers to generate valuable insights into vaccine safety profiles, detect rare adverse events, and support regulatory decisions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib303" title="">303</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS5.p5">
<p class="ltx_p" id="S6.SS5.p5.1">Moreover, LLMs facilitate cross-referencing of adverse event data with other sources, such as scientific literature, clinical trial reports, and patient feedback, providing a comprehensive view of vaccine safety. By automating this process, LLMs improve the robustness of post-marketing surveillance systems and enhance public confidence in vaccination programs.</p>
</div>
<div class="ltx_para" id="S6.SS5.p6">
<p class="ltx_p" id="S6.SS5.p6.1">Together, these studies underscore the transformative role of Large Language Models in vaccine safety monitoring and adverse event detection. By efficiently processing and analyzing large-scale unstructured data, LLMs enable faster, more accurate identification of adverse events, ensuring vaccine safety and maintaining public trust in immunization efforts.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.6 </span>Vaccine-Related Social and Health Data Analysis</h3>
<div class="ltx_para" id="S6.SS6.p1">
<p class="ltx_p" id="S6.SS6.p1.1">The analysis of social and health data plays a critical role in understanding vaccine uptake, disease spread, and public health outcomes. Large Language Models have become essential tools for processing and interpreting large-scale social, health, and demographic datasets, enabling researchers to identify patterns and design targeted interventions for improving vaccination strategies.</p>
</div>
<div class="ltx_para" id="S6.SS6.p2">
<p class="ltx_p" id="S6.SS6.p2.1">LLMs are particularly effective in synthesizing diverse data sources, including electronic health records, socioeconomic surveys, and real-time reports. For instance, studies addressing unmet needs in pneumonia research benefit from LLMs’ ability to integrate textual clinical data with structured epidemiological datasets, facilitating a comprehensive understanding of disease burden, treatment gaps, and prevention strategies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib330" title="">330</a>]</cite>. Similarly, LLMs assist in analyzing socioeconomic, health, and safety data to explain the spread of diseases such as COVID-19. By processing large datasets across regions, LLMs highlight key factors, such as healthcare access, education, and income, that impact infection rates and vaccine coverage <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib327" title="">327</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS6.p3">
<p class="ltx_p" id="S6.SS6.p3.1">The Human Vaccines Project, which focuses on leveraging immunological and epidemiological data to improve vaccination strategies, has incorporated LLMs to process vast volumes of immunology literature, trial reports, and population health data. LLMs enable the identification of critical trends and insights that advance the understanding of human immune responses to vaccines, accelerating the development of targeted immunization programs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib328" title="">328</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS6.p4">
<p class="ltx_p" id="S6.SS6.p4.1">Sociodemographic predictors of vaccine acceptance, such as age, education, and geographic location, have been extensively studied with the support of LLMs. These models efficiently process large-scale national surveys, extracting patterns and correlations that inform targeted interventions to address vaccine hesitancy and acceptance across diverse populations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib307" title="">307</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS6.p5">
<p class="ltx_p" id="S6.SS6.p5.1">Real-time health data collected through wearable sensors, as demonstrated in the WE SENSE protocol, have also been enhanced by LLMs’ ability to integrate sensor outputs with epidemiological trends. LLMs process this real-time data alongside other health records to detect early warning signs of viral infections and identify potential outbreaks, highlighting their role in improving public health preparedness and surveillance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib329" title="">329</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS6.p6">
<p class="ltx_p" id="S6.SS6.p6.1">These studies collectively demonstrate the transformative role of Large Language Models in integrating social, health, and demographic data for vaccine-related research. By efficiently processing and analyzing vast, heterogeneous datasets, LLMs offer valuable insights that shape public health policies, improve vaccination strategies, and enhance disease preparedness efforts.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Discussion and Future Directions</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Although large language models have achieved remarkable success in bioinformatics, they still face numerous challenges. The performance of LLMs in bioinformatics heavily relies on the quality of training data, yet available datasets such as genomic or proteomic sequences often contain noise and biases. This issue leads to inaccurate predictions and limited generalizability. Additionally, the limited availability of labeled biological data further hinders the adaptability of LLMs to diverse bioinformatics tasks. Computational cost and scalability present another significant challenge. LLMs are resource-intensive, requiring substantial computational power and memory for training and inference, which becomes particularly problematic when analyzing ultra-long sequences like genomic regions spanning thousands of base pairs. Transformer-based architectures, despite their advancements, still struggle with scaling efficiently for such long sequences due to inherent memory constraints.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">Generalizability and interpretability also remain critical concerns. While LLMs excel at specific tasks, their ability to generalize across unseen datasets or tasks is often inadequate. Moreover, the lack of interpretability in model outputs makes it difficult for researchers to understand the underlying biological mechanisms, which is essential for result validation. Ethical and privacy concerns further complicate the application of LLMs, particularly in sensitive areas such as personalized medicine. The use of patient data in training models raises significant ethical questions and potential privacy risks, limiting widespread adoption.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">Despite these challenges, the future of LLMs in bioinformatics presents exciting opportunities. Efforts are likely to focus on developing lightweight and efficient architectures, such as LoRA and QLoRA, to mitigate computational and memory requirements. Innovations in Transformer variants and hybrid architectures are expected to overcome scalability challenges, enabling more effective analysis of long-sequence bioinformatics tasks. Integrating diverse biological data types, including DNA, RNA, protein sequences, epigenetic, and transcriptomic data, will enhance LLMs’ capability to generate comprehensive biological insights. Improved interpretability will also become a priority, with advancements aimed at visualizing attention mechanisms and uncovering the biological basis behind predictions.</p>
</div>
<div class="ltx_para" id="S7.p4">
<p class="ltx_p" id="S7.p4.1">Applications in personalized medicine highlight the transformative potential of LLMs. For example, they can revolutionize precision medicine by tailoring treatments to individual patients, predicting drug efficacy, or identifying possible side effects based on genomic data. Addressing data scarcity through open data initiatives and interdisciplinary collaborations will further accelerate progress, enabling broader applications of LLMs in bioinformatics. Additionally, as Transformer models reach maturity, exploration of alternative architectures may drive innovation beyond their current limitations, ensuring continuous advancement in the field. These trends underscore the dynamic evolution of LLMs in bioinformatics, presenting opportunities for groundbreaking developments while emphasizing the need to address existing limitations.</p>
</div>
<div class="ltx_para" id="S7.p5">
<p class="ltx_p" id="S7.p5.1">The integration of multimodal biomedical data presents another promising direction for future research. Sequence-to-sequence models, which have demonstrated remarkable success in natural language processing, offer a promising technical approach for fusing diverse biomedical data types. These models can potentially bridge the gap between different modalities - including medical imaging, clinical texts, temporal data (such as electronic health records and vital signs), and various forms of biological sequence data (DNA, RNA, and proteins). For instance, sequence-to-sequence architectures could be adapted to translate between modalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib377" title="">377</a>, <a class="ltx_ref" href="https://arxiv.org/html/2501.06271v1#bib.bib17" title="">17</a>]</cite>, such as converting radiological images to diagnostic text descriptions while incorporating relevant genomic information. This multimodal fusion could enable more comprehensive disease diagnosis and treatment planning by leveraging complementary information from different data sources. Furthermore, innovative attention mechanisms and cross-modal transformers could help capture complex relationships between different data types, leading to more robust and interpretable models. The challenge lies in developing architectures that can effectively handle the inherent heterogeneity of these data types while maintaining computational efficiency and biological interpretability.</p>
</div>
</section>
<section class="ltx_section" id="S8" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">This comprehensive survey has explored the transformative impact of LLMs in bioinformatics, spanning applications in genomics, proteomics, drug discovery, and clinical medicine. Our review has highlighted the successful adaptation of transformer architectures for biological sequences, the emergence of specialized biomedical LLMs, and the promising integration of multiple data modalities. These advances have enabled significant progress in protein structure prediction, drug-target interaction analysis, and disease diagnosis.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">Despite notable achievements, challenges persist in data quality, computational scalability, model interpretability, and ethical considerations regarding patient privacy. These challenges present opportunities for future research, particularly in developing efficient architectures, improving multimodal data integration, and ensuring model interpretability. The convergence of LLMs with emerging biotechnologies promises to accelerate discovery in bioinformatics, potentially leading to more precise and personalized medical interventions.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Proceedings of naacL-HLT</span>, volume 1, page 2. Minneapolis, Minnesota, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Advances in neural information processing systems</span>, 33:1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Jiajia Liu, Mengyuan Yang, Yankai Yu, Haixia Xu, Kang Li, and Xiaobo Zhou.

</span>
<span class="ltx_bibblock">Large language models in bioinformatics: applications and perspectives.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2401.04155v1</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Oluwafemi A Sarumi and Dominik Heider.

</span>
<span class="ltx_bibblock">Large language models and their applications in bioinformatics.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Computational and Structural Biotechnology Journal</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Satvik Tripathi, Kyla Gabriel, Pushpendra Kumar Tripathi, and Edward Kim.

</span>
<span class="ltx_bibblock">Large language models reshaping molecular biology and drug development.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Chemical Biology &amp; Drug Design</span>, 103(6):e14568, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, et al.

</span>
<span class="ltx_bibblock">Summary of chatgpt/gpt-4 research and perspective towards the future of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2304.01852</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Lin Zhao, Lu Zhang, Zihao Wu, Yuzhong Chen, Haixing Dai, Xiaowei Yu, Zhengliang Liu, Tuo Zhang, Xintao Hu, Xi Jiang, et al.

</span>
<span class="ltx_bibblock">When brain-inspired ai meets agi.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2303.15935</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Yifan Zhou, Shizhe Liang, Zihao Wu, Yanjun Lyu, Peng Shu, Xiaowei Yu, et al.

</span>
<span class="ltx_bibblock">Evaluation of openai o1: Opportunities and challenges of agi.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2409.18486</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Chong Ma, Zihao Wu, Jiaqi Wang, Shaochen Xu, Yaonai Wei, Zhengliang Liu, Fang Zeng, Xi Jiang, Lei Guo, Xiaoyan Cai, et al.

</span>
<span class="ltx_bibblock">An iterative optimizing framework for radiology report summarization with chatgpt.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">IEEE Transactions on Artificial Intelligence</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Zihao Wu, Lin Zhao, Wei Liu, Ninghao Liu, Sheng Li, Dajiang Zhu, et al.

</span>
<span class="ltx_bibblock">Chataug: Leveraging chatgpt for text data augmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2302.13007</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Zhengliang Liu, Yiwei Li, Peng Shu, Aoxiao Zhong, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma, Jie Luo, Cheng Chen, et al.

</span>
<span class="ltx_bibblock">Radiology-llama2: Best-in-class large language model for radiology.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2309.06419</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Wenxiong Liao, Zhengliang Liu, Haixing Dai, Shaochen Xu, Zihao Wu, Yiyang Zhang, Xiaoke Huang, Dajiang Zhu, Hongmin Cai, Quanzheng Li, et al.

</span>
<span class="ltx_bibblock">Differentiating chatgpt-generated and human-written medical texts: quantitative study.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">JMIR Medical Education</span>, 9(1):e48904, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Zhengliang Liu, Xinyu He, Lei Liu, Tianming Liu, and Xiaoming Zhai.

</span>
<span class="ltx_bibblock">Context matters: A strategy to pre-train language model for science education.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2301.12031</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Saed Rezayi, Haixing Dai, Zhengliang Liu, Zihao Wu, Akarsh Hebbar, Andrew H Burns, Lin Zhao, Dajiang Zhu, Quanzheng Li, Wei Liu, et al.

</span>
<span class="ltx_bibblock">Clinicalradiobert: Knowledge-infused few shot learning for clinical notes named entity recognition.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Machine Learning in Medical Imaging: 13th International Workshop, MLMI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 18, 2022, Proceedings</span>, pages 269–278. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Haixing Dai, Yiwei Li, Zhengliang Liu, Lin Zhao, Zihao Wu, Suhang Song, Ye Shen, Dajiang Zhu, Xiang Li, Sheng Li, et al.

</span>
<span class="ltx_bibblock">Ad-autogpt: An autonomous gpt for alzheimer’s disease infodemiology.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2306.10095</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Huan Zhao, Qian Ling, Yi Pan, Tianyang Zhong, Jin-Yu Hu, Junjie Yao, Fengqian Xiao, Zhenxiang Xiao, Yutong Zhang, San-Hua Xu, et al.

</span>
<span class="ltx_bibblock">Ophtha-llama2: A large language model for ophthalmology.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2312.04906</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Kai Zhang, Rong Zhou, Eashan Adhikarla, Zhiling Yan, Yixin Liu, Jun Yu, Zhengliang Liu, Xun Chen, Brian D Davison, Hui Ren, et al.

</span>
<span class="ltx_bibblock">A generalist vision–language foundation model for diverse biomedical tasks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">Nature Medicine</span>, pages 1–13, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Zhengliang Liu, Peilong Wang, Yiwei Li, Jason Holmes, Peng Shu, Lian Zhang, Chenbin Liu, Ninghao Liu, Dajiang Zhu, Xiang Li, et al.

</span>
<span class="ltx_bibblock">Radonc-gpt: A large language model for radiation oncology.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2309.10160</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Zhengliang Liu, Peilong Wang, Yiwei Li, Jason M Holmes, Peng Shu, Lian Zhang, Xiang Li, Quanzheng Li, Sujay A Vora, Samir Patel, et al.

</span>
<span class="ltx_bibblock">Fine-tuning large language models for radiation oncology, a highly specialized healthcare domain.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">International Journal of Particle Therapy</span>, 12:100428, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Yanjun Lyu, Zihao Wu, Lu Zhang, Jing Zhang, Yiwei Li, Wei Ruan, Zhengliang Liu, Xiaowei Yu, Chao Cao, Tong Chen, et al.

</span>
<span class="ltx_bibblock">Gp-gpt: Large language model for gene-phenotype mapping.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2409.09825</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Jiaqi Wang, Hanqi Jiang, Yiheng Liu, Chong Ma, Xu Zhang, Yi Pan, Mengyuan Liu, Peiran Gu, Sichen Xia, Wenjun Li, et al.

</span>
<span class="ltx_bibblock">A comprehensive review of multimodal large language models: Performance and challenges across different tasks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2408.01319</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Yue Huang, Lichao Sun, Haoran Wang, Siyuan Wu, Qihui Zhang, Yuan Li, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, et al.

</span>
<span class="ltx_bibblock">Position: Trustllm: Trustworthiness in large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">International Conference on Machine Learning</span>, pages 20166–20270. PMLR, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Zhengliang Liu, Lu Zhang, Zihao Wu, Xiaowei Yu, Chao Cao, Haixing Dai, Ninghao Liu, Jun Liu, Wei Liu, Quanzheng Li, et al.

</span>
<span class="ltx_bibblock">Surviving chatgpt in healthcare.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">Frontiers in Radiology</span>, 3:1224682, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Yue Huang, Lichao Sun, Haoran Wang, Siyuan Wu, Qihui Zhang, Yuan Li, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, et al.

</span>
<span class="ltx_bibblock">Trustllm: Trustworthiness in large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2401.05561</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Yang Zhenyuan, Liu Zhengliang, Zhang Jing, Lu Cen, Tai Jiaxin, Zhong Tianyang, Li Yiwei, Zhao Siyan, Yao Teng, Liu Qing, et al.

</span>
<span class="ltx_bibblock">Analyzing nobel prize literature with large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2410.18142</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Jiaqi Wang, Enze Shi, Sigang Yu, Zihao Wu, Chong Ma, Haixing Dai, Qiushi Yang, Yanqing Kang, Jinru Wu, Huawen Hu, et al.

</span>
<span class="ltx_bibblock">Prompt engineering for healthcare: Methodologies and applications.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2304.14670</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Zhengliang Liu, Aoxiao Zhong, Yiwei Li, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma, Peng Shu, Cheng Chen, Sekeun Kim, et al.

</span>
<span class="ltx_bibblock">Tailoring large language models to radiology: A preliminary approach to llm adaptation for a highly specialized domain.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">International Workshop on Machine Learning in Medical Imaging</span>, pages 464–473. Springer, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Jie Tian, Jixin Hou, Zihao Wu, Peng Shu, Zhengliang Liu, Yujie Xiang, Beikang Gu, Nicholas Filla, Yiwei Li, Ning Liu, et al.

</span>
<span class="ltx_bibblock">Assessing large language models in mechanical engineering education: A study on mechanics-focused conceptual understanding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2401.12983</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Gyeong-Geon Lee, Lehong Shi, Ehsan Latif, Yizhu Gao, Arne Bewersdorf, Matthew Nyaaba, Shuchen Guo, Zihao Wu, Zhengliang Liu, Hui Wang, et al.

</span>
<span class="ltx_bibblock">Multimodality of ai for education: Towards artificial general intelligence.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2312.06037</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Peng Shu, Huaqin Zhao, Hanqi Jiang, Yiwei Li, Shaochen Xu, Yi Pan, Zihao Wu, Zhengliang Liu, Guoyu Lu, Le Guan, et al.

</span>
<span class="ltx_bibblock">Llms for coding and robotics education.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2402.06116</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Ehsan Latif, Gengchen Mai, Matthew Nyaaba, Xuansheng Wu, Ninghao Liu, Guoyu Lu, Sheng Li, Tianming Liu, and Xiaoming Zhai.

</span>
<span class="ltx_bibblock">Artificial general intelligence (agi) for education.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2304.12479</span>, 1, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Jiaqi Wang, Zihao Wu, Yiwei Li, Hanqi Jiang, Peng Shu, Enze Shi, Huawen Hu, Chong Ma, Yiheng Liu, Xuhui Wang, et al.

</span>
<span class="ltx_bibblock">Large language models for robotics: Opportunities, challenges, and perspectives.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2401.04334</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Yiheng Liu, Hao He, Tianle Han, Xu Zhang, Mengyuan Liu, Jiaming Tian, Yutong Zhang, Jiaqi Wang, Xiaohui Gao, Tianyang Zhong, et al.

</span>
<span class="ltx_bibblock">Understanding llms: A comprehensive overview from training to inference.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2401.02038</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Ehsan Latif, Yifan Zhou, Shuchen Guo, Yizhu Gao, Lehong Shi, Matthew Nayaaba, Gyeonggeon Lee, Liang Zhang, Arne Bewersdorff, Luyang Fang, et al.

</span>
<span class="ltx_bibblock">A systematic assessment of openai o1-preview for higher order thinking in education.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2410.21287</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Xiang Li, Lin Zhao, Lu Zhang, Zihao Wu, Zhengliang Liu, Hanqi Jiang, Chao Cao, Shaochen Xu, Yiwei Li, Haixing Dai, et al.

</span>
<span class="ltx_bibblock">Artificial general intelligence for medical imaging analysis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">IEEE Reviews in Biomedical Engineering</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Peilong Wang, Jason Holmes, Zhengliang Liu, Dequan Chen, Tianming Liu, Jiajian Shen, and Wei Liu.

</span>
<span class="ltx_bibblock">A recent evaluation on the performance of llms on radiation oncology physics using questions of randomly shuffled options.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2412.10622</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Zhaojun Ding, Zhengliang Liu, Hanqi Jiang, Yizhu Gao, Xiaoming Zhai, Tianming Liu, and Ninghao Liu.

</span>
<span class="ltx_bibblock">Foundation models for low-resource language education (vision paper).

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2412.04774</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Junhao Chen, Peng Shu, Yiwei Li, Huaqin Zhao, Hanqi Jiang, Yi Pan, Yifan Zhou, Zhengliang Liu, Lewis C Howe, and Tianming Liu.

</span>
<span class="ltx_bibblock">Queen: A large language model for quechua-english translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2412.05184</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Yutong Zhang, Yi Pan, Tianyang Zhong, Peixin Dong, Kangni Xie, Yuxiao Liu, Hanqi Jiang, Zihao Wu, Zhengliang Liu, Wei Zhao, et al.

</span>
<span class="ltx_bibblock">Potential of multimodal large language models for data mining of medical images and free-text reports.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">Meta-Radiology</span>, 2(4):100103, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Tianyang Zhong, Zhenyuan Yang, Zhengliang Liu, Ruidong Zhang, Yiheng Liu, Haiyang Sun, Yi Pan, Yiwei Li, Yifan Zhou, Hanqi Jiang, et al.

</span>
<span class="ltx_bibblock">Opportunities and challenges of large language models for low-resource languages in humanities research.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2412.04497</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Hanqi Jiang, Yi Pan, Junhao Chen, Zhengliang Liu, Yifan Zhou, Peng Shu, Yiwei Li, Huaqin Zhao, Stephen Mihm, Lewis C Howe, et al.

</span>
<span class="ltx_bibblock">Oraclesage: Towards unified visual-linguistic understanding of oracle bone scripts through cross-modal knowledge fusion.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2411.17837</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Wenxiong Liao, Zhengliang Liu, Yiyang Zhang, Xiaoke Huang, Ninghao Liu, Tianming Liu, Quanzheng Li, Xiang Li, and Hongmin Cai.

</span>
<span class="ltx_bibblock">Zero-shot relation triplet extraction as next-sentence prediction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">Knowledge-Based Systems</span>, 304:112507, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Lian Zhang, Zhengliang Liu, Lu Zhang, Zihao Wu, Xiaowei Yu, Jason Holmes, Hongying Feng, Haixing Dai, Xiang Li, Quanzheng Li, et al.

</span>
<span class="ltx_bibblock">Generalizable and promptable artificial intelligence model to augment clinical delineation in radiation oncology.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">Medical physics</span>, 51(3):2187–2199, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Chenjiao Tan, Qian Cao, Yiwei Li, Jielu Zhang, Xiao Yang, Huaqin Zhao, Zihao Wu, Zhengliang Liu, Hao Yang, Nemin Wu, et al.

</span>
<span class="ltx_bibblock">On the promises and challenges of multimodal foundation models for geographical, environmental, agricultural, and urban planning applications.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2312.17016</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Yucheng Shi, Peng Shu, Zhengliang Liu, Zihao Wu, Quanzheng Li, and Xiang Li.

</span>
<span class="ltx_bibblock">Mgh radiology llama: A llama 3 70b model for radiology.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:2408.11848</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Peng Shu, Junhao Chen, Zhengliang Liu, Hui Wang, Zihao Wu, Tianyang Zhong, Yiwei Li, Huaqin Zhao, Hanqi Jiang, Yi Pan, et al.

</span>
<span class="ltx_bibblock">Transcending language boundaries: Harnessing llms for low-resource language translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2411.11295</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Jiaqi Wang, Huan Zhao, Zhenyuan Yang, Peng Shu, Junhao Chen, Haobo Sun, Ruixi Liang, Shixin Li, Pengcheng Shi, Longjun Ma, et al.

</span>
<span class="ltx_bibblock">Legal evalutions and challenges of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:2411.10137</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Jason Holmes, Lian Zhang, Yuzhen Ding, Hongying Feng, Zhengliang Liu, Tianming Liu, William W Wong, Sujay A Vora, Jonathan B Ashman, and Wei Liu.

</span>
<span class="ltx_bibblock">Benchmarking a foundation large language model on its ability to relabel structure names in accordance with the american association of physicists in medicine task group-263 report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">Practical Radiation Oncology</span>, 14(6):e515–e521, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Lin Zhao, Zihao Wu, Haixing Dai, Zhengliang Liu, Tuo Zhang, Dajiang Zhu, and Tianming Liu.

</span>
<span class="ltx_bibblock">Embedding human brain function via transformer.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">International Conference on Medical Image Computing and Computer-Assisted Intervention</span>, pages 366–375. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Lin Zhao, Zihao Wu, Haixing Dai, Zhengliang Liu, Xintao Hu, Tuo Zhang, Dajiang Zhu, and Tianming Liu.

</span>
<span class="ltx_bibblock">A generic framework for embedding human brain function with temporally correlated autoencoder.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">Medical Image Analysis</span>, 89:102892, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Zhengliang Liu, Yiwei Li, Oleksandra Zolotarevych, Rongwei Yang, and Tianming Liu.

</span>
<span class="ltx_bibblock">Llm-potus score: A framework of analyzing presidential debates with large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2409.08147</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Zhenyuan Yang, Xuhui Lin, Qinyi He, Ziye Huang, Zhengliang Liu, Hanqi Jiang, Peng Shu, Zihao Wu, Yiwei Li, Stephen Law, et al.

</span>
<span class="ltx_bibblock">Examining the commitments and difficulties inherent in multimodal foundation models for street view imagery.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:2408.12821</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Xinyu Gong, Jianli Zhang, Qi Gan, Yuxi Teng, Jixin Hou, Yanjun Lyu, Zhengliang Liu, Zihao Wu, Runpeng Dai, Yusong Zou, et al.

</span>
<span class="ltx_bibblock">Advancing microbial production through artificial intelligence-aided biology.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">Biotechnology Advances</span>, page 108399, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Subhabrata Mukherjee, Paul Gamble, Markel Sanz Ausin, Neel Kant, Kriti Aggarwal, Neha Manjunath, Debajyoti Datta, Zhengliang Liu, Jiayuan Ding, Sophia Busacca, et al.

</span>
<span class="ltx_bibblock">Polaris: A safety-focused llm constellation architecture for healthcare.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2403.13313</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Shaochen Xu, Zihao Wu, Huaqin Zhao, Peng Shu, Zhengliang Liu, Wenxiong Liao, Sheng Li, Andrea Sikora, Tianming Liu, and Xiang Li.

</span>
<span class="ltx_bibblock">Reasoning before comparison: Llm-enhanced semantic similarity metrics for domain specialized text analysis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">arXiv preprint arXiv:2402.11398</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Ehsan Latif, Luyang Fang, Ping Ma, and Xiaoming Zhai.

</span>
<span class="ltx_bibblock">Knowledge distillation of llms for automatic scoring of science assessments.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">International Conference on Artificial Intelligence in Education</span>, pages 166–174. Springer, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Zhengliang Liu, Jason Holmes, Wenxiong Liao, Chenbin Liu, Lian Zhang, Hongying Feng, Peilong Wang, Muhammad Ali Elahi, Hongmin Cai, Lichao Sun, et al.

</span>
<span class="ltx_bibblock">The radiation oncology nlp database.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib57.1.1">arXiv preprint arXiv:2401.10995</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Yaonai Wei, Tianyang Zhong, Songyao Zhang, Xiao Li, Tuo Zhang, Lin Zhao, Zhengliang Liu, Muheng Shang, Tianming Liu, Han Zhang, et al.

</span>
<span class="ltx_bibblock">Chat2brain: A method for mapping open-ended semantic queries to brain activation maps.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</span>, pages 1523–1530. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Zhengliang Liu, Hanqi Jiang, Tianyang Zhong, Zihao Wu, Chong Ma, Yiwei Li, Xiaowei Yu, Yutong Zhang, Yi Pan, Peng Shu, et al.

</span>
<span class="ltx_bibblock">Holistic evaluation of gpt-4v for biomedical imaging.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib59.1.1">arXiv preprint arXiv:2312.05256</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Zhengliang Liu, Mengshen He, Zuowei Jiang, Zihao Wu, Haixing Dai, Lian Zhang, Siyi Luo, Tianle Han, Xiang Li, Xi Jiang, et al.

</span>
<span class="ltx_bibblock">Survey on natural language processing in medical image analysis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib60.1.1">Zhong nan da xue xue bao. Yi xue ban= Journal of Central South University. Medical Sciences</span>, 47(8):981–993, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Zihao Wu, Lu Zhang, Chao Cao, Xiaowei Yu, Haixing Dai, Chong Ma, Zhengliang Liu, Lin Zhao, Gang Li, Wei Liu, et al.

</span>
<span class="ltx_bibblock">Exploring the trade-offs: Unified large language models vs local fine-tuned models for highly-specific radiology nli task.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib61.1.1">arXiv preprint arXiv:2304.09138</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Zhenxiang Xiao, Yuzhong Chen, Lu Zhang, Junjie Yao, Zihao Wu, Xiaowei Yu, Yi Pan, Lin Zhao, Chong Ma, Xinyu Liu, et al.

</span>
<span class="ltx_bibblock">Instruction-vit: Multi-modal prompts for instruction learning in vit.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib62.1.1">arXiv preprint arXiv:2305.00201</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Hongmin Cai, Xiaoke Huang, Zhengliang Liu, Wenxiong Liao, Haixing Dai, Zihao Wu, Dajiang Zhu, Hui Ren, Quanzheng Li, Tianming Liu, et al.

</span>
<span class="ltx_bibblock">Exploring multimodal approaches for alzheimer’s disease detection using patient speech transcript and audio data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib63.1.1">arXiv preprint arXiv:2307.02514</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Jason Holmes, Zhengliang Liu, Lian Zhang, Yuzhen Ding, Terence T Sio, Lisa A McGee, Jonathan B Ashman, Xiang Li, Tianming Liu, Jiajian Shen, et al.

</span>
<span class="ltx_bibblock">Evaluating large language models on a highly-specialized topic, radiation oncology physics.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib64.1.1">arXiv preprint arXiv:2304.01938</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Zhengliang Liu, Zihao Wu, Mengxuan Hu, Bokai Zhao, Lin Zhao, Tianyi Zhang, Haixing Dai, Xianyan Chen, Ye Shen, Sheng Li, et al.

</span>
<span class="ltx_bibblock">Pharmacygpt: The ai pharmacist.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib65.1.1">arXiv preprint arXiv:2307.10432</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Zihan Guan, Zihao Wu, Zhengliang Liu, Dufan Wu, Hui Ren, Quanzheng Li, Xiang Li, and Ninghao Liu.

</span>
<span class="ltx_bibblock">Cohortgpt: An enhanced gpt for participant recruitment in clinical study.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib66.1.1">arXiv preprint arXiv:2307.11346</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Zhengliang Liu, Tianyang Zhong, Yiwei Li, Yutong Zhang, Yi Pan, Zihao Zhao, Peixin Dong, Chao Cao, Yuxiao Liu, Peng Shu, et al.

</span>
<span class="ltx_bibblock">Evaluating large language models for radiology natural language processing.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib67.1.1">arXiv preprint arXiv:2307.13693</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Hongmin Cai, Xiaoke Huang, Zhengliang Liu, Wenxiong Liao, Haixing Dai, Zihao Wu, Dajiang Zhu, Hui Ren, Quanzheng Li, Tianming Liu, et al.

</span>
<span class="ltx_bibblock">Multimodal approaches for alzheimer’s detection using patients’ speech and transcript.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib68.1.1">International Conference on Brain Informatics</span>, pages 395–406. Springer, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Yucheng Shi, Shaochen Xu, Zhengliang Liu, Tianming Liu, Xiang Li, and Ninghao Liu.

</span>
<span class="ltx_bibblock">Mededit: Model editing for medical question answering with external knowledge bases.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib69.1.1">arXiv preprint arXiv:2309.16035</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Chenhao Tang, Zhengliang Liu, Chong Ma, Zihao Wu, Yiwei Li, Wei Liu, Dajiang Zhu, Quanzheng Li, Xiang Li, Tianming Liu, et al.

</span>
<span class="ltx_bibblock">Policygpt: Automated analysis of privacy policies with large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib70.1.1">arXiv preprint arXiv:2309.10238</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Zhengliang Liu, Yiwei Li, Qian Cao, Junwen Chen, Tianze Yang, Zihao Wu, John Hale, John Gibbs, Khaled Rasheed, Ninghao Liu, et al.

</span>
<span class="ltx_bibblock">Transformation vs tradition: Artificial general intelligence (agi) for arts and humanities.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib71.1.1">arXiv preprint arXiv:2310.19626</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Tianyang Zhong, Wei Zhao, Yutong Zhang, Yi Pan, Peixin Dong, Zuowei Jiang, Xiaoyan Kui, Youlan Shang, Li Yang, Yaonai Wei, et al.

</span>
<span class="ltx_bibblock">Chatradio-valuer: a chat large language model for generalizable radiology report generation based on multi-institution and multi-system data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib72.1.1">arXiv preprint arXiv:2310.05242</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Xinyu Gong, Jason Holmes, Yiwei Li, Zhengliang Liu, Qi Gan, Zihao Wu, Jianli Zhang, Yusong Zou, Yuxi Teng, Tian Jiang, et al.

</span>
<span class="ltx_bibblock">Evaluating the potential of leading large language models in reasoning biology questions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib73.1.1">arXiv preprint arXiv:2311.07582</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Wenxiong Liao, Zhengliang Liu, Yiyang Zhang, Xiaoke Huang, Fei Qi, Siqi Ding, Hui Ren, Zihao Wu, Haixing Dai, Sheng Li, et al.

</span>
<span class="ltx_bibblock">Coarse-to-fine knowledge graph domain adaptation based on distantly-supervised iterative training.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib74.1.1">2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</span>, pages 1294–1299. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Jason Holmes, Rui Peng, Yiwei Li, Jinyu Hu, Zhengliang Liu, Zihao Wu, Huan Zhao, Xi Jiang, Wei Liu, Hong Wei, et al.

</span>
<span class="ltx_bibblock">Evaluating multiple large language models in pediatric ophthalmology.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib75.1.1">arXiv preprint arXiv:2311.04368</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Saed Rezayi, Zhengliang Liu, Zihao Wu, Chandra Dhakal, Bao Ge, Haixing Dai, Gengchen Mai, Ninghao Liu, Chen Zhen, Tianming Liu, et al.

</span>
<span class="ltx_bibblock">Exploring new frontiers in agricultural nlp: Investigating the potential of large language models for food applications.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib76.1.1">IEEE Transactions on Big Data</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Fei Dou, Jin Ye, Geng Yuan, Qin Lu, Wei Niu, Haijian Sun, Le Guan, Guoyu Lu, Gengchen Mai, Ninghao Liu, et al.

</span>
<span class="ltx_bibblock">Towards artificial general intelligence (agi) in the internet of things (iot): Opportunities and challenges.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib77.1.1">arXiv preprint arXiv:2309.07438</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Jason Holmes, Lian Zhang, Yuzhen Ding, Hongying Feng, Zhengliang Liu, Tianming Liu, William W Wong, Sujay A Vora, Jonathan B Ashman, and Wei Liu.

</span>
<span class="ltx_bibblock">Benchmarking a foundation llm on its ability to re-label structure names in accordance with the aapm tg-263 report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib78.1.1">arXiv preprint arXiv:2310.03874</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Rico Sennrich.

</span>
<span class="ltx_bibblock">Neural machine translation of rare words with subword units.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib79.1.1">arXiv preprint arXiv:1508.07909</span>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Mike Schuster and Kaisuke Nakajima.

</span>
<span class="ltx_bibblock">Japanese and korean voice search.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib80.1.1">2012 IEEE international conference on acoustics, speech and signal processing (ICASSP)</span>, pages 5149–5152. IEEE, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Taku Kudo.

</span>
<span class="ltx_bibblock">Subword regularization: Improving neural network translation models with multiple subword candidates.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib81.1.1">arXiv preprint arXiv:1804.10959</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Yoshua Bengio, Aaron Courville, and Pascal Vincent.

</span>
<span class="ltx_bibblock">Representation learning: A review and new perspectives.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib82.1.1">IEEE transactions on pattern analysis and machine intelligence</span>, 35(8):1798–1828, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib83.1.1">Advances in neural information processing systems</span>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">A structured self-attentive sentence embedding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib84.1.1">arXiv preprint arXiv:1703.03130</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Ziyu Liu, Azadeh Alavi, Minyi Li, and Xiang Zhang.

</span>
<span class="ltx_bibblock">Self-supervised contrastive learning for medical time series: A systematic review.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib85.1.1">Sensors</span>, 23(9):4221, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Trevor Hastie, Robert Tibshirani, Jerome Friedman, Trevor Hastie, Robert Tibshirani, and Jerome Friedman.

</span>
<span class="ltx_bibblock">Overview of supervised learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib86.1.1">The elements of statistical learning: Data mining, inference, and prediction</span>, pages 9–41, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Geoffrey E Hinton and Ruslan R Salakhutdinov.

</span>
<span class="ltx_bibblock">Reducing the dimensionality of data with neural networks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib87.1.1">science</span>, 313(5786):504–507, 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Mark A Kramer.

</span>
<span class="ltx_bibblock">Nonlinear principal component analysis using autoassociative neural networks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib88.1.1">AIChE journal</span>, 37(2):233–243, 1991.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol.

</span>
<span class="ltx_bibblock">Extracting and composing robust features with denoising autoencoders.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib89.1.1">Proceedings of the 25th international conference on Machine learning</span>, pages 1096–1103, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, and Léon Bottou.

</span>
<span class="ltx_bibblock">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib90.1.1">Journal of machine learning research</span>, 11(12), 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel.

</span>
<span class="ltx_bibblock">Language models as knowledge bases?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib91.1.1">arXiv preprint arXiv:1909.01066</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Jeremy Howard and Sebastian Ruder.

</span>
<span class="ltx_bibblock">Universal language model fine-tuning for text classification.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib92.1.1">arXiv preprint arXiv:1801.06146</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib93.1.1">The Journal of Machine Learning Research</span>, 21(1):5485–5551, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Yuan Yao, Ao Zhang, Liang Zhang, et al.

</span>
<span class="ltx_bibblock">Pre-trained models: Past, present and future.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib94.1.1">AI Open</span>, 2:225–250, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Lefteris Koumakis.

</span>
<span class="ltx_bibblock">Deep learning models in genomics; are we there yet?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib95.1.1">Computational and Structural Biotechnology Journal</span>, 18:1466–1473, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
JD Watson and FH Crick.

</span>
<span class="ltx_bibblock">On protein synthesis.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib96.1.1">The Symposia of the Society for Experimental Biology</span>, volume 12, pages 138–163, 1958.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
Thomas Schlitt, Kimmo Palin, Johan Rung, Sabine Dietmann, Michael Lappe, Esko Ukkonen, and Alvis Brazma.

</span>
<span class="ltx_bibblock">From gene networks to gene function.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib97.1.1">Genome research</span>, 13(12):2568–2576, 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
Chan Yeong Kim, Seungbyn Baek, Junha Cha, Sunmo Yang, Eiru Kim, Edward M Marcotte, Traver Hart, and Insuk Lee.

</span>
<span class="ltx_bibblock">Humannet v3: an improved database of human gene networks for disease research.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib98.1.1">Nucleic acids research</span>, 50(D1):D632–D639, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
Dan Ofer, Nadav Brandes, and Michal Linial.

</span>
<span class="ltx_bibblock">The language of proteins: Nlp, machine learning &amp; protein sequences.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib99.1.1">Computational and Structural Biotechnology Journal</span>, 19:1750–1758, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al.

</span>
<span class="ltx_bibblock">Evolutionary-scale prediction of atomic-level protein structure with a language model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib100.1.1">Science</span>, 379(6637):1123–1130, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
Tomas Hayes, Roshan Rao, Halil Akin, Nicholas J Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Q Tran, Jonathan Deaton, Marius Wiggert, et al.

</span>
<span class="ltx_bibblock">Simulating 500 million years of evolution with a language model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib101.1.1">bioRxiv</span>, pages 2024–07, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
Vincent J Lynch, Robert D Leclerc, Gemma May, and Günter P Wagner.

</span>
<span class="ltx_bibblock">Transposon-mediated rewiring of gene regulatory networks contributed to the evolution of pregnancy in mammals.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib102.1.1">Nature genetics</span>, 43(11):1154–1159, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
Gene Ontology Consortium.

</span>
<span class="ltx_bibblock">The gene ontology (go) database and informatics resource.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib103.1.1">Nucleic acids research</span>, 32(suppl_1):D258–D261, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
Wojciech Samek, Grégoire Montavon, Sebastian Lapuschkin, Christopher J Anders, and Klaus-Robert Müller.

</span>
<span class="ltx_bibblock">Explaining deep neural networks and beyond: A review of methods and applications.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib104.1.1">Proceedings of the IEEE</span>, 109(3):247–278, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib105.1.1">2009 IEEE conference on computer vision and pattern recognition</span>, pages 248–255. Ieee, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
Farhad Pourpanah, Moloud Abdar, Yuxuan Luo, Xinlei Zhou, Ran Wang, Chee Peng Lim, Xi-Zhao Wang, and QM Jonathan Wu.

</span>
<span class="ltx_bibblock">A review of generalized zero-shot learning methods.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib106.1.1">IEEE transactions on pattern analysis and machine intelligence</span>, 45(4):4051–4070, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni.

</span>
<span class="ltx_bibblock">Generalizing from a few examples: A survey on few-shot learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib107.1.1">ACM computing surveys (csur)</span>, 53(3):1–34, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
Roberto Navigli, Simone Conia, and Björn Ross.

</span>
<span class="ltx_bibblock">Biases in large language models: origins, inventory, and discussion.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib108.1.1">ACM Journal of Data and Information Quality</span>, 15(2):1–21, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
Honglin Li, Chenglu Zhu, Yunlong Zhang, Yuxuan Sun, Zhongyi Shui, Wenwei Kuang, Sunyi Zheng, and Lin Yang.

</span>
<span class="ltx_bibblock">Task-specific fine-tuning via variational information bottleneck for weakly-supervised pathology whole slide image classification.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib109.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 7454–7463, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
Jiawei Zheng, Hanghai Hong, Xiaoli Wang, Jingsong Su, Yonggui Liang, and Shikai Wu.

</span>
<span class="ltx_bibblock">Fine-tuning large language models for domain-specific machine translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib110.1.1">arXiv preprint arXiv:2402.15061</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib111.1.1">Advances in neural information processing systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al.

</span>
<span class="ltx_bibblock">Parameter-efficient fine-tuning of large-scale pre-trained language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib112.1.1">Nature Machine Intelligence</span>, 5(3):220–235, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib113.1.1">arXiv preprint arXiv:2106.09685</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jia-Wei Low, Lidong Bing, and Luo Si.

</span>
<span class="ltx_bibblock">On the effectiveness of adapter-based tuning for pretrained language model adaptation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib114.1.1">arXiv preprint arXiv:2106.03164</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei.

</span>
<span class="ltx_bibblock">Deep reinforcement learning from human preferences.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib115.1.1">Advances in neural information processing systems</span>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.

</span>
<span class="ltx_bibblock">Proximal policy optimization algorithms.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib116.1.1">arXiv preprint arXiv:1707.06347</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a reward model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib117.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané.

</span>
<span class="ltx_bibblock">Concrete problems in ai safety.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib118.1.1">arXiv preprint arXiv:1606.06565</span>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
Alexander Pan, Erik Jones, Meena Jagadeesan, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Feedback loops with language models drive in-context reward hacking.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib119.1.1">arXiv preprint arXiv:2402.06627</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
Geoffrey Hinton.

</span>
<span class="ltx_bibblock">Distilling the knowledge in a neural network.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib120.1.1">arXiv preprint arXiv:1503.02531</span>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou.

</span>
<span class="ltx_bibblock">A survey on knowledge distillation of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib121.1.1">arXiv preprint arXiv:2402.13116</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li, Guiming Chen, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, et al.

</span>
<span class="ltx_bibblock">Huatuogpt, towards taming language model to be a doctor.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib122.1.1">arXiv preprint arXiv:2305.15075</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.

</span>
<span class="ltx_bibblock">Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib124.1.1">arXiv preprint arXiv:2305.02301</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al.

</span>
<span class="ltx_bibblock">Deepseek-v3 technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib125.1.1">arXiv preprint arXiv:2412.19437</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
Roya Firoozi, Johnathan Tucker, Stephen Tian, Anirudha Majumdar, Jiankai Sun, Weiyu Liu, Yuke Zhu, Shuran Song, Ashish Kapoor, Karol Hausman, et al.

</span>
<span class="ltx_bibblock">Foundation models in robotics: Applications, challenges, and the future.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib126.1.1">The International Journal of Robotics Research</span>, page 02783649241281508, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
Jiaheng Liu, Chenchen Zhang, Jinyang Guo, Yuanxing Zhang, Haoran Que, Ken Deng, Zhiqi Bai, Jie Liu, Ge Zhang, Jiakai Wang, et al.

</span>
<span class="ltx_bibblock">Ddk: Distilling domain knowledge for efficient large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib127.1.1">arXiv preprint arXiv:2407.16154</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
Luyang Fang, Yongkai Chen, Wenxuan Zhong, and Ping Ma.

</span>
<span class="ltx_bibblock">Bayesian knowledge distillation: A bayesian perspective of distillation with uncertainty quantification.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib128.1.1">Forty-first International Conference on Machine Learning</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
Anoop Korattikara, Vivek Rathod, Kevin Murphy, and Max Welling.

</span>
<span class="ltx_bibblock">Bayesian dark knowledge.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib129.1.1">arXiv preprint arXiv:1506.04416</span>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Hervé Jegou, Edouard Grave, Armand Joulin, Timothée Lacroix, et al.

</span>
<span class="ltx_bibblock">The llama 3 herd of models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib130.1.1">arXiv preprint arXiv:2407.21783</span>, July 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
Shaochen Xu, Yifan Zhou, Zhengliang Liu, Zihao Wu, Tianyang Zhong, Huaqin Zhao, Yiwei Li, Hanqi Jiang, Yi Pan, Junhao Chen, et al.

</span>
<span class="ltx_bibblock">Towards next-generation medical agent: How o1 is reshaping decision-making in medical scenarios.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib131.1.1">arXiv preprint arXiv:2411.14461</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
Shubo Tian, Qiao Jin, Lana Yeganova, Po-Ting Lai, Qingqing Zhu, Xiuying Chen, Yifan Yang, Qingyu Chen, Won Kim, Donald C Comeau, et al.

</span>
<span class="ltx_bibblock">Opportunities and challenges for chatgpt and large language models in biomedicine and health.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib132.1.1">Briefings in Bioinformatics</span>, 25(1):bbad493, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu.

</span>
<span class="ltx_bibblock">Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib133.1.1">Conference on health, inference, and learning</span>, pages 248–260. PMLR, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits.

</span>
<span class="ltx_bibblock">What disease does this patient have? a large-scale open domain question answering dataset from medical exams.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib134.1.1">Applied Sciences</span>, 11(14):6421, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael R Alvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopoulos, et al.

</span>
<span class="ltx_bibblock">An overview of the bioasq large-scale biomedical semantic indexing and question answering competition.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib135.1.1">BMC bioinformatics</span>, 16:1–28, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
Anastasios Nentidis, Georgios Katsimpras, Anastasia Krithara, and Georgios Paliouras.

</span>
<span class="ltx_bibblock">Overview of bioasq tasks 12b and synergy12 in clef2024.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib136.1.1">Working Notes of CLEF</span>, 2024, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua Lu.

</span>
<span class="ltx_bibblock">Pubmedqa: A dataset for biomedical research question answering.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib137.1.1">arXiv preprint arXiv:1909.06146</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
Alistair EW Johnson, Tom J Pollard, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz, and Steven Horng.

</span>
<span class="ltx_bibblock">Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib138.1.1">arXiv preprint arXiv:1901.07042</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
Chih-Hsuan Wei, Yifan Peng, Robert Leaman, Allan Peter Davis, Carolyn J Mattingly, Jiao Li, Thomas C Wiegers, and Zhiyong Lu.

</span>
<span class="ltx_bibblock">Assessing the state of the art in biomedical relation extraction: overview of the biocreative v chemical-disease relation (cdr) task.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib139.1.1">Database</span>, 2016:baw032, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
Rezarta Islamaj Doğan, Robert Leaman, and Zhiyong Lu.

</span>
<span class="ltx_bibblock">Ncbi disease corpus: a resource for disease name recognition and concept normalization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib140.1.1">Journal of biomedical informatics</span>, 47:1–10, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
Olivier Taboureau, Sonny Kim Nielsen, Karine Audouze, Nils Weinhold, Daniel Edsgärd, Francisco S Roque, Irene Kouskoumvekaki, Alina Bora, Ramona Curpan, Thomas Skøt Jensen, et al.

</span>
<span class="ltx_bibblock">Chemprot: a disease chemical biology database.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib141.1.1">Nucleic acids research</span>, 39(suppl_1):D367–D372, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
Sonny Kim Kjærulff, Louis Wich, Jens Kringelum, Ulrik P Jacobsen, Irene Kouskoumvekaki, Karine Audouze, Ole Lund, Søren Brunak, Tudor I Oprea, and Olivier Taboureau.

</span>
<span class="ltx_bibblock">Chemprot-2.0: visual navigation in a disease chemical biology database.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib142.1.1">Nucleic acids research</span>, 41(D1):D464–D469, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
Jens Kringelum, Sonny Kim Kjaerulff, Søren Brunak, Ole Lund, Tudor I Oprea, and Olivier Taboureau.

</span>
<span class="ltx_bibblock">Chemprot-3.0: a global chemical biology diseases mapping.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib143.1.1">Database</span>, 2016:bav123, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
María Herrero-Zazo, Isabel Segura-Bedmar, Paloma Martínez, and Thierry Declerck.

</span>
<span class="ltx_bibblock">The ddi corpus: An annotated corpus with pharmacological substances and drug–drug interactions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib144.1.1">Journal of biomedical informatics</span>, 46(5):914–920, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
Àlex Bravo, Janet Piñero, Núria Queralt-Rosinach, Michael Rautschka, and Laura I Furlong.

</span>
<span class="ltx_bibblock">Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib145.1.1">BMC bioinformatics</span>, 16:1–17, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
Larry Smith, Lorraine K Tanabe, Rie Johnson nee Ando, Cheng-Ju Kuo, I-Fang Chung, Chun-Nan Hsu, Yu-Shi Lin, Roman Klinger, Christoph M Friedrich, Kuzman Ganchev, et al.

</span>
<span class="ltx_bibblock">Overview of biocreative ii gene mention recognition.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib146.1.1">Genome biology</span>, 9:1–19, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
Nigel Collier, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka Tateisi, and Jin-Dong Kim.

</span>
<span class="ltx_bibblock">Introduction to the bio-entity recognition task at jnlpba.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib147.1.1">Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP)</span>, pages 73–78, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
James Pustejovsky, José Castano, Roser Sauri, Jason Zhang, and Wei Luo.

</span>
<span class="ltx_bibblock">Medstract: creating large-scale information servers from biomedical texts.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib148.1.1">Proceedings of the ACL-02 workshop on Natural language processing in the biomedical domain</span>, pages 85–92, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
Caroline Gasperin, Nikiforos Karamanis, and Ruth Seal.

</span>
<span class="ltx_bibblock">Annotation of anaphoric relations in biomedical full-text articles using a domain-relevant scheme.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib149.1.1">Proceedings of DAARC</span>, volume 2007. Citeseer, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
Jian Su, Xiaofeng Yang, Huaqing Hong, Yuka Tateisi, and Jun’ichi Tsujii.

</span>
<span class="ltx_bibblock">Coreference resolution in biomedical texts: a machine learning approach.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib150.1.1">Ontologies and Text Mining for Life Sciences</span>, 8, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
Isabel Segura-Bedmar, Mario Crespo, César de Pablo-Sánchez, and Paloma Martínez.

</span>
<span class="ltx_bibblock">Resolving anaphoras for the extraction of drug-drug interactions in pharmacological documents.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib151.1.1">BMC bioinformatics</span>, volume 11, pages 1–9. Springer, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
Ngan Nguyen, Jin-Dong Kim, and Jun’ichi Tsujii.

</span>
<span class="ltx_bibblock">Overview of bionlp 2011 protein coreference shared task.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib152.1.1">Proceedings of BioNLP Shared Task 2011 Workshop</span>, pages 74–82, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
Riza Theresa Batista-Navarro and Sophia Ananiadou.

</span>
<span class="ltx_bibblock">Building a coreference-annotated corpus from the domain of biochemistry.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib153.1.1">Proceedings of BioNLP 2011 workshop</span>, pages 83–91, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
K Bretonnel Cohen, Arrick Lanfranchi, Miji Joo-young Choi, Michael Bada, William A Baumgartner, Natalya Panteleyeva, Karin Verspoor, Martha Palmer, and Lawrence E Hunter.

</span>
<span class="ltx_bibblock">Coreference annotation and resolution in the colorado richly annotated full text (craft) corpus of biomedical journal articles.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib154.1.1">BMC bioinformatics</span>, 18:1–14, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.

</span>
<span class="ltx_bibblock">Biobert: a pre-trained biomedical language representation model for biomedical text mining.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib155.1.1">Bioinformatics</span>, 36(4):1234–1240, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.

</span>
<span class="ltx_bibblock">Spanbert: Improving pre-training by representing and predicting spans.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib156.1.1">Transactions of the association for computational linguistics</span>, 8:64–77, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
Simon Baker, Ilona Silins, Yufan Guo, Imran Ali, Johan Högberg, Ulla Stenius, and Anna Korhonen.

</span>
<span class="ltx_bibblock">Automatic semantic classification of scientific literature according to the hallmarks of cancer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib157.1.1">Bioinformatics</span>, 32(3):432–440, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training.

</span>
<span class="ltx_bibblock">2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
Yanrong Ji, Zhihan Zhou, Han Liu, and Ramana V Davuluri.

</span>
<span class="ltx_bibblock">Dnabert: pre-trained bidirectional encoder representations from transformers model for dna-language in genome.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib159.1.1">Bioinformatics</span>, 37(15):2112–2120, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
Melissa Sanabria, Jonas Hirsch, and Anna R Poetsch.

</span>
<span class="ltx_bibblock">The human genome’s vocabulary as proposed by the dna language model grover.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib160.1.1">bioRxiv</span>, pages 2023–07, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
Ken Chen, Yue Zhou, Maolin Ding, Yu Wang, Zhixiang Ren, and Yuedong Yang.

</span>
<span class="ltx_bibblock">Self-supervised learning on millions of pre-mrna sequences improves sequence-based rna splicing prediction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib161.1.1">bioRxiv</span>, pages 2023–01, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
Jiayang Chen, Zhihang Hu, Siqi Sun, Qingxiong Tan, Yixuan Wang, Qinze Yu, Licheng Zong, Liang Hong, Jin Xiao, Tao Shen, et al.

</span>
<span class="ltx_bibblock">Interpretable rna foundation model from unannotated data for highly accurate rna structure and function predictions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib162.1.1">arXiv preprint arXiv:2204.00300</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rihawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, et al.

</span>
<span class="ltx_bibblock">Prottrans: Towards cracking the language of life’s code through self-supervised deep learning and high performance computing. arxiv 2020.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib163.1.1">arXiv preprint arXiv:2007.06225</span>, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
Noelia Ferruz, Steffen Schmidt, and Birte Höcker.

</span>
<span class="ltx_bibblock">Protgpt2 is a deep unsupervised language model for protein design.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib164.1.1">Nature communications</span>, 13(1):4348, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock">
Patrick Bryant, Gabriele Pozzati, and Arne Elofsson.

</span>
<span class="ltx_bibblock">Improved prediction of protein-protein interactions using alphafold2.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib165.1.1">Nature communications</span>, 13(1):1265, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock">
Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J Ballard, Joshua Bambrick, et al.

</span>
<span class="ltx_bibblock">Accurate structure prediction of biomolecular interactions with alphafold 3.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib166.1.1">Nature</span>, pages 1–3, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock">
Hugo Dalla-Torre, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, Bernardo P de Almeida, Hassan Sirelkhatim, et al.

</span>
<span class="ltx_bibblock">Nucleotide transformer: building and evaluating robust foundation models for human genomics.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib167.1.1">Nature Methods</span>, pages 1–11, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock">
Zhihan Zhou, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu.

</span>
<span class="ltx_bibblock">Dnabert-2: Efficient foundation model and benchmark for multi-species genome.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib168.1.1">arXiv preprint arXiv:2306.15006</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock">
Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Michael Wornow, Callum Birch-Sykes, Stefano Massaroli, Aman Patel, Clayton Rabideau, Yoshua Bengio, et al.

</span>
<span class="ltx_bibblock">Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib169.1.1">Advances in neural information processing systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock">
Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré.

</span>
<span class="ltx_bibblock">Hyena hierarchy: Towards larger convolutional language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib170.1.1">International Conference on Machine Learning</span>, pages 28043–28078. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock">
Daoan Zhang, Weitong Zhang, Bing He, Jianguo Zhang, Chenchen Qin, and Jianhua Yao.

</span>
<span class="ltx_bibblock">Dnagpt: a generalized pretrained tool for multiple dna sequence analysis tasks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib171.1.1">bioRxiv</span>, pages 2023–07, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock">
Wenhuan Zeng, Anupam Gautam, and Daniel H Huson.

</span>
<span class="ltx_bibblock">Mulan-methyl—multiple transformer-based language models for accurate dna methylation prediction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib172.1.1">GigaScience</span>, 12:giad054, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock">
Ofir Press, Noah A Smith, and Mike Lewis.

</span>
<span class="ltx_bibblock">Train short, test long: Attention with linear biases enables input length extrapolation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib173.1.1">arXiv preprint arXiv:2108.12409</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock">
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.

</span>
<span class="ltx_bibblock">Flashattention: Fast and memory-efficient exact attention with io-awareness.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib174.1.1">Advances in Neural Information Processing Systems</span>, 35:16344–16359, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock">
Zhou Zhihan, Wu Weimin, Ho Harrison, Wang Jiayi, Shi Lizhen, V Davuluri Ramana, Wang Zhong, and Liu Han.

</span>
<span class="ltx_bibblock">Dnabert-s: Pioneering species differentiation with species-aware dna embeddings.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib175.1.1">arXiv</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock">
René Dreos, Giovanna Ambrosini, Rouayda Cavin Périer, and Philipp Bucher.

</span>
<span class="ltx_bibblock">Epd and epdnew, high-quality promoter resources in the next-generation sequencing era.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib176.1.1">Nucleic acids research</span>, 41(D1):D157–D164, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock">
ENCODE Project Consortium et al.

</span>
<span class="ltx_bibblock">An integrated encyclopedia of dna elements in the human genome.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib177.1.1">Nature</span>, 489(7414):57, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock">
Jill E Moore, Michael J Purcaro, Henry E Pratt, Charles B Epstein, Noam Shoresh, Jessika Adrian, Trupti Kawli, Carrie A Davis, Alexander Dobin, et al.

</span>
<span class="ltx_bibblock">Expanded encyclopaedias of dna elements in the human and mouse genomes.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib178.1.1">Nature</span>, 583(7818):699–710, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock">
Jennifer Harrow, Adam Frankish, Jose M Gonzalez, Electra Tapanari, Mark Diekhans, Felix Kokocinski, Bronwen L Aken, Daniel Barrell, Amonida Zadissa, Stephen Searle, et al.

</span>
<span class="ltx_bibblock">Gencode: the reference human genome annotation for the encode project.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib179.1.1">Genome research</span>, 22(9):1760–1774, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock">
Colin Hall Kalicki and Esin Darici Haritaoglu.

</span>
<span class="ltx_bibblock">Rnabert: Rna family classification and secondary structure prediction with bert pretrained on rna sequences.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock">
Ken Chen, Yue Zhou, Maolin Ding, Yu Wang, Zhixiang Ren, and Yuedong Yang.

</span>
<span class="ltx_bibblock">Self-supervised learning on millions of primary rna sequences from 72 vertebrates improves sequence-based rna splicing prediction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib181.1.1">Briefings in Bioinformatics</span>, 25(3):bbae163, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock">
Yikun Zhang, Mei Lang, Jiuhong Jiang, Zhiqiang Gao, Fan Xu, Thomas Litfin, Ke Chen, Jaswinder Singh, Xiansong Huang, Guoli Song, et al.

</span>
<span class="ltx_bibblock">Multiple sequence alignment-based rna language model and its application to structural inference.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib182.1.1">Nucleic Acids Research</span>, 52(1):e3–e3, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock">
Keisuke Yamada and Michiaki Hamada.

</span>
<span class="ltx_bibblock">Prediction of rna–protein interactions using a nucleotide language model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib183.1.1">Bioinformatics Advances</span>, 2(1):vbac023, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock">
Erik S Wright.

</span>
<span class="ltx_bibblock">Rnacontest: comparing tools for noncoding rna multiple sequence alignment based on structural consistency.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib184.1.1">Rna</span>, 26(5):531–540, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock">
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al.

</span>
<span class="ltx_bibblock">Highly accurate protein structure prediction with alphafold.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib185.1.1">nature</span>, 596(7873):583–589, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_tag_bibitem">[186]</span>
<span class="ltx_bibblock">
Rnacentral 2021: secondary structure integration, improved sequence search and new member databases.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib186.1.1">Nucleic acids research</span>, 49(D1):D212–D220, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_tag_bibitem">[187]</span>
<span class="ltx_bibblock">
Maximilian Haeussler, Ann S Zweig, Cath Tyner, Matthew L Speir, Kate R Rosenbloom, Brian J Raney, Christopher M Lee, Brian T Lee, Angie S Hinrichs, Jairo Navarro Gonzalez, et al.

</span>
<span class="ltx_bibblock">The ucsc genome browser database: 2019 update.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib187.1.1">Nucleic acids research</span>, 47(D1):D853–D858, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_tag_bibitem">[188]</span>
<span class="ltx_bibblock">
Xiaoyong Pan, Yi Fang, Xianfeng Li, Yang Yang, and Hong-Bin Shen.

</span>
<span class="ltx_bibblock">Rbpsuite: Rna-protein binding sites prediction suite based on deep learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib188.1.1">BMC genomics</span>, 21:1–8, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_tag_bibitem">[189]</span>
<span class="ltx_bibblock">
Qing Zhang, Xiaodan Fan, Yejun Wang, Ming-an Sun, Jianlin Shao, and Dianjing Guo.

</span>
<span class="ltx_bibblock">Bpp: a sequence-based algorithm for branch point prediction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib189.1.1">Bioinformatics</span>, 33(20):3166–3172, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_tag_bibitem">[190]</span>
<span class="ltx_bibblock">
Nicolas Scalzitti, Arnaud Kress, Romain Orhand, Thomas Weber, Luc Moulinier, Anne Jeannin-Girardon, Pierre Collet, Olivier Poch, and Julie D Thompson.

</span>
<span class="ltx_bibblock">Spliceator: multi-species splice site prediction using convolutional neural networks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib190.1.1">BMC bioinformatics</span>, 22:1–26, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_tag_bibitem">[191]</span>
<span class="ltx_bibblock">
Jaswinder Singh, Jack Hanson, Kuldip Paliwal, and Yaoqi Zhou.

</span>
<span class="ltx_bibblock">Rna secondary structure prediction using an ensemble of two-dimensional deep neural networks and transfer learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib191.1.1">Nature communications</span>, 10(1):5407, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_tag_bibitem">[192]</span>
<span class="ltx_bibblock">
Amos Tanay and Aviv Regev.

</span>
<span class="ltx_bibblock">Scaling single-cell genomics from phenomenology to mechanism.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib192.1.1">Nature</span>, 541(7637):331–338, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_tag_bibitem">[193]</span>
<span class="ltx_bibblock">
Daniel Levine, Syed Asad Rizvi, Sacha Lévy, Nazreen Pallikkavaliyaveetil, David Zhang, Xingyu Chen, Sina Ghadermarzi, Ruiming Wu, Zihe Zheng, Ivan Vrkic, et al.

</span>
<span class="ltx_bibblock">Cell2sentence: teaching large language models the language of biology.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib193.1.1">BioRxiv</span>, pages 2023–09, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_tag_bibitem">[194]</span>
<span class="ltx_bibblock">
Hongru Shen, Jilei Liu, Jiani Hu, Xilin Shen, Chao Zhang, Dan Wu, Mengyao Feng, Meng Yang, Yang Li, Yichen Yang, et al.

</span>
<span class="ltx_bibblock">Generative pretraining from large-scale transcriptomes for single-cell deciphering.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib194.1.1">Iscience</span>, 26(5), 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_tag_bibitem">[195]</span>
<span class="ltx_bibblock">
Christina V Theodoris, Ling Xiao, Anant Chopra, Mark D Chaffin, Zeina R Al Sayed, Matthew C Hill, Helene Mantineo, Elizabeth M Brydon, Zexian Zeng, X Shirley Liu, et al.

</span>
<span class="ltx_bibblock">Transfer learning enables predictions in network biology.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib195.1.1">Nature</span>, 618(7965):616–624, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib196">
<span class="ltx_tag ltx_tag_bibitem">[196]</span>
<span class="ltx_bibblock">
Haotian Cui, Chloe Wang, Hassaan Maan, Kuan Pang, Fengning Luo, Nan Duan, and Bo Wang.

</span>
<span class="ltx_bibblock">scgpt: toward building a foundation model for single-cell multi-omics using generative ai.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib196.1.1">Nature Methods</span>, pages 1–11, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib197">
<span class="ltx_tag ltx_tag_bibitem">[197]</span>
<span class="ltx_bibblock">
Fan Yang, Wenchuan Wang, Fang Wang, Yuan Fang, Duyu Tang, Junzhou Huang, Hui Lu, and Jianhua Yao.

</span>
<span class="ltx_bibblock">scbert as a large-scale pretrained deep language model for cell type annotation of single-cell rna-seq data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib197.1.1">Nature Machine Intelligence</span>, 4(10):852–866, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib198">
<span class="ltx_tag ltx_tag_bibitem">[198]</span>
<span class="ltx_bibblock">
Jingcheng Du, Peilin Jia, Yulin Dai, Cui Tao, Zhongming Zhao, and Degui Zhi.

</span>
<span class="ltx_bibblock">Gene2vec: distributed representation of genes based on co-expression.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib198.1.1">BMC genomics</span>, 20:7–15, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib199">
<span class="ltx_tag ltx_tag_bibitem">[199]</span>
<span class="ltx_bibblock">
Jing Xu, Aidi Zhang, Fang Liu, Liang Chen, and Xiujun Zhang.

</span>
<span class="ltx_bibblock">Ciform as a transformer-based model for cell-type annotation of large-scale single-cell rna-seq data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib199.1.1">Briefings in Bioinformatics</span>, 24(4):bbad195, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib200">
<span class="ltx_tag ltx_tag_bibitem">[200]</span>
<span class="ltx_bibblock">
Jiawei Chen, Hao Xu, Wanyu Tao, Zhaoxiong Chen, Yuxuan Zhao, and Jing-Dong J Han.

</span>
<span class="ltx_bibblock">Transformer for one stop interpretable cell type annotation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib200.1.1">Nature Communications</span>, 14(1):223, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib201">
<span class="ltx_tag ltx_tag_bibitem">[201]</span>
<span class="ltx_bibblock">
Linfang Jiao, Gan Wang, Huanhuan Dai, Xue Li, Shuang Wang, and Tao Song.

</span>
<span class="ltx_bibblock">sctranssort: Transformers for intelligent annotation of cell types by gene embeddings.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib201.1.1">Biomolecules</span>, 13(4):611, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib202">
<span class="ltx_tag ltx_tag_bibitem">[202]</span>
<span class="ltx_bibblock">
Tao Song, Huanhuan Dai, Shuang Wang, Gan Wang, Xudong Zhang, Ying Zhang, and Linfang Jiao.

</span>
<span class="ltx_bibblock">Transcluster: A cell-type identification method for single-cell rna-seq data using deep learning based on transformer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib202.1.1">Frontiers in Genetics</span>, 13:1038919, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib203">
<span class="ltx_tag ltx_tag_bibitem">[203]</span>
<span class="ltx_bibblock">
Sebastian Preissl, Kyle J Gaulton, and Bing Ren.

</span>
<span class="ltx_bibblock">Characterizing cis-regulatory elements using single-cell epigenomics.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib203.1.1">Nature Reviews Genetics</span>, 24(1):21–43, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib204">
<span class="ltx_tag ltx_tag_bibitem">[204]</span>
<span class="ltx_bibblock">
Žiga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley.

</span>
<span class="ltx_bibblock">Effective gene expression prediction from sequence by integrating long-range interactions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib204.1.1">Nature methods</span>, 18(10):1196–1203, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib205">
<span class="ltx_tag ltx_tag_bibitem">[205]</span>
<span class="ltx_bibblock">
Zijing Gao, Qiao Liu, Wanwen Zeng, Rui Jiang, and Wing Hung Wong.

</span>
<span class="ltx_bibblock">Epigept: a pretrained transformer-based language model for context-specific human epigenomics.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib205.1.1">Genome Biology</span>, 25(1):1–30, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib206">
<span class="ltx_tag ltx_tag_bibitem">[206]</span>
<span class="ltx_bibblock">
Maxwell R Mumbach, Adam J Rubin, Ryan A Flynn, Chao Dai, Paul A Khavari, William J Greenleaf, and Howard Y Chang.

</span>
<span class="ltx_bibblock">Hichip: efficient and sensitive analysis of protein-directed genome architecture.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib206.1.1">Nature methods</span>, 13(11):919–922, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib207">
<span class="ltx_tag ltx_tag_bibitem">[207]</span>
<span class="ltx_bibblock">
Jon-Matthew Belton, Rachel Patton McCord, Johan Harmen Gibcus, Natalia Naumova, Ye Zhan, and Job Dekker.

</span>
<span class="ltx_bibblock">Hi–c: a comprehensive technique to capture the conformation of genomes.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib207.1.1">Methods</span>, 58(3):268–276, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib208">
<span class="ltx_tag ltx_tag_bibitem">[208]</span>
<span class="ltx_bibblock">
Ruedi Aebersold and Matthias Mann.

</span>
<span class="ltx_bibblock">Mass spectrometry-based proteomics.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib208.1.1">Nature</span>, 422(6928):198–207, 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib209">
<span class="ltx_tag ltx_tag_bibitem">[209]</span>
<span class="ltx_bibblock">
Steven R Shuken.

</span>
<span class="ltx_bibblock">An introduction to mass spectrometry-based proteomics.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib209.1.1">Journal of Proteome Research</span>, 22(7):2151–2171, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib210">
<span class="ltx_tag ltx_tag_bibitem">[210]</span>
<span class="ltx_bibblock">
Fang Wang, Chunpu Liu, Jiawei Li, Fan Yang, Jiangning Song, Tianyi Zang, Jianhua Yao, and Guohua Wang.

</span>
<span class="ltx_bibblock">Spdb: a comprehensive resource and knowledgebase for proteomic data at the single-cell resolution.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib210.1.1">Nucleic Acids Research</span>, 52(D1):D562–D571, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib211">
<span class="ltx_tag ltx_tag_bibitem">[211]</span>
<span class="ltx_bibblock">
Ning Ding, Shang Qu, Linhai Xie, Yifei Li, Zaoqu Liu, Kaiyan Zhang, Yibai Xiong, Yuxin Zuo, Zhangren Chen, Ermo Hua, et al.

</span>
<span class="ltx_bibblock">Automating exploratory proteomics research via language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib211.1.1">arXiv preprint arXiv:2411.03743</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib212">
<span class="ltx_tag ltx_tag_bibitem">[212]</span>
<span class="ltx_bibblock">
Qiang Zhang, Keyang Ding, Tianwen Lyv, Xinda Wang, Qingyu Yin, Yiwen Zhang, Jing Yu, Yuhao Wang, Xiaotong Li, Zhuoyi Xiang, et al.

</span>
<span class="ltx_bibblock">Scientific large language models: A survey on biological &amp; chemical domains.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib212.1.1">arXiv preprint arXiv:2401.14656</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib213">
<span class="ltx_tag ltx_tag_bibitem">[213]</span>
<span class="ltx_bibblock">
Hanguang Xiao, Feizhong Zhou, Xingyue Liu, Tianqi Liu, Zhipeng Li, Xin Liu, and Xiaoxuan Huang.

</span>
<span class="ltx_bibblock">A comprehensive survey of large language models and multimodal large language models in medicine.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib213.1.1">arXiv preprint arXiv:2405.08603</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib214">
<span class="ltx_tag ltx_tag_bibitem">[214]</span>
<span class="ltx_bibblock">
Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma, et al.

</span>
<span class="ltx_bibblock">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib214.1.1">Proceedings of the National Academy of Sciences</span>, 118(15):e2016239118, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib215">
<span class="ltx_tag ltx_tag_bibitem">[215]</span>
<span class="ltx_bibblock">
Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alex Rives.

</span>
<span class="ltx_bibblock">Language models enable zero-shot prediction of the effects of mutations on protein function.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib215.1.1">Advances in neural information processing systems</span>, 34:29287–29303, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib216">
<span class="ltx_tag ltx_tag_bibitem">[216]</span>
<span class="ltx_bibblock">
Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, and Michal Linial.

</span>
<span class="ltx_bibblock">Proteinbert: a universal deep-learning model of protein sequence and function.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib216.1.1">Bioinformatics</span>, 38(8):2102–2110, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib217">
<span class="ltx_tag ltx_tag_bibitem">[217]</span>
<span class="ltx_bibblock">
Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, et al.

</span>
<span class="ltx_bibblock">Prottrans: Toward understanding the language of life through self-supervised learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib217.1.1">IEEE transactions on pattern analysis and machine intelligence</span>, 44(10):7112–7127, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib218">
<span class="ltx_tag ltx_tag_bibitem">[218]</span>
<span class="ltx_bibblock">
Ali Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R Eguchi, Po-Ssu Huang, and Richard Socher.

</span>
<span class="ltx_bibblock">Progen: Language modeling for protein generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib218.1.1">arXiv preprint arXiv:2004.03497</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib219">
<span class="ltx_tag ltx_tag_bibitem">[219]</span>
<span class="ltx_bibblock">
Noelia Ferruz, Steffen Schmidt, and Birte Höcker.

</span>
<span class="ltx_bibblock">A deep unsupervised language model for protein design.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib219.1.1">BioRxiv</span>, pages 2022–03, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib220">
<span class="ltx_tag ltx_tag_bibitem">[220]</span>
<span class="ltx_bibblock">
Geraldene Munsamy, Sebastian Lindner, Philipp Lorenz, and Noelia Ferruz.

</span>
<span class="ltx_bibblock">Zymctrl: a conditional language model for the controllable generation of artificial enzymes.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib220.1.1">NeurIPS Machine Learning in Structural Biology Workshop</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib221">
<span class="ltx_tag ltx_tag_bibitem">[221]</span>
<span class="ltx_bibblock">
Daniel Hesslow, Niccoló Zanichelli, Pascal Notin, Iacopo Poli, and Debora Marks.

</span>
<span class="ltx_bibblock">Rita: a study on scaling up generative protein sequence models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib221.1.1">arXiv preprint arXiv:2205.05789</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib222">
<span class="ltx_tag ltx_tag_bibitem">[222]</span>
<span class="ltx_bibblock">
Richard W Shuai, Jeffrey A Ruffolo, and Jeffrey J Gray.

</span>
<span class="ltx_bibblock">Generative language modeling for antibody design.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib222.1.1">BioRxiv</span>, pages 2021–12, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib223">
<span class="ltx_tag ltx_tag_bibitem">[223]</span>
<span class="ltx_bibblock">
Matt Sternke and Joel Karpiak.

</span>
<span class="ltx_bibblock">Proteinrl: Reinforcement learning with generative protein language models for property-directed sequence design.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib223.1.1">NeurIPS 2023 Generative AI and Biology (GenBio) Workshop</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib224">
<span class="ltx_tag ltx_tag_bibitem">[224]</span>
<span class="ltx_bibblock">
Timothy Truong Jr and Tristan Bepler.

</span>
<span class="ltx_bibblock">Poet: A generative model of protein families as sequences-of-sequences.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib224.1.1">Advances in Neural Information Processing Systems</span>, 36:77379–77415, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib225">
<span class="ltx_tag ltx_tag_bibitem">[225]</span>
<span class="ltx_bibblock">
Yue Cao, Payel Das, Vijil Chenthamarakshan, Pin-Yu Chen, Igor Melnyk, and Yang Shen.

</span>
<span class="ltx_bibblock">Fold2seq: A joint sequence (1d)-fold (3d) embedding-based generative model for protein design.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib225.1.1">International Conference on Machine Learning</span>, pages 1261–1271. PMLR, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib226">
<span class="ltx_tag ltx_tag_bibitem">[226]</span>
<span class="ltx_bibblock">
Soumya Ram and Tristan Bepler.

</span>
<span class="ltx_bibblock">Few shot protein generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib226.1.1">arXiv preprint arXiv:2204.01168</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib227">
<span class="ltx_tag ltx_tag_bibitem">[227]</span>
<span class="ltx_bibblock">
Damiano Sgarbossa, Umberto Lupo, and Anne-Florence Bitbol.

</span>
<span class="ltx_bibblock">Generative power of a protein language model trained on multiple sequence alignments.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib227.1.1">Elife</span>, 12:e79854, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib228">
<span class="ltx_tag ltx_tag_bibitem">[228]</span>
<span class="ltx_bibblock">
Minji Lee, Luiz Felipe Vecchietti, Hyunkyu Jung, Hyunjoo Ro, Meeyoung Cha, and Ho Min Kim.

</span>
<span class="ltx_bibblock">Protein sequence design in a latent space via model-based reinforcement learning.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib229">
<span class="ltx_tag ltx_tag_bibitem">[229]</span>
<span class="ltx_bibblock">
Zaixiang Zheng, Yifan Deng, Dongyu Xue, Yi Zhou, Fei Ye, and Quanquan Gu.

</span>
<span class="ltx_bibblock">Structure-informed language models are protein designers. biorxiv.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib230">
<span class="ltx_tag ltx_tag_bibitem">[230]</span>
<span class="ltx_bibblock">
Le Zhang, Jiayang Chen, Tao Shen, Yu Li, and Siqi Sun.

</span>
<span class="ltx_bibblock">Enhancing the protein tertiary structure prediction by multiple sequence alignment generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib230.1.1">arXiv preprint arXiv:2306.01824</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib231">
<span class="ltx_tag ltx_tag_bibitem">[231]</span>
<span class="ltx_bibblock">
Michael Heinzinger, Konstantin Weissenow, Joaquin Gomez Sanchez, Adrian Henkel, Martin Steinegger, and Burkhard Rost.

</span>
<span class="ltx_bibblock">Prostt5: Bilingual language model for protein sequence and structure. biorxiv.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib232">
<span class="ltx_tag ltx_tag_bibitem">[232]</span>
<span class="ltx_bibblock">
Bo Chen, Xingyi Cheng, Pan Li, Yangli-ao Geng, Jing Gong, Shen Li, Zhilei Bei, Xu Tan, Boyan Wang, Xin Zeng, et al.

</span>
<span class="ltx_bibblock">xtrimopglm: unified 100b-scale pre-trained transformer for deciphering the language of protein.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib232.1.1">arXiv preprint arXiv:2401.06199</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib233">
<span class="ltx_tag ltx_tag_bibitem">[233]</span>
<span class="ltx_bibblock">
Yaiza Serrano, Sergi Roda, Victor Guallar, and Alexis Molina.

</span>
<span class="ltx_bibblock">Efficient and accurate sequence generation with small-scale protein language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib233.1.1">bioRxiv</span>, pages 2023–08, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib234">
<span class="ltx_tag ltx_tag_bibitem">[234]</span>
<span class="ltx_bibblock">
Simon KS Chu and Kathy Y Wei.

</span>
<span class="ltx_bibblock">Generative antibody design for complementary chain pairing sequences through encoder-decoder language model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib234.1.1">arXiv preprint arXiv:2301.02748</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib235">
<span class="ltx_tag ltx_tag_bibitem">[235]</span>
<span class="ltx_bibblock">
Youhan Lee, Hasun Yu, Jaemyung Lee, and Jaehoon Kim.

</span>
<span class="ltx_bibblock">Pre-training sequence, structure, and surface features for comprehensive protein representation learning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib235.1.1">The Twelfth International Conference on Learning Representations</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib236">
<span class="ltx_tag ltx_tag_bibitem">[236]</span>
<span class="ltx_bibblock">
Viet Thanh Duy Nguyen and Truong Son Hy.

</span>
<span class="ltx_bibblock">Multimodal pretraining for unsupervised protein representation learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib236.1.1">Biology Methods and Protocols</span>, page bpae043, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib237">
<span class="ltx_tag ltx_tag_bibitem">[237]</span>
<span class="ltx_bibblock">
Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, et al.

</span>
<span class="ltx_bibblock">Language models of protein sequences at the scale of evolution enable accurate structure prediction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib237.1.1">BioRxiv</span>, 2022:500902, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib238">
<span class="ltx_tag ltx_tag_bibitem">[238]</span>
<span class="ltx_bibblock">
Jesse Durham, Jing Zhang, Ian R Humphreys, Jimin Pei, and Qian Cong.

</span>
<span class="ltx_bibblock">Recent advances in predicting and modeling protein–protein interactions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib238.1.1">Trends in biochemical sciences</span>, 48(6):527–538, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib239">
<span class="ltx_tag ltx_tag_bibitem">[239]</span>
<span class="ltx_bibblock">
Mohammed AlQuraishi.

</span>
<span class="ltx_bibblock">Alphafold at casp13.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib239.1.1">Bioinformatics</span>, 35(22):4862–4865, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib240">
<span class="ltx_tag ltx_tag_bibitem">[240]</span>
<span class="ltx_bibblock">
Vinayak Agarwal and Andrew C McShan.

</span>
<span class="ltx_bibblock">The power and pitfalls of alphafold2 for structure prediction beyond rigid globular proteins.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib240.1.1">Nature Chemical Biology</span>, 20(8):950–959, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib241">
<span class="ltx_tag ltx_tag_bibitem">[241]</span>
<span class="ltx_bibblock">
Kanchan Jha, Sourav Karmakar, and Sriparna Saha.

</span>
<span class="ltx_bibblock">Graph-bert and language model-based framework for protein–protein interaction identification.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib241.1.1">Scientific Reports</span>, 13(1):5663, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib242">
<span class="ltx_tag ltx_tag_bibitem">[242]</span>
<span class="ltx_bibblock">
Xue Li, Peifu Han, Wenqi Chen, Changnan Gao, Shuang Wang, Tao Song, Muyuan Niu, and Alfonso Rodriguez-Patón.

</span>
<span class="ltx_bibblock">Marppi: boosting prediction of protein–protein interactions with multi-scale architecture residual network.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib242.1.1">Briefings in Bioinformatics</span>, 24(1):bbac524, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib243">
<span class="ltx_tag ltx_tag_bibitem">[243]</span>
<span class="ltx_bibblock">
Ji Min Lee, Henrik M Hammarén, Mikhail M Savitski, and Sung Hee Baek.

</span>
<span class="ltx_bibblock">Control of protein stability by post-translational modifications.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib243.1.1">Nature Communications</span>, 14(1):201, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib244">
<span class="ltx_tag ltx_tag_bibitem">[244]</span>
<span class="ltx_bibblock">
Palistha Shrestha, Jeevan Kandel, Hilal Tayara, and Kil To Chong.

</span>
<span class="ltx_bibblock">Post-translational modification prediction via prompt-based fine-tuning of a gpt-2 model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib244.1.1">Nature Communications</span>, 15(1):6699, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib245">
<span class="ltx_tag ltx_tag_bibitem">[245]</span>
<span class="ltx_bibblock">
Farzaneh Esmaili, Mahdi Pourmirzaei, Shahin Ramazi, Seyedehsamaneh Shojaeilangari, and Elham Yavari.

</span>
<span class="ltx_bibblock">A review of machine learning and algorithmic methods for protein phosphorylation site prediction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib245.1.1">Genomics, Proteomics &amp; Bioinformatics</span>, 21(6):1266–1285, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib246">
<span class="ltx_tag ltx_tag_bibitem">[246]</span>
<span class="ltx_bibblock">
Letícia MF Bertoline, Angélica N Lima, Jose E Krieger, and Samantha K Teixeira.

</span>
<span class="ltx_bibblock">Before and after alphafold2: An overview of protein structure prediction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib246.1.1">Frontiers in bioinformatics</span>, 3:1120370, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib247">
<span class="ltx_tag ltx_tag_bibitem">[247]</span>
<span class="ltx_bibblock">
Gyuri Kim, Sewon Lee, Eli Levy Karin, Hyunbin Kim, Yoshitaka Moriwaki, Sergey Ovchinnikov, Martin Steinegger, and Milot Mirdita.

</span>
<span class="ltx_bibblock">Easy and accurate protein structure prediction using colabfold.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib247.1.1">Nature Protocols</span>, pages 1–23, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib248">
<span class="ltx_tag ltx_tag_bibitem">[248]</span>
<span class="ltx_bibblock">
Bowen Jing, Ezra Erives, Peter Pao-Huang, Gabriele Corso, Bonnie Berger, and Tommi Jaakkola.

</span>
<span class="ltx_bibblock">Eigenfold: Generative protein structure prediction with diffusion models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib248.1.1">arXiv preprint arXiv:2304.02198</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib249">
<span class="ltx_tag ltx_tag_bibitem">[249]</span>
<span class="ltx_bibblock">
Uniprot: the universal protein knowledgebase in 2023.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib249.1.1">Nucleic acids research</span>, 51(D1):D523–D531, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib250">
<span class="ltx_tag ltx_tag_bibitem">[250]</span>
<span class="ltx_bibblock">
Thu Nguyen, Willy Wriggers, and Jing He.

</span>
<span class="ltx_bibblock">A data set of paired structural segments between protein data bank and alphafold db for medium-resolution cryo-em density maps: A gap in overall structural quality.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib250.1.1">International Symposium on Bioinformatics Research and Applications</span>, pages 52–63. Springer, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib251">
<span class="ltx_tag ltx_tag_bibitem">[251]</span>
<span class="ltx_bibblock">
Gi Hyun Lee, Cheol Woo Min, Jeong Woo Jang, Ravi Gupta, and Sun Tae Kim.

</span>
<span class="ltx_bibblock">Dataset on post-translational modifications proteome analysis of msp1-overexpressing rice leaf proteins.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib251.1.1">Data in brief</span>, 50:109573, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib252">
<span class="ltx_tag ltx_tag_bibitem">[252]</span>
<span class="ltx_bibblock">
Polina Lakrisenko and Daniel Weindl.

</span>
<span class="ltx_bibblock">Dynamic models for metabolomics data integration.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib252.1.1">Current Opinion in Systems Biology</span>, 28:100358, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib253">
<span class="ltx_tag ltx_tag_bibitem">[253]</span>
<span class="ltx_bibblock">
Leqi Tian and Tianwei Yu.

</span>
<span class="ltx_bibblock">An integrated deep learning framework for the interpretation of untargeted metabolomics data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib253.1.1">Briefings in Bioinformatics</span>, 24(4):bbad244, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib254">
<span class="ltx_tag ltx_tag_bibitem">[254]</span>
<span class="ltx_bibblock">
Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy.

</span>
<span class="ltx_bibblock">Challenges and applications of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib254.1.1">arXiv preprint arXiv:2307.10169</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib255">
<span class="ltx_tag ltx_tag_bibitem">[255]</span>
<span class="ltx_bibblock">
Thao Vu, Parker Siemek, Fatema Bhinderwala, Yuhang Xu, and Robert Powers.

</span>
<span class="ltx_bibblock">Evaluation of multivariate classification models for analyzing nmr metabolomics data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib255.1.1">Journal of proteome research</span>, 18(9):3282–3294, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib256">
<span class="ltx_tag ltx_tag_bibitem">[256]</span>
<span class="ltx_bibblock">
Chengsheng Mao, Jie Xu, Luke Rasmussen, Yikuan Li, Prakash Adekkanattu, Jennifer Pacheco, Borna Bonakdarpour, Robert Vassar, Li Shen, Guoqian Jiang, et al.

</span>
<span class="ltx_bibblock">Ad-bert: Using pre-trained language model to predict the progression from mild cognitive impairment to alzheimer’s disease.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib256.1.1">Journal of Biomedical Informatics</span>, 144:104442, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib257">
<span class="ltx_tag ltx_tag_bibitem">[257]</span>
<span class="ltx_bibblock">
Yingjie Feng, Xiaoyin Xu, Yueting Zhuang, and Min Zhang.

</span>
<span class="ltx_bibblock">Large language models improve alzheimer’s disease diagnosis using multi-modality data.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib257.1.1">2023 IEEE International Conference on Medical Artificial Intelligence (MedAI)</span>, pages 61–66. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib258">
<span class="ltx_tag ltx_tag_bibitem">[258]</span>
<span class="ltx_bibblock">
Kevin Xie, Ryan S Gallagher, Erin C Conrad, Chadric O Garrick, Steven N Baldassano, John M Bernabei, Peter D Galer, Nina J Ghosn, Adam S Greenblatt, Tara Jennings, et al.

</span>
<span class="ltx_bibblock">Extracting seizure frequency from epilepsy clinic notes: a machine reading approach to natural language processing.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib258.1.1">Journal of the American Medical Informatics Association</span>, 29(5):873–881, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib259">
<span class="ltx_tag ltx_tag_bibitem">[259]</span>
<span class="ltx_bibblock">
Shunsuke Koga, Nicholas B Martin, and Dennis W Dickson.

</span>
<span class="ltx_bibblock">Evaluating the performance of large language models: Chatgpt and google bard in generating differential diagnoses in clinicopathological conferences of neurodegenerative disorders.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib259.1.1">Brain Pathology</span>, 34(3):e13207, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib260">
<span class="ltx_tag ltx_tag_bibitem">[260]</span>
<span class="ltx_bibblock">
Bastien Le Guellec, Alexandre Lefèvre, Charlotte Geay, Lucas Shorten, Cyril Bruge, Lotfi Hacein-Bey, Philippe Amouyel, Jean-Pierre Pruvo, Grégory Kuchcinski, and Aghiles Hamroun.

</span>
<span class="ltx_bibblock">Performance of an open-source large language model in extracting information from free-text radiology reports.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib260.1.1">Radiology: Artificial Intelligence</span>, page e230364, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib261">
<span class="ltx_tag ltx_tag_bibitem">[261]</span>
<span class="ltx_bibblock">
Jun Kanzawa, Koichiro Yasaka, Nana Fujita, Shin Fujiwara, and Osamu Abe.

</span>
<span class="ltx_bibblock">Automated classification of brain mri reports using fine-tuned large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib261.1.1">Neuroradiology</span>, pages 1–7, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib262">
<span class="ltx_tag ltx_tag_bibitem">[262]</span>
<span class="ltx_bibblock">
Akshay Valsaraj, Ithihas Madala, Nikhil Garg, and Veeky Baths.

</span>
<span class="ltx_bibblock">Alzheimer’s dementia detection using acoustic &amp; linguistic features and pre-trained bert.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib262.1.1">2021 8th International Conference on Soft Computing &amp; Machine Intelligence (ISCMI)</span>, pages 171–175. IEEE, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib263">
<span class="ltx_tag ltx_tag_bibitem">[263]</span>
<span class="ltx_bibblock">
Nayan Anand Vats, Aditya Yadavalli, Krishna Gurugubelli, and Anil Kumar Vuppala.

</span>
<span class="ltx_bibblock">Acoustic features, bert model and their complementary nature for alzheimer’s dementia detection.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib263.1.1">Proceedings of the 2021 Thirteenth International Conference on Contemporary Computing</span>, pages 267–272, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib264">
<span class="ltx_tag ltx_tag_bibitem">[264]</span>
<span class="ltx_bibblock">
Jeong-Uk Bang, Seung-Hoon Han, and Byung-Ok Kang.

</span>
<span class="ltx_bibblock">Alzheimer’s disease recognition from spontaneous speech using large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib264.1.1">ETRI Journal</span>, 46(1):96–105, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib265">
<span class="ltx_tag ltx_tag_bibitem">[265]</span>
<span class="ltx_bibblock">
F Agbavor and H Liang.

</span>
<span class="ltx_bibblock">Predicting dementia from spontaneous speech using large language models. plos digit health 2022; 1: e0000168.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib266">
<span class="ltx_tag ltx_tag_bibitem">[266]</span>
<span class="ltx_bibblock">
Yan Cong, Arianna N LaCroix, and Jiyeon Lee.

</span>
<span class="ltx_bibblock">Clinical efficacy of pre-trained large language models through the lens of aphasia.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib266.1.1">Scientific Reports</span>, 14(1):15573, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib267">
<span class="ltx_tag ltx_tag_bibitem">[267]</span>
<span class="ltx_bibblock">
Dave Van Veen, Cara Van Uden, Louis Blankemeier, Jean-Benoit Delbrouck, Asad Aali, Christian Bluethgen, Anuj Pareek, Malgorzata Polacin, Eduardo Pontes Reis, Anna Seehofnerová, et al.

</span>
<span class="ltx_bibblock">Adapted large language models can outperform medical experts in clinical text summarization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib267.1.1">Nature medicine</span>, 30(4):1134–1142, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib268">
<span class="ltx_tag ltx_tag_bibitem">[268]</span>
<span class="ltx_bibblock">
Jung-Hyun Lee, Eunhee Choi, Robert McDougal, and William W Lytton.

</span>
<span class="ltx_bibblock">Gpt-4 performance for neurologic localization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib268.1.1">Neurology: Clinical Practice</span>, 14(3):e200293, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib269">
<span class="ltx_tag ltx_tag_bibitem">[269]</span>
<span class="ltx_bibblock">
Taeyoon Kwon, Kai Tzu-iunn Ong, Dongjin Kang, Seungjun Moon, Jeong Ryong Lee, Dosik Hwang, Beomseok Sohn, Yongsik Sim, Dongha Lee, and Jinyoung Yeo.

</span>
<span class="ltx_bibblock">Large language models are clinical reasoners: Reasoning-aware diagnosis framework with prompt-generated rationales.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib269.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</span>, volume 38, pages 18417–18425, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib270">
<span class="ltx_tag ltx_tag_bibitem">[270]</span>
<span class="ltx_bibblock">
Manato Akiyama and Yasubumi Sakakibara.

</span>
<span class="ltx_bibblock">Informative rna base embedding for rna structural alignment and clustering by deep representation learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib270.1.1">NAR genomics and bioinformatics</span>, 4(1):lqac012, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib271">
<span class="ltx_tag ltx_tag_bibitem">[271]</span>
<span class="ltx_bibblock">
Minghao Xu, Xinyu Yuan, Santiago Miret, and Jian Tang.

</span>
<span class="ltx_bibblock">Protst: Multi-modality learning of protein sequences and biomedical texts.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib271.1.1">International Conference on Machine Learning</span>, pages 38749–38767. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib272">
<span class="ltx_tag ltx_tag_bibitem">[272]</span>
<span class="ltx_bibblock">
Qiao Liu, Wanwen Zeng, Hongtu Zhu, Lexin Li, Wing Hung Wong, and Alzheimer’s Disease Neuroimaging Initiative.

</span>
<span class="ltx_bibblock">Leveraging genomic large language models to enhance causal genotype-brain-clinical pathways in alzheimer’s disease.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib272.1.1">medRxiv</span>, pages 2024–10, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib273">
<span class="ltx_tag ltx_tag_bibitem">[273]</span>
<span class="ltx_bibblock">
Mor Frank, Pengyu Ni, Matthew Jensen, and Mark B Gerstein.

</span>
<span class="ltx_bibblock">Leveraging a large language model to predict protein phase transition: A physical, multiscale, and interpretable approach.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib273.1.1">Proceedings of the National Academy of Sciences</span>, 121(33):e2320510121, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib274">
<span class="ltx_tag ltx_tag_bibitem">[274]</span>
<span class="ltx_bibblock">
Jonathan W Kim, Ahmed Alaa, and Danilo Bernardo.

</span>
<span class="ltx_bibblock">Eeg-gpt: exploring capabilities of large language models for eeg classification and interpretation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib274.1.1">arXiv preprint arXiv:2401.18006</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib275">
<span class="ltx_tag ltx_tag_bibitem">[275]</span>
<span class="ltx_bibblock">
Mengjun Liu, Zhiyun Song, Dongdong Chen, Xin Wang, Zixu Zhuang, Manman Fei, Lichi Zhang, and Qian Wang.

</span>
<span class="ltx_bibblock">Affinity learning based brain function representation for disease diagnosis.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib275.1.1">International Conference on Medical Image Computing and Computer-Assisted Intervention</span>, pages 14–23. Springer, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib276">
<span class="ltx_tag ltx_tag_bibitem">[276]</span>
<span class="ltx_bibblock">
Ali Behrouz and Farnoosh Hashemi.

</span>
<span class="ltx_bibblock">Brain-mamba: Encoding brain activity via selective state space models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib276.1.1">Conference on Health, Inference, and Learning</span>, pages 233–250. PMLR, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib277">
<span class="ltx_tag ltx_tag_bibitem">[277]</span>
<span class="ltx_bibblock">
Tjalling Jan Holwerda, Dorly JH Deeg, Aartjan TF Beekman, Theo G Van Tilburg, Max L Stek, Cees Jonker, and Robert A Schoevers.

</span>
<span class="ltx_bibblock">Feelings of loneliness, but not social isolation, predict dementia onset: results from the amsterdam study of the elderly (amstel).

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib277.1.1">Journal of Neurology, Neurosurgery &amp; Psychiatry</span>, 85(2):135–142, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib278">
<span class="ltx_tag ltx_tag_bibitem">[278]</span>
<span class="ltx_bibblock">
Xiang Qi.

</span>
<span class="ltx_bibblock">Chatgpt: A promising tool to combat social isolation and loneliness in older adults with mild cognitive impairment.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib278.1.1">Neurology Live</span>, pages NA–NA, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib279">
<span class="ltx_tag ltx_tag_bibitem">[279]</span>
<span class="ltx_bibblock">
Paolo Raile.

</span>
<span class="ltx_bibblock">The usefulness of chatgpt for psychotherapists and patients.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib279.1.1">Humanities and Social Sciences Communications</span>, 11(1):1–8, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib280">
<span class="ltx_tag ltx_tag_bibitem">[280]</span>
<span class="ltx_bibblock">
Ibrahim Ali Mohammed and Shiva Venkataraman.

</span>
<span class="ltx_bibblock">An innovative study for the development of a wearable ai device to monitor parkinson’s disease using generative ai and llm techniques.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib280.1.1">International Journal of Creative Research Thoughts (IJCRT) www. ijcrt. org, ISSN</span>, pages 2320–2882, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib281">
<span class="ltx_tag ltx_tag_bibitem">[281]</span>
<span class="ltx_bibblock">
Shamiha Binta Manir, KM Sajjadul Islam, Praveen Madiraju, and Priya Deshpande.

</span>
<span class="ltx_bibblock">Llm-based text prediction and question answer models for aphasia speech.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib281.1.1">IEEE Access</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib282">
<span class="ltx_tag ltx_tag_bibitem">[282]</span>
<span class="ltx_bibblock">
Giulia Liberati, Josué Luiz Dalboni Da Rocha, Linda Van der Heiden, Antonino Raffone, Niels Birbaumer, Marta Olivetti Belardinelli, and Ranganatha Sitaram.

</span>
<span class="ltx_bibblock">Toward a brain-computer interface for alzheimer’s disease patients by combining classical conditioning and brain state classification.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib282.1.1">Journal of Alzheimer’s Disease</span>, 31(s3):S211–S220, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib283">
<span class="ltx_tag ltx_tag_bibitem">[283]</span>
<span class="ltx_bibblock">
Aleksandar Miladinović, Miloš Ajčević, Pierpaolo Busan, Joanna Jarmolowska, Giulia Silveri, Manuela Deodato, Susanna Mezzarobba, Piero Paolo Battaglini, and Agostino Accardo.

</span>
<span class="ltx_bibblock">Evaluation of motor imagery-based bci methods in neurorehabilitation of parkinson’s disease patients.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib283.1.1">2020 42nd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</span>, pages 3058–3061. IEEE, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib284">
<span class="ltx_tag ltx_tag_bibitem">[284]</span>
<span class="ltx_bibblock">
Zhijun Li, Suna Zhao, Jiding Duan, Chun-Yi Su, Chenguang Yang, and Xingang Zhao.

</span>
<span class="ltx_bibblock">Human cooperative wheelchair with brain–machine interaction based on shared control strategy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib284.1.1">IEEE/ASME Transactions on Mechatronics</span>, 22(1):185–195, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib285">
<span class="ltx_tag ltx_tag_bibitem">[285]</span>
<span class="ltx_bibblock">
Zehong Cao.

</span>
<span class="ltx_bibblock">A review of artificial intelligence for eeg-based brain- computer interfaces and applications.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib285.1.1">Brain Science Advances</span>, 6(3):162–170, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib286">
<span class="ltx_tag ltx_tag_bibitem">[286]</span>
<span class="ltx_bibblock">
Paolo Sorino, Giovanni Maria Biancofiore, Domenico Lofù, Tommaso Colafiglio, Angela Lombardi, Fedelucio Narducci, and Tommaso Di Noia.

</span>
<span class="ltx_bibblock">Ariel: Brain-computer interfaces meet large language models for emotional support conversation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib286.1.1">Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization</span>, pages 601–609, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib287">
<span class="ltx_tag ltx_tag_bibitem">[287]</span>
<span class="ltx_bibblock">
Donato Manuel Jiménez Benetó.

</span>
<span class="ltx_bibblock">Arithmetic reasoning in large language models and a speech brain-computer interface.

</span>
<span class="ltx_bibblock">B.S. thesis, Universitat Politècnica de Catalunya, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib288">
<span class="ltx_tag ltx_tag_bibitem">[288]</span>
<span class="ltx_bibblock">
Hamid Reza Saeidnia, Marcin Kozak, Brady D Lund, and Mohammad Hassanzadeh.

</span>
<span class="ltx_bibblock">Evaluation of chatgpt’s responses to information needs and information seeking of dementia patients.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib288.1.1">Scientific Reports</span>, 14(1):10273, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib289">
<span class="ltx_tag ltx_tag_bibitem">[289]</span>
<span class="ltx_bibblock">
Vagelis Hristidis, Nicole Ruggiano, Ellen L Brown, Sai Rithesh Reddy Ganta, and Selena Stewart.

</span>
<span class="ltx_bibblock">Chatgpt vs google for queries related to dementia and other cognitive decline: comparison of results.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib289.1.1">Journal of Medical Internet Research</span>, 25:e48966, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib290">
<span class="ltx_tag ltx_tag_bibitem">[290]</span>
<span class="ltx_bibblock">
Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie, and Yanfeng Wang.

</span>
<span class="ltx_bibblock">Pmc-llama: toward building open-source language models for medicine.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib290.1.1">Journal of the American Medical Informatics Association</span>, page ocae045, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib291">
<span class="ltx_tag ltx_tag_bibitem">[291]</span>
<span class="ltx_bibblock">
Yujin Oh, Sangjoon Park, Hwa Kyung Byun, Yeona Cho, Ik Jae Lee, Jin Sung Kim, and Jong Chul Ye.

</span>
<span class="ltx_bibblock">Llm-driven multimodal target volume contouring in radiation oncology.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib291.1.1">Nature Communications</span>, 15(1):9186, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib292">
<span class="ltx_tag ltx_tag_bibitem">[292]</span>
<span class="ltx_bibblock">
Yujin Oh, Sangjoon Park, Xiang Li, Wang Yi, Jonathan Paly, Jason Efstathiou, Annie Chan, Jun Won Kim, Hwa Kyung Byun, Ik Jae Lee, et al.

</span>
<span class="ltx_bibblock">Mixture of multicenter experts in multimodal generative ai for advanced radiotherapy target delineation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib292.1.1">arXiv preprint arXiv:2410.00046</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib293">
<span class="ltx_tag ltx_tag_bibitem">[293]</span>
<span class="ltx_bibblock">
P Wang, Z Liu, Y Li, J Holmes, P Shu, L Zhang, X Li, Q Li, SA Vora, SH Patel, et al.

</span>
<span class="ltx_bibblock">Fine-tuning large language models for radiation oncology, a specialized health care domain.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib293.1.1">International Journal of Radiation Oncology, Biology, Physics</span>, 120(2):e664, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib294">
<span class="ltx_tag ltx_tag_bibitem">[294]</span>
<span class="ltx_bibblock">
Yujin Oh, Sangjoon Park, Hwa Kyung Byun, Yeona Cho, Ik Jae Lee, Jin Sung Kim, and Jong Chul Ye.

</span>
<span class="ltx_bibblock">Llm-driven multimodal target volume contouring in radiation oncology.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib294.1.1">NATURE COMMUNICATIONS</span>, 15(1), OCT 24 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib295">
<span class="ltx_tag ltx_tag_bibitem">[295]</span>
<span class="ltx_bibblock">
Zehao Dong, Yixin Chen, Hiram Gay, Yao Hao, Geoffrey D. Hugo, Pamela Samson, and Tianyu Zhao.

</span>
<span class="ltx_bibblock">Large-language-model empowered 3d dose prediction for intensity-modulated radiotherapy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib295.1.1">MEDICAL PHYSICS</span>, 2024 SEP 24 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib296">
<span class="ltx_tag ltx_tag_bibitem">[296]</span>
<span class="ltx_bibblock">
Jason Holmes, Lian Zhang, Yuzhen Ding, Hongying Feng, Zhengliang Liu, Tianming Liu, William W. Wong, Sujay A. Vora, Jonathan B. Ashman, and Wei Liu.

</span>
<span class="ltx_bibblock">Benchmarking a foundation large language model on its ability to relabel structure names in accordance with the american association of physicists in medicine task group-263 report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib296.1.1">Practical Radiation Oncology</span>, 14(6):e515–e521, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib297">
<span class="ltx_tag ltx_tag_bibitem">[297]</span>
<span class="ltx_bibblock">
Yuexing Hao, Jason M Holmes, Jared Hobson, Alexandra Bennett, Daniel K Ebner, David M Routman, Satomi Shiraishi, Samir H Patel, Nathan Y Yu, Chris L Hallemeier, et al.

</span>
<span class="ltx_bibblock">Retrospective comparative analysis of prostate cancer in-basket messages: Responses from closed-domain llm vs. clinical teams.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib297.1.1">arXiv preprint arXiv:2409.18290</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib298">
<span class="ltx_tag ltx_tag_bibitem">[298]</span>
<span class="ltx_bibblock">
Peilong Wang, Jason Holmes, Zhengliang Liu, Dequan Chen, Tianming Liu, Jiajian Shen, and Wei Liu.

</span>
<span class="ltx_bibblock">A recent evaluation on the performance of llms on radiation oncology physics using questions of randomly shuffled options, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib299">
<span class="ltx_tag ltx_tag_bibitem">[299]</span>
<span class="ltx_bibblock">
Ljiljana Trtica-Majnaric, Marijana Zekic-Susac, Natasa Sarlija, and Branko Vitale.

</span>
<span class="ltx_bibblock">Prediction of influenza vaccination outcome by neural networks and logistic regression.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib299.1.1">Journal of Biomedical Informatics</span>, 43(5):774–781, October 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib300">
<span class="ltx_tag ltx_tag_bibitem">[300]</span>
<span class="ltx_bibblock">
Khalid Alhasan, Jaffar Al-Tawfiq, Fadi Aljamaan, Amr Jamal, Ayman Al-Eyadhy, and Mohamad-Hani Temsah.

</span>
<span class="ltx_bibblock">Mitigating the burden of severe pediatric respiratory viruses in the post-covid-19 era: Chatgpt insights and recommendations.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib300.1.1">Cureus</span>, 15(3), 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib301">
<span class="ltx_tag ltx_tag_bibitem">[301]</span>
<span class="ltx_bibblock">
Shang-Kai Hung, Chin-Chieh Wu, Avichandra Singh, Jin-Hua Li, Christian Lee, Eric H. Chou, Andrew Pekosz, Richard Rothman, and Kuan-Fu Chen.

</span>
<span class="ltx_bibblock">Developing and validating clinical features-based machine learning algorithms to predict influenza infection in influenza-like illness patients.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib301.1.1">Biomedical Journal</span>, 46(5):100561, October 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib302">
<span class="ltx_tag ltx_tag_bibitem">[302]</span>
<span class="ltx_bibblock">
David Furman, Vladimir Jojic, Brian Kidd, Shai Shen-Orr, Jordan Price, Justin Jarrell, Tiffany Tse, Huang Huang, Peder Lund, Holden T Maecker, et al.

</span>
<span class="ltx_bibblock">Apoptosis and other immune biomarkers predict influenza vaccine responsiveness.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib302.1.1">Molecular systems biology</span>, 9(1):659, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib303">
<span class="ltx_tag ltx_tag_bibitem">[303]</span>
<span class="ltx_bibblock">
Jingcheng Du, Yang Xiang, Madhuri Sankaranarayanapillai, Meng Zhang, Jingqi Wang, Yuqi Si, Huy Anh Pham, Hua Xu, Yong Chen, and Cui Tao.

</span>
<span class="ltx_bibblock">Extracting postmarketing adverse events from safety reports in the vaccine adverse event reporting system (vaers) using deep learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib303.1.1">Journal of the American Medical Informatics Association</span>, 28(7):1393–1400, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib304">
<span class="ltx_tag ltx_tag_bibitem">[304]</span>
<span class="ltx_bibblock">
Syed Awais W. Shah, Daniel P. Palomar, Ian Barr, Leo L. M. Poon, Ahmed Abdul Quadeer, and Matthew R. McKay.

</span>
<span class="ltx_bibblock">Seasonal antigenic prediction of influenza A H3N2 using machine learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib304.1.1">Nature Communications</span>, 15(1):3833, May 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib305">
<span class="ltx_tag ltx_tag_bibitem">[305]</span>
<span class="ltx_bibblock">
Moojung Kim, Young Jae Kim, Sung Jin Park, Kwang Gi Kim, Pyung Chun Oh, Young Saing Kim, and Eun Young Kim.

</span>
<span class="ltx_bibblock">Machine learning models to identify low adherence to influenza vaccination among Korean adults with cardiovascular disease.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib305.1.1">BMC cardiovascular disorders</span>, 21(1):129, March 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib306">
<span class="ltx_tag ltx_tag_bibitem">[306]</span>
<span class="ltx_bibblock">
Xiaolei Huang, Michael C. Smith, Amelia M. Jamison, David A. Broniatowski, Mark Dredze, Sandra Crouse Quinn, Justin Cai, and Michael J. Paul.

</span>
<span class="ltx_bibblock">Can online self-reports assist in real-time identification of influenza vaccination uptake? A cross-sectional study of influenza vaccine-related tweets in the USA, 2013-2017.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib306.1.1">BMJ open</span>, 9(1):e024018, January 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib307">
<span class="ltx_tag ltx_tag_bibitem">[307]</span>
<span class="ltx_bibblock">
Pritish Mondal, Ankita Sinharoy, and Lilly Su.

</span>
<span class="ltx_bibblock">Sociodemographic predictors of COVID-19 vaccine acceptance: A nationwide US-based survey study.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib307.1.1">Public Health</span>, 198:252–259, September 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib308">
<span class="ltx_tag ltx_tag_bibitem">[308]</span>
<span class="ltx_bibblock">
Ramy Mohamed Ghazy, Sally Waheed Elkhadry, Suzan Abdel-Rahman, Sarah Hamed N Taha, Naglaa Youssef, Abdelhamid Elshabrawy, Sarah Assem Ibrahim, Salah Al Awaidy, Tareq Al-Ahdal, Bijaya Kumar Padhi, et al.

</span>
<span class="ltx_bibblock">External validation of the parental attitude about childhood vaccination scale.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib308.1.1">Frontiers in Public Health</span>, 11:1146792, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib309">
<span class="ltx_tag ltx_tag_bibitem">[309]</span>
<span class="ltx_bibblock">
Fiona Sammut, David Suda, Mark Anthony Caruana, and Olga Bogolyubova.

</span>
<span class="ltx_bibblock">COVID-19 vaccination attitudes across the European continent.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib309.1.1">Heliyon</span>, 9(8):e18903, August 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib310">
<span class="ltx_tag ltx_tag_bibitem">[310]</span>
<span class="ltx_bibblock">
Ty J. Skyles, Harlan P. Stevens, Spencer C. Davis, Acelan M. Obray, Dashiell S. Miner, Matthew J. East, Tyler Davis, Haley Hoelzer, Stephen R. Piccolo, Jamie L. Jensen, and Brian D. Poole.

</span>
<span class="ltx_bibblock">Comparison of Predictive Factors of Flu Vaccine Uptake Pre- and Post-COVID-19 Using the NIS-Teen Survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib310.1.1">Vaccines</span>, 12(10):1164, October 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib311">
<span class="ltx_tag ltx_tag_bibitem">[311]</span>
<span class="ltx_bibblock">
Peter B. McGarvey, Baris E. Suzek, James N. Baraniuk, Shruti Rao, Brian Conkright, Samir Lababidi, Andrea Sutherland, Richard Forshee, and Subha Madhavan.

</span>
<span class="ltx_bibblock">In silico analysis of autoimmune diseases and genetic relationships to vaccination against infectious diseases.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib311.1.1">BMC immunology</span>, 15:61, December 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib312">
<span class="ltx_tag ltx_tag_bibitem">[312]</span>
<span class="ltx_bibblock">
Caleb A. Lareau, Bill C. White, Ann L. Oberg, and Brett A. McKinney.

</span>
<span class="ltx_bibblock">Differential co-expression network centrality and machine learning feature selection for identifying susceptibility hubs in networks with scale-free structure.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib312.1.1">BioData Mining</span>, 8:5, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib313">
<span class="ltx_tag ltx_tag_bibitem">[313]</span>
<span class="ltx_bibblock">
Emmanuel S Adabor.

</span>
<span class="ltx_bibblock">A statistical analysis of antigenic similarity among influenza a (h3n2) viruses.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib313.1.1">Heliyon</span>, 7(11), 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib314">
<span class="ltx_tag ltx_tag_bibitem">[314]</span>
<span class="ltx_bibblock">
David R. McIlwain, Han Chen, Zainab Rahil, Neda Hajiakhoond Bidoki, Sizun Jiang, Zach Bjornson, Nikita S. Kolhatkar, C. Josefina Martinez, Brice Gaudillière, Julien Hedou, Nilanjan Mukherjee, Christian M. Schürch, Angelica Trejo, Melton Affrime, Bonnie Bock, Kenneth Kim, David Liebowitz, Nima Aghaeepour, Sean N. Tucker, and Garry P. Nolan.

</span>
<span class="ltx_bibblock">Human influenza virus challenge identifies cellular correlates of protection for oral vaccination.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib314.1.1">Cell Host &amp; Microbe</span>, 29(12):1828–1837.e5, December 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib315">
<span class="ltx_tag ltx_tag_bibitem">[315]</span>
<span class="ltx_bibblock">
Lauren L. Luciani, Leigh M. Miller, Bo Zhai, Karen Clarke, Kailey Hughes Kramer, Lucas J. Schratz, G. K. Balasubramani, Klancie Dauer, M. Patricia Nowalk, Richard K. Zimmerman, Jason E. Shoemaker, and John F. Alcorn.

</span>
<span class="ltx_bibblock">Blood Inflammatory Biomarkers Differentiate Inpatient and Outpatient Coronavirus Disease 2019 From Influenza.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib315.1.1">Open Forum Infectious Diseases</span>, 10(3):ofad095, March 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib316">
<span class="ltx_tag ltx_tag_bibitem">[316]</span>
<span class="ltx_bibblock">
Maryam Hayati, Benjamin Sobkowiak, Jessica E. Stockdale, and Caroline Colijn.

</span>
<span class="ltx_bibblock">Phylogenetic identification of influenza virus candidates for seasonal vaccines.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib316.1.1">Science Advances</span>, 9(44):eabp9185, November 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib317">
<span class="ltx_tag ltx_tag_bibitem">[317]</span>
<span class="ltx_bibblock">
Cheng Gao, Feng Wen, Minhui Guan, Bijaya Hatuwal, Lei Li, Beatriz Praena, Cynthia Y Tang, Jieze Zhang, Feng Luo, Hang Xie, et al.

</span>
<span class="ltx_bibblock">Maivess: streamlined selection of antigenically matched, high-yield viruses for seasonal influenza vaccine production.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib317.1.1">Nature Communications</span>, 15(1):1128, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib318">
<span class="ltx_tag ltx_tag_bibitem">[318]</span>
<span class="ltx_bibblock">
Nicola Cotugno, Veronica Santilli, Giuseppe Rubens Pascucci, Emma Concetta Manno, Lesley De Armas, Suresh Pallikkuth, Annalisa Deodati, Donato Amodio, Paola Zangari, Sonia Zicari, et al.

</span>
<span class="ltx_bibblock">Artificial intelligence applied to in vitro gene expression testing (iviget) to predict trivalent inactivated influenza vaccine immunogenicity in hiv infected children.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib318.1.1">Frontiers in immunology</span>, 11:559590, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib319">
<span class="ltx_tag ltx_tag_bibitem">[319]</span>
<span class="ltx_bibblock">
Eva K. Lee, Haozheng Tian, and Helder I. Nakaya.

</span>
<span class="ltx_bibblock">Antigenicity prediction and vaccine recommendation of human influenza virus A (H3N2) using convolutional neural networks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib319.1.1">Human Vaccines &amp; Immunotherapeutics</span>, 16(11):2690–2708, November 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib320">
<span class="ltx_tag ltx_tag_bibitem">[320]</span>
<span class="ltx_bibblock">
Christopher Meaney, Michael Escobar, Therese A. Stukel, Peter C. Austin, and Liisa Jaakkimainen.

</span>
<span class="ltx_bibblock">Comparison of Methods for Estimating Temporal Topic Models From Primary Care Clinical Text Data: Retrospective Closed Cohort Study.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib320.1.1">JMIR medical informatics</span>, 10(12):e40102, December 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib321">
<span class="ltx_tag ltx_tag_bibitem">[321]</span>
<span class="ltx_bibblock">
Valeria Valerio, Emmanouil Rampakakis, Theodoros P. Zanos, Todd J. Levy, Hao Cheng Shen, Emily G. McDonald, Charles Frenette, Sasha Bernatsky, Marie Hudson, Brian J. Ward, and Inés Colmegna.

</span>
<span class="ltx_bibblock">High Frequency of COVID-19 Vaccine Hesitancy among Canadians Immunized for Influenza: A Cross-Sectional Survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib321.1.1">Vaccines</span>, 10(9):1514, September 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib322">
<span class="ltx_tag ltx_tag_bibitem">[322]</span>
<span class="ltx_bibblock">
Qin Xiang Ng, Clara Xinyi Ng, Clarence Ong, Dawn Yi Xin Lee, and Tau Ming Liew.

</span>
<span class="ltx_bibblock">Examining Public Messaging on Influenza Vaccine over Social Media: Unsupervised Deep Learning of 235,261 Twitter Posts from 2017 to 2023.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib322.1.1">Vaccines</span>, 11(10):1518, September 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib323">
<span class="ltx_tag ltx_tag_bibitem">[323]</span>
<span class="ltx_bibblock">
Qin Xiang Ng, Dawn Yi Xin Lee, Clara Xinyi Ng, Chun En Yau, Yu Liang Lim, and Tau Ming Liew.

</span>
<span class="ltx_bibblock">Examining the Negative Sentiments Related to Influenza Vaccination from 2017 to 2022: An Unsupervised Deep Learning Analysis of 261,613 Twitter Posts.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib323.1.1">Vaccines</span>, 11(6):1018, May 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib324">
<span class="ltx_tag ltx_tag_bibitem">[324]</span>
<span class="ltx_bibblock">
Yosi Levi, Margaret L. Brandeau, Erez Shmueli, and Dan Yamin.

</span>
<span class="ltx_bibblock">Prediction and detection of side effects severity following COVID-19 and influenza vaccinations: Utilizing smartwatches and smartphones.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib324.1.1">Scientific Reports</span>, 14(1):6012, March 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib325">
<span class="ltx_tag ltx_tag_bibitem">[325]</span>
<span class="ltx_bibblock">
Matthew Deady, Hussein Ezzeldin, Kerry Cook, Douglas Billings, Jeno Pizarro, Amalia A Plotogea, Patrick Saunders-Hastings, Artur Belov, Barbee I Whitaker, and Steven A Anderson.

</span>
<span class="ltx_bibblock">The food and drug administration biologics effectiveness and safety initiative facilitates detection of vaccine administrations from unstructured data in medical records through natural language processing.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib325.1.1">Frontiers in Digital Health</span>, 3:777905, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib326">
<span class="ltx_tag ltx_tag_bibitem">[326]</span>
<span class="ltx_bibblock">
Michael T. Zimmermann, Richard B. Kennedy, Diane E. Grill, Ann L. Oberg, Krista M. Goergen, Inna G. Ovsyannikova, Iana H. Haralambieva, and Gregory A. Poland.

</span>
<span class="ltx_bibblock">Integration of Immune Cell Populations, mRNA-Seq, and CpG Methylation to Better Predict Humoral Immunity to Influenza Vaccination: Dependence of mRNA-Seq/CpG Methylation on Immune Cell Populations.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib326.1.1">Frontiers in Immunology</span>, 8:445, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib327">
<span class="ltx_tag ltx_tag_bibitem">[327]</span>
<span class="ltx_bibblock">
Diego Galvan, Luciane Effting, Hágata Cremasco, and Carlos Adam Conte-Junior.

</span>
<span class="ltx_bibblock">Can socioeconomic, health, and safety data explain the spread of covid-19 outbreak on brazilian federative units?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib327.1.1">International journal of environmental research and public health</span>, 17(23):8921, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib328">
<span class="ltx_tag ltx_tag_bibitem">[328]</span>
<span class="ltx_bibblock">
Stacey L. Wooden and Wayne C. Koff.

</span>
<span class="ltx_bibblock">The Human Vaccines Project: Towards a comprehensive understanding of the human immune response to immunization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib328.1.1">Human Vaccines &amp; Immunotherapeutics</span>, 14(9):2214–2216, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib329">
<span class="ltx_tag ltx_tag_bibitem">[329]</span>
<span class="ltx_bibblock">
Amir Hadid, Emily G. McDonald, Matthew P. Cheng, Jesse Papenburg, Michael Libman, Phillipe C. Dixon, and Dennis Jensen.

</span>
<span class="ltx_bibblock">The WE SENSE study protocol: A controlled, longitudinal clinical trial on the use of wearable sensors for early detection and tracking of viral respiratory tract infections.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib329.1.1">Contemporary Clinical Trials</span>, 128:107103, May 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib330">
<span class="ltx_tag ltx_tag_bibitem">[330]</span>
<span class="ltx_bibblock">
Mathias W. Pletz, Andreas Vestergaard Jensen, Christina Bahrs, Claudia Davenport, Jan Rupp, Martin Witzenrath, Grit Barten-Neiner, Martin Kolditz, Sabine Dettmer, James D. Chalmers, Daiana Stolz, Norbert Suttorp, Stefano Aliberti, Wolfgang M. Kuebler, and Gernot Rohde.

</span>
<span class="ltx_bibblock">Unmet needs in pneumonia research: A comprehensive approach by the CAPNETZ study group.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib330.1.1">Respiratory Research</span>, 23(1):239, September 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib331">
<span class="ltx_tag ltx_tag_bibitem">[331]</span>
<span class="ltx_bibblock">
Nurken Berdigaliyev and Mohamad Aljofan.

</span>
<span class="ltx_bibblock">An overview of drug discovery and development.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib331.1.1">Future Medicinal Chemistry</span>, 12(10):939–947, May 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib332">
<span class="ltx_tag ltx_tag_bibitem">[332]</span>
<span class="ltx_bibblock">
Jeffrey Cummings, Yadi Zhou, Garam Lee, Kate Zhong, Jorge Fonseca, and Feixiong Cheng.

</span>
<span class="ltx_bibblock">Alzheimer’s disease drug development pipeline: 2024.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib332.1.1">Alzheimer’s &amp; Dementia: Translational Research &amp; Clinical Interventions</span>, 10(2):e12465, 2024.

</span>
<span class="ltx_bibblock">_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/trc2.12465.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib333">
<span class="ltx_tag ltx_tag_bibitem">[333]</span>
<span class="ltx_bibblock">
Anastasiia V. Sadybekov and Vsevolod Katritch.

</span>
<span class="ltx_bibblock">Computational approaches streamlining drug discovery.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib333.1.1">Nature</span>, 616(7958):673–685, April 2023.

</span>
<span class="ltx_bibblock">Publisher: Nature Publishing Group.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib334">
<span class="ltx_tag ltx_tag_bibitem">[334]</span>
<span class="ltx_bibblock">
Jingru Wang, Yihang Xiao, Xuequn Shang, and Jiajie Peng.

</span>
<span class="ltx_bibblock">Predicting drug–target binding affinity with cross-scale graph contrastive learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib334.1.1">Briefings in Bioinformatics</span>, 25(1):bbad516, January 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib335">
<span class="ltx_tag ltx_tag_bibitem">[335]</span>
<span class="ltx_bibblock">
Nathan C. Frey, Ryan Soklaski, Simon Axelrod, Siddharth Samsi, Rafael Gómez-Bombarelli, Connor W. Coley, and Vijay Gadepally.

</span>
<span class="ltx_bibblock">Neural scaling of deep chemical models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib335.1.1">Nature Machine Intelligence</span>, 5(11):1297–1305, November 2023.

</span>
<span class="ltx_bibblock">Publisher: Nature Publishing Group.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib336">
<span class="ltx_tag ltx_tag_bibitem">[336]</span>
<span class="ltx_bibblock">
Kexin Huang, Payal Chandak, Qianwen Wang, Shreyas Havaldar, Akhil Vaid, Jure Leskovec, Girish N. Nadkarni, Benjamin S. Glicksberg, Nils Gehlenborg, and Marinka Zitnik.

</span>
<span class="ltx_bibblock">A foundation model for clinician-centered drug repurposing.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib336.1.1">Nature Medicine</span>, 30(12):3601–3613, December 2024.

</span>
<span class="ltx_bibblock">Publisher: Nature Publishing Group.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib337">
<span class="ltx_tag ltx_tag_bibitem">[337]</span>
<span class="ltx_bibblock">
Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Abubakr Babiker, Nathanael Schärli, Aakanksha Chowdhery, Philip Mansfield, Dina Demner-Fushman, Blaise Agüera y Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan.

</span>
<span class="ltx_bibblock">Large language models encode clinical knowledge.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib337.1.1">Nature</span>, 620(7972):172–180, August 2023.

</span>
<span class="ltx_bibblock">Publisher: Nature Publishing Group.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib338">
<span class="ltx_tag ltx_tag_bibitem">[338]</span>
<span class="ltx_bibblock">
Barbara Zdrazil, Eloy Felix, Fiona Hunter, Emma J Manners, James Blackshaw, Sybilla Corbett, Marleen de Veij, Harris Ioannidis, David Mendez Lopez, Juan F Mosquera, Maria Paula Magarinos, Nicolas Bosc, Ricardo Arcila, Tevfik Kizilören, Anna Gaulton, A Patrícia Bento, Melissa F Adasme, Peter Monecke, Gregory A Landrum, and Andrew R Leach.

</span>
<span class="ltx_bibblock">The ChEMBL Database in 2023: a drug discovery platform spanning multiple bioactivity data types and time periods.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib338.1.1">Nucleic Acids Research</span>, 52(D1):D1180–D1192, January 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib339">
<span class="ltx_tag ltx_tag_bibitem">[339]</span>
<span class="ltx_bibblock">
Gautier Koscielny, Peter An, Denise Carvalho-Silva, Jennifer A. Cham, Luca Fumis, Rippa Gasparyan, Samiul Hasan, Nikiforos Karamanis, Michael Maguire, Eliseo Papa, Andrea Pierleoni, Miguel Pignatelli, Theo Platt, Francis Rowland, Priyanka Wankar, A. Patrícia Bento, Tony Burdett, Antonio Fabregat, Simon Forbes, Anna Gaulton, Cristina Yenyxe Gonzalez, Henning Hermjakob, Anne Hersey, Steven Jupe, Şenay Kafkas, Maria Keays, Catherine Leroy, Francisco-Javier Lopez, Maria Paula Magarinos, James Malone, Johanna McEntyre, Alfonso Munoz-Pomer Fuentes, Claire O’Donovan, Irene Papatheodorou, Helen Parkinson, Barbara Palka, Justin Paschall, Robert Petryszak, Naruemon Pratanwanich, Sirarat Sarntivijal, Gary Saunders, Konstantinos Sidiropoulos, Thomas Smith, Zbyslaw Sondka, Oliver Stegle, Y. Amy Tang, Edward Turner, Brendan Vaughan, Olga Vrousgou, Xavier Watkins, Maria-Jesus Martin, Philippe Sanseau, Jessica Vamathevan, Ewan Birney, Jeffrey Barrett, and Ian Dunham.

</span>
<span class="ltx_bibblock">Open Targets: a platform for therapeutic target identification and validation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib339.1.1">Nucleic Acids Research</span>, 45(D1):D985–D994, January 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib340">
<span class="ltx_tag ltx_tag_bibitem">[340]</span>
<span class="ltx_bibblock">
David S Wishart, Yannick D Feunang, An C Guo, Elvis J Lo, Ana Marcu, Jason R Grant, Tanvir Sajed, Daniel Johnson, Carin Li, Zinat Sayeeda, Nazanin Assempour, Ithayavani Iynkkaran, Yifeng Liu, Adam Maciejewski, Nicola Gale, Alex Wilson, Lucy Chin, Ryan Cummings, Diana Le, Allison Pon, Craig Knox, and Michael Wilson.

</span>
<span class="ltx_bibblock">DrugBank 5.0: a major update to the DrugBank database for 2018.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib340.1.1">Nucleic Acids Research</span>, 46(D1):D1074–D1082, January 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib341">
<span class="ltx_tag ltx_tag_bibitem">[341]</span>
<span class="ltx_bibblock">
Frank W. Pun, Ivan V. Ozerov, and Alex Zhavoronkov.

</span>
<span class="ltx_bibblock">AI-powered therapeutic target discovery.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib341.1.1">Trends in Pharmacological Sciences</span>, 44(9):561–572, September 2023.

</span>
<span class="ltx_bibblock">Publisher: Elsevier.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib342">
<span class="ltx_tag ltx_tag_bibitem">[342]</span>
<span class="ltx_bibblock">
Neil Savage.

</span>
<span class="ltx_bibblock">Drug discovery companies are customizing ChatGPT: here’s how.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib342.1.1">Nature Biotechnology</span>, 41(5):585–586, May 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib343">
<span class="ltx_tag ltx_tag_bibitem">[343]</span>
<span class="ltx_bibblock">
Mahsa Sheikholeslami, Navid Mazrouei, Yousof Gheisari, Afshin Fasihi, Matin Irajpour, and Ali Motahharynia.

</span>
<span class="ltx_bibblock">DrugGen: Advancing Drug Discovery with Large Language Models and Reinforcement Learning Feedback, November 2024.

</span>
<span class="ltx_bibblock">arXiv:2411.14157 [q-bio].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib344">
<span class="ltx_tag ltx_tag_bibitem">[344]</span>
<span class="ltx_bibblock">
Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D. White, and Philippe Schwaller.

</span>
<span class="ltx_bibblock">Augmenting large language models with chemistry tools.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib344.1.1">Nature Machine Intelligence</span>, 6:525–535, May 2024.

</span>
<span class="ltx_bibblock">Publisher: Nature Publishing Group.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib345">
<span class="ltx_tag ltx_tag_bibitem">[345]</span>
<span class="ltx_bibblock">
Daniil A. Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes.

</span>
<span class="ltx_bibblock">Autonomous chemical research with large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib345.1.1">Nature</span>, 624(7992):570–578, December 2023.

</span>
<span class="ltx_bibblock">Publisher: Nature Publishing Group.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib346">
<span class="ltx_tag ltx_tag_bibitem">[346]</span>
<span class="ltx_bibblock">
Taha ValizadehAslani, Yiwen Shi, Ping Ren, Jing Wang, Yi Zhang, Meng Hu, Liang Zhao, and Hualou Liang.

</span>
<span class="ltx_bibblock">PharmBERT: a domain-specific BERT model for drug labels.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib346.1.1">Briefings in Bioinformatics</span>, 24(4):bbad226, July 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib347">
<span class="ltx_tag ltx_tag_bibitem">[347]</span>
<span class="ltx_bibblock">
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.

</span>
<span class="ltx_bibblock">BioBERT: a pre-trained biomedical language representation model for biomedical text mining.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib347.1.1">Bioinformatics</span>, 36(4):1234–1240, February 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib348">
<span class="ltx_tag ltx_tag_bibitem">[348]</span>
<span class="ltx_bibblock">
Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, and Michal Linial.

</span>
<span class="ltx_bibblock">ProteinBERT: a universal deep-learning model of protein sequence and function.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib348.1.1">Bioinformatics</span>, 38(8):2102–2110, April 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib349">
<span class="ltx_tag ltx_tag_bibitem">[349]</span>
<span class="ltx_bibblock">
Juan Manuel Zambrano Chaves, Eric Wang, Tao Tu, Eeshit Dhaval Vaishnav, Byron Lee, S. Sara Mahdavi, Christopher Semturs, David Fleet, Vivek Natarajan, and Shekoofeh Azizi.

</span>
<span class="ltx_bibblock">Tx-LLM: A Large Language Model for Therapeutics, June 2024.

</span>
<span class="ltx_bibblock">arXiv:2406.06316 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib350">
<span class="ltx_tag ltx_tag_bibitem">[350]</span>
<span class="ltx_bibblock">
Rohit Singh, Samuel Sledzieski, Bryan Bryson, Lenore Cowen, and Bonnie Berger.

</span>
<span class="ltx_bibblock">Contrastive learning in protein language space predicts interactions between drugs and protein targets.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib350.1.1">Proceedings of the National Academy of Sciences of the United States of America</span>, 120(24):e2220778120, June 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib351">
<span class="ltx_tag ltx_tag_bibitem">[351]</span>
<span class="ltx_bibblock">
Zhihui Yang, Juan Liu, Feng Yang, Xiaolei Zhang, Qiang Zhang, Xuekai Zhu, and Peng Jiang.

</span>
<span class="ltx_bibblock">Advancing Drug-Target Interaction prediction with BERT and subsequence embedding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib351.1.1">Computational Biology and Chemistry</span>, 110:108058, June 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib352">
<span class="ltx_tag ltx_tag_bibitem">[352]</span>
<span class="ltx_bibblock">
Yogesh Kalakoti, Shashank Yadav, and Durai Sundar.

</span>
<span class="ltx_bibblock">TransDTI: Transformer-Based Language Models for Estimating DTIs and Building a Drug Recommendation Workflow.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib352.1.1">ACS omega</span>, 7(3):2706–2717, January 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib353">
<span class="ltx_tag ltx_tag_bibitem">[353]</span>
<span class="ltx_bibblock">
Zhengchao Luo, Wei Wu, Qichen Sun, and Jinzhuo Wang.

</span>
<span class="ltx_bibblock">Accurate and transferable drug–target interaction prediction with druglamp.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib353.1.1">Bioinformatics</span>, 40(12):btae693, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib354">
<span class="ltx_tag ltx_tag_bibitem">[354]</span>
<span class="ltx_bibblock">
Rakesh Bal, Yijia Xiao, and Wei Wang.

</span>
<span class="ltx_bibblock">PGraphDTA: Improving Drug Target Interaction Prediction using Protein Language Models and Contact Maps, February 2024.

</span>
<span class="ltx_bibblock">arXiv:2310.04017 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib355">
<span class="ltx_tag ltx_tag_bibitem">[355]</span>
<span class="ltx_bibblock">
Qing Fan, Yingxu Liu, Simeng Zhang, Xiangzhen Ning, Chengcheng Xu, Weijie Han, Yanmin Zhang, Yadong Chen, Jun Shen, and Haichun Liu.

</span>
<span class="ltx_bibblock">CGPDTA: An Explainable Transfer Learning-Based Predictor With Molecule Substructure Graph for Drug-Target Binding Affinity.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib355.1.1">Journal of Computational Chemistry</span>, 46(1):e27538, January 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib356">
<span class="ltx_tag ltx_tag_bibitem">[356]</span>
<span class="ltx_bibblock">
Youwei Liang, Ruiyi Zhang, Yongce Li, Mingjia Huo, Zinnia Ma, Digvijay Singh, Chengzhan Gao, Hamidreza Rahmani, Satvik Bandi, Li Zhang, Robert Weinreb, Atul Malhotra, Danielle A. Grotjahn, Linda Awdishu, Trey Ideker, Michael Gilson, and Pengtao Xie.

</span>
<span class="ltx_bibblock">Multi-Modal Large Language Model Enables All-Purpose Prediction of Drug Mechanisms and Properties, October 2024.

</span>
<span class="ltx_bibblock">Pages: 2024.09.29.615524 Section: New Results.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib357">
<span class="ltx_tag ltx_tag_bibitem">[357]</span>
<span class="ltx_bibblock">
Tengfei Ma, Xuan Lin, Tianle Li, Chaoyi Li, Long Chen, Peng Zhou, Xibao Cai, Xinyu Yang, Daojian Zeng, Dongsheng Cao, and Xiangxiang Zeng.

</span>
<span class="ltx_bibblock">Y-Mol: A Multiscale Biomedical Knowledge-Guided Large Language Model for Drug Development, October 2024.

</span>
<span class="ltx_bibblock">arXiv:2410.11550 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib358">
<span class="ltx_tag ltx_tag_bibitem">[358]</span>
<span class="ltx_bibblock">
Yoshitaka Inoue, Tianci Song, and Tianfan Fu.

</span>
<span class="ltx_bibblock">DrugAgent: Explainable Drug Repurposing Agent with Large Language Model-based Reasoning, September 2024.

</span>
<span class="ltx_bibblock">arXiv:2408.13378 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib359">
<span class="ltx_tag ltx_tag_bibitem">[359]</span>
<span class="ltx_bibblock">
Allan Peter Davis, Cynthia J Grondin, Robin J Johnson, Daniela Sciaky, Jolene Wiegers, Thomas C Wiegers, and Carolyn J Mattingly.

</span>
<span class="ltx_bibblock">Comparative Toxicogenomics Database (CTD): update 2021.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib359.1.1">Nucleic Acids Research</span>, 49(D1):D1138–D1143, January 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib360">
<span class="ltx_tag ltx_tag_bibitem">[360]</span>
<span class="ltx_bibblock">
Lifan Chen, Zisheng Fan, Jie Chang, Ruirui Yang, Hui Hou, Hao Guo, Yinghui Zhang, Tianbiao Yang, Chenmao Zhou, Qibang Sui, et al.

</span>
<span class="ltx_bibblock">Sequence-based drug design as a concept in computational drug design.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib360.1.1">Nature Communications</span>, 14(1):4217, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib361">
<span class="ltx_tag ltx_tag_bibitem">[361]</span>
<span class="ltx_bibblock">
Shuo Zhang and Lei Xie.

</span>
<span class="ltx_bibblock">Protein language model-powered 3d ligand binding site prediction from protein sequence.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib361.1.1">NeurIPS 2023 AI for Science Workshop</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib362">
<span class="ltx_tag ltx_tag_bibitem">[362]</span>
<span class="ltx_bibblock">
Xiaomin Fang, Fan Wang, Lihang Liu, Jingzhou He, Dayong Lin, Yingfei Xiang, Kunrui Zhu, Xiaonan Zhang, Hua Wu, Hui Li, et al.

</span>
<span class="ltx_bibblock">A method for multiple-sequence-alignment-free protein structure prediction using a protein language model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib362.1.1">Nature Machine Intelligence</span>, 5(10):1087–1096, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib363">
<span class="ltx_tag ltx_tag_bibitem">[363]</span>
<span class="ltx_bibblock">
Chiranjib Chakraborty, Manojit Bhattacharya, and Sang-Soo Lee.

</span>
<span class="ltx_bibblock">Artificial intelligence enabled chatgpt and large language models in drug target discovery, drug discovery, and development.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib363.1.1">Molecular Therapy-Nucleic Acids</span>, 33:866–868, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib364">
<span class="ltx_tag ltx_tag_bibitem">[364]</span>
<span class="ltx_bibblock">
Bilal Shaker, Sajjad Ahmad, Jingyu Lee, Chanjin Jung, and Dokyun Na.

</span>
<span class="ltx_bibblock">In silico methods and tools for drug discovery.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib364.1.1">Computers in biology and medicine</span>, 137:104851, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib365">
<span class="ltx_tag ltx_tag_bibitem">[365]</span>
<span class="ltx_bibblock">
Gaurav Sharma and Abhishek Thakur.

</span>
<span class="ltx_bibblock">Chatgpt in drug discovery.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib366">
<span class="ltx_tag ltx_tag_bibitem">[366]</span>
<span class="ltx_bibblock">
Garrett M Morris, Ruth Huey, and Arthur J Olson.

</span>
<span class="ltx_bibblock">Using autodock for ligand-receptor docking.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib366.1.1">Current protocols in bioinformatics</span>, 24(1):8–14, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib367">
<span class="ltx_tag ltx_tag_bibitem">[367]</span>
<span class="ltx_bibblock">
Youwei Liang, Ruiyi Zhang, Li Zhang, and Pengtao Xie.

</span>
<span class="ltx_bibblock">Drugchat: towards enabling chatgpt-like capabilities on drug molecule graphs.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib367.1.1">arXiv preprint arXiv:2309.03907</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib368">
<span class="ltx_tag ltx_tag_bibitem">[368]</span>
<span class="ltx_bibblock">
Chao Shen, Xujun Zhang, Yafeng Deng, Junbo Gao, Dong Wang, Lei Xu, Peichen Pan, Tingjun Hou, and Yu Kang.

</span>
<span class="ltx_bibblock">Boosting protein–ligand binding pose prediction and virtual screening based on residue–atom distance likelihood potential and graph transformer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib368.1.1">Journal of Medicinal Chemistry</span>, 65(15):10691–10706, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib369">
<span class="ltx_tag ltx_tag_bibitem">[369]</span>
<span class="ltx_bibblock">
Oleg Trott and Arthur J Olson.

</span>
<span class="ltx_bibblock">Autodock vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib369.1.1">Journal of computational chemistry</span>, 31(2):455–461, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib370">
<span class="ltx_tag ltx_tag_bibitem">[370]</span>
<span class="ltx_bibblock">
Jingxiao Bao, Xiao He, and John ZH Zhang.

</span>
<span class="ltx_bibblock">Deepbsp—a machine learning method for accurate prediction of protein–ligand docking structures.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib370.1.1">Journal of chemical information and modeling</span>, 61(5):2231–2240, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib371">
<span class="ltx_tag ltx_tag_bibitem">[371]</span>
<span class="ltx_bibblock">
Oscar Méndez-Lucio, Mazen Ahmad, Ehecatl Antonio del Rio-Chanona, and Jörg Kurt Wegner.

</span>
<span class="ltx_bibblock">A geometric deep learning approach to predict binding conformations of bioactive molecules.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib371.1.1">Nature Machine Intelligence</span>, 3(12):1033–1039, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib372">
<span class="ltx_tag ltx_tag_bibitem">[372]</span>
<span class="ltx_bibblock">
Rohit Singh, Samuel Sledzieski, Bryan Bryson, Lenore Cowen, and Bonnie Berger.

</span>
<span class="ltx_bibblock">Contrastive learning in protein language space predicts interactions between drugs and protein targets.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib372.1.1">Proceedings of the National Academy of Sciences</span>, 120(24):e2220778120, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib373">
<span class="ltx_tag ltx_tag_bibitem">[373]</span>
<span class="ltx_bibblock">
Zhangming Niu, Xianglu Xiao, Wenfan Wu, Qiwei Cai, Yinghui Jiang, Wangzhen Jin, Minhao Wang, Guojian Yang, Lingkang Kong, Xurui Jin, et al.

</span>
<span class="ltx_bibblock">Pharmabench: Enhancing admet benchmarks with large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib373.1.1">Scientific Data</span>, 11(1):985, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib374">
<span class="ltx_tag ltx_tag_bibitem">[374]</span>
<span class="ltx_bibblock">
Adriana Tomic, Ivan Tomic, Cornelia L. Dekker, Holden T. Maecker, and Mark M. Davis.

</span>
<span class="ltx_bibblock">The FluPRINT dataset, a multidimensional analysis of the influenza vaccine imprint on the immune system.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib374.1.1">Scientific Data</span>, 6(1):214, October 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib375">
<span class="ltx_tag ltx_tag_bibitem">[375]</span>
<span class="ltx_bibblock">
Adriana Tomic, Ivan Tomic, Yael Rosenberg-Hasson, Cornelia L. Dekker, Holden T. Maecker, and Mark M. Davis.

</span>
<span class="ltx_bibblock">SIMON, an Automated Machine Learning System, Reveals Immune Signatures of Influenza Vaccine Responses.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib375.1.1">Journal of Immunology (Baltimore, Md.: 1950)</span>, 203(3):749–759, August 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib376">
<span class="ltx_tag ltx_tag_bibitem">[376]</span>
<span class="ltx_bibblock">
Saeid Parvandeh, Greg A. Poland, Richard B. Kennedy, and Brett A. McKinney.

</span>
<span class="ltx_bibblock">Multi-Level Model to Predict Antibody Response to Influenza Vaccine Using Gene Expression Interaction Network Feature Selection.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib376.1.1">Microorganisms</span>, 7(3):79, March 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib377">
<span class="ltx_tag ltx_tag_bibitem">[377]</span>
<span class="ltx_bibblock">
I Sutskever.

</span>
<span class="ltx_bibblock">Sequence to sequence learning with neural networks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib377.1.1">arXiv preprint arXiv:1409.3215</span>, 2014.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Jan 10 01:39:36 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
